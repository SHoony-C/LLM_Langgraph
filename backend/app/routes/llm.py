import os
import openai
from openai import AsyncOpenAI
import asyncio
import json
import httpx
import uuid
from fastapi import APIRouter, HTTPException, Response, Depends, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from langgraph.graph import END, StateGraph
from collections import defaultdict
from qdrant_client import QdrantClient
from app.utils.config import (
    OPENAI_API_KEY, OPENAI_BASE_URL,
    QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION,
    IMAGE_BASE_URL, IMAGE_PATH_PREFIX
)
from app.database import get_db
from app.models import Conversation, Message
from sqlalchemy.orm import Session

from datetime import datetime

# Create router 
router = APIRouter()

# Redis ì œê±°ë¨ - SSE ë°©ì‹ ì‚¬ìš©

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”© í™•ì¸
print(f"[Config] OpenAI API Key: {'ì„¤ì •ë¨' if OPENAI_API_KEY else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
print(f"[Config] Qdrant: {QDRANT_HOST}:{QDRANT_PORT} (ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'})")

# ì§ì ‘ êµ¬í˜„í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° í´ë˜ìŠ¤
class DirectSimilarityCalculator:
    def __init__(self):
        self.stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 'í•œ', 'í•˜ëŠ”', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê·¸', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ'}
        
    def preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”"""
        import re
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text.lower())
        # í† í°í™”
        tokens = text.split()
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ 2 ì´ìƒ í† í°ë§Œ ìœ ì§€
        tokens = [token for token in tokens if token not in self.stopwords and len(token) >= 2]
        return tokens
    
    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        tokens1 = set(self.preprocess_text(text1))
        tokens2 = set(self.preprocess_text(text2))
        
        if not tokens1 and not tokens2:
            return 1.0
        if not tokens1 or not tokens2:
            return 0.0
            
        intersection = len(tokens1.intersection(tokens2))
        union = len(tokens1.union(tokens2))
        
        return intersection / union if union > 0 else 0.0
    
    def calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (TF ê¸°ë°˜)"""
        tokens1 = self.preprocess_text(text1)
        tokens2 = self.preprocess_text(text2)
        
        # TF ê³„ì‚°
        tf1 = {}
        tf2 = {}
        
        for token in tokens1:
            tf1[token] = tf1.get(token, 0) + 1
        for token in tokens2:
            tf2[token] = tf2.get(token, 0) + 1
        
        # ì „ì²´ ë‹¨ì–´ ì§‘í•©
        all_tokens = set(tokens1 + tokens2)
        
        if not all_tokens:
            return 1.0
        
        # ë²¡í„° ìƒì„±
        vector1 = [tf1.get(token, 0) for token in all_tokens]
        vector2 = [tf2.get(token, 0) for token in all_tokens]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        dot_product = sum(a * b for a, b in zip(vector1, vector2))
        magnitude1 = sum(a * a for a in vector1) ** 0.5
        magnitude2 = sum(b * b for b in vector2) ** 0.5
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
            
        return dot_product / (magnitude1 * magnitude2)
    
    def calculate_combined_similarity(self, text1: str, text2: str) -> float:
        """ìì¹´ë“œì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ ìµœì¢… ìœ ì‚¬ë„"""
        jaccard_sim = self.calculate_jaccard_similarity(text1, text2)
        cosine_sim = self.calculate_cosine_similarity(text1, text2)
        
        # ê°€ì¤‘ í‰ê·  (ìì¹´ë“œ 0.4, ì½”ì‚¬ì¸ 0.6)
        combined_sim = (jaccard_sim * 0.4) + (cosine_sim * 0.6)
        return combined_sim
    
    def find_similar_documents(self, query: str, documents: List[dict], top_k: int = 5) -> List[dict]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ì°¾ì•„ ë°˜í™˜"""
        results = []
        
        for doc in documents:
            # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì œëª© + ë‚´ìš©)document_name
            doc_title = doc.get('document_name', '')
            vector_data = doc.get("vector", {})
            # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
            doc_content = vector_data.get("text") if isinstance(vector_data, dict) else None
            doc_text = f"{doc_title} {doc_content}"
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = self.calculate_combined_similarity(query, doc_text)
            
            results.append({
                'document': doc,
                'similarity': similarity,
                'matched_text': doc_text[:200] + '...' if len(doc_text) > 200 else doc_text
            })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]

# ì§ì ‘ êµ¬í˜„í•œ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
async def direct_document_search(question_type: str, limit: int, queries: List[str], 
                               ip: str, port: int, collection: str) -> List[dict]:
    """ì§ì ‘ êµ¬í˜„í•œ ë¬¸ì„œ ê²€ìƒ‰ (ìœ ì‚¬ë„ ê¸°ë°˜)"""
    try:
        print(f"[DIRECT_SEARCH] ì§ì ‘ ê²€ìƒ‰ ì‹œì‘: {len(queries)}ê°œ ì¿¼ë¦¬")
        
        # Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        client = QdrantClient(host=ip, port=port)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        try:
            collections = client.get_collections()
            print(f"[DIRECT_SEARCH] ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {[col.name for col in collections.collections]}")
            
            if not collection or collection not in [col.name for col in collections.collections]:
                print(f"[DIRECT_SEARCH] ì»¬ë ‰ì…˜ '{collection}' ì‚¬ìš© ë¶ˆê°€")
                return []
                
        except Exception as e:
            print(f"[DIRECT_SEARCH] Qdrant ì—°ê²° ì˜¤ë¥˜: {e}")
            return []
        
        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë¡¤ ë°©ì‹)
        all_documents = []
        try:
            scroll_result = client.scroll(
                collection_name=collection,
                limit=1000,  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜
                with_payload=True
            )
            
            for point in scroll_result[0]:
                if point.payload:
                    all_documents.append({
                        'id': point.id,
                        'payload': point.payload
                    })
            
            print(f"[DIRECT_SEARCH] ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ë¡œë“œ")
            
        except Exception as e:
            print(f"[DIRECT_SEARCH] ë¬¸ì„œ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return []
        
        if not all_documents:
            print(f"[DIRECT_SEARCH] ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        # ìœ ì‚¬ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        similarity_calc = DirectSimilarityCalculator()
        
        # ê° ì¿¼ë¦¬ì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
        all_results = []
        for query in queries:
            print(f"[DIRECT_SEARCH] ì¿¼ë¦¬ ì²˜ë¦¬: {query}")
            
            # ë¬¸ì„œë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
            query_results = similarity_calc.find_similar_documents(
                query=query,
                documents=[doc['payload'] for doc in all_documents],
                top_k=limit
            )
            
            # ê²°ê³¼ ë³€í™˜
            for i, result in enumerate(query_results):
                all_results.append({
                    'res_id': all_documents[i]['id'] if i < len(all_documents) else f"doc_{i}",
                    'res_score': result['similarity'],
                    'type_question': question_type,
                    'type_vector': 'direct_similarity',
                    'res_payload': result['document']
                })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        all_results.sort(key=lambda x: x['res_score'], reverse=True)
        final_results = all_results[:limit]
        
        print(f"[DIRECT_SEARCH] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(final_results)}ê±´")
        for i, result in enumerate(final_results[:3]):
            title = result['res_payload'].get('document_name', 'ì œëª©ì—†ìŒ')
            score = result['res_score']
            print(f"[DIRECT_SEARCH]   {i+1}. {title} (ìœ ì‚¬ë„: {score:.4f})")
        
        return final_results
        
    except Exception as e:
        print(f"[DIRECT_SEARCH] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜ë“¤ ì œê±° - ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´




# ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì„ ìœ„í•œ í´ë˜ìŠ¤
class StreamRequest(BaseModel):
    question: str
    conversation_id: Optional[int] = None
    generate_image: Optional[bool] = False  # ì´ë¯¸ì§€ ìƒì„± í”Œë˜ê·¸ ì¶”ê°€
    # LangGraph ì»¨í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ê°€
    langgraph_context: Optional[dict] = None
    include_langgraph_context: Optional[bool] = False

# ì´ë¯¸ì§€ URL (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
# SAMPLE_IMAGE_URLS = []

# LangGraph ìƒíƒœ ì •ì˜
class SearchState(dict):
    question: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    keyword: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    candidates_each: List[dict] 
    candidates_total: List[dict] 
    response: List[dict]     # LLMì´ ìƒì„±í•œ ì‘ë‹µ

# SSE ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì œë„ˆë ˆì´í„° í´ë˜ìŠ¤
class SSEGenerator:
    def __init__(self, generator_id: str):
        self.generator_id = generator_id
        self.message_queue = asyncio.Queue()
        self.is_active = True
        
    async def send_message(self, message: dict):
        """ë©”ì‹œì§€ë¥¼ íì— ì¶”ê°€"""
        if self.is_active:
            await self.message_queue.put(message)
    
    async def close(self):
        """ì œë„ˆë ˆì´í„° ì¢…ë£Œ"""
        self.is_active = False
        await self.message_queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸

# SSE ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
sse_generators = {}

# SSE ìƒíƒœ ë°œí–‰ í•¨ìˆ˜
async def yield_node_status(generator_id: str, node_name: str, status: str, data: dict):
    """SSEë¥¼ í†µí•´ ë…¸ë“œ ìƒíƒœë¥¼ ë°œí–‰"""
    if generator_id not in sse_generators:
        print(f"[SSE] âŒ ì œë„ˆë ˆì´í„° IDê°€ ì—†ìŒ - {node_name}: {status}")
        return
        
    try:
        message = {
            "stage": node_name,
            "status": status,
            "result": data,
            "timestamp": asyncio.get_event_loop().time()
        }
        
        # SSE ì œë„ˆë ˆì´í„°ì— ë©”ì‹œì§€ ì „ì†¡
        generator = sse_generators.get(generator_id)
        if generator and generator.is_active:
            await generator.send_message(message)
            print(f"[SSE] âœ… ë©”ì‹œì§€ ì „ì†¡: {node_name}:{status}")
        else:
            print(f"[SSE] âŒ ì œë„ˆë ˆì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {generator_id}")
    except Exception as e:
        print(f"[SSE] âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {node_name}:{status} - ì˜¤ë¥˜: {e}")
        import traceback
        print(f"[SSE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        pass

# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
async def node_rc_init(state: SearchState) -> SearchState:
    """ì´ˆê¸°í™” ë…¸ë“œ"""
    print("[inform]: node_rc_init")
    try: 
        question = state['question']
        generator_id = state.get('generator_id')
        
        if generator_id:
            await yield_node_status(
                generator_id,
                "A",
                "completed",
                {
                    "message": "RAG ê²€ìƒ‰ ì™„ë£Œ",
                    "documents_count": len(candidates_total),
                    "documents": candidates_total,
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ3")
                        for candidate in candidates_total
                    ],
                },
            )
        
        return {
            "question": question,
            "keyword": "",
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": generator_id
        }
    except Exception as e:
        print("[error]: node_rc_init")
        raise RuntimeError(f"[error]: node_rc_init: {str(e)}")

async def node_rc_keyword(state: SearchState) -> SearchState:
    """í‚¤ì›Œë“œ ì¦ê°• ë…¸ë“œ - LLMì„ ì‚¬ìš©í•œ ë™ì  í‚¤ì›Œë“œ ìƒì„±"""
    print("[inform]: node_rc_keyword")
    full_text_parts = []
    try:
        question = state['question']
        
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your-openai-api-key-here":
            print("[error]: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            base_keywords = [question]
            generator_id = state.get('generator_id')
            if generator_id:
                await yield_node_status(
                generator_id,
                "B",
                "completed",
                {
                    "message": "RAG ê²€ìƒ‰ ì™„ë£Œ",
                    "documents_count": len(candidates_total),
                    "documents": candidates_total,
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ3")
                        for candidate in candidates_total
                    ],
                },
            )
            
            yield {
                "question": state['question'],
                "keyword": base_keywords,
                "candidates_each": [],
                "candidates_total": [],
                "response": [],
                "generator_id": generator_id
            }
        
        try:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¦ê°• - ìƒˆë¡œìš´ LLM ë°©ì‹
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ì „ë¬¸ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³ , ìµœëŒ€ 15ê°œê¹Œì§€ ìƒì„±í•˜ì„¸ìš”."},
                {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”: {question}"}
            ]
            
            # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            httpx_client = httpx.AsyncClient(verify=False, timeout=None)

            # print(f"[messages í™•ì¸] {messages}")              
            
            # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "c.seunghoon",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
            # ë¹„ë™ê¸° í˜¸ì¶œ
            response = await client.chat.completions.create(
                model="openai/gpt-oss-120b",
                messages=messages,
                stream=True,
            )
            # ìŠ¤íŠ¸ë¦¼ ì²­í¬ ìˆ˜ì‹ 
            async for chunk in response:
                if not chunk.choices:
                    continue
                delta = chunk.choices[0].delta
                content = getattr(delta, "content", None)
                if not content:
                    continue

                # ë¶€ë¶„ ì‘ë‹µ ëˆ„ì 
                full_text_parts.append(content)

                # ë„ˆë¬´ ë¹¡ë¹¡í•œ ë£¨í”„ ë°©ì§€
                await asyncio.sleep(0)

        except Exception as e:
            print(f"[error] streaming failed: {type(e).__name__}: {e}")
            full_text_parts = []

        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ì „ì²´ ì‘ë‹µ ë¬¸ìì—´ ì¡°ë¦½
        llm_response = "".join(full_text_parts).strip()

        # âš™ï¸ í‚¤ì›Œë“œ ë³€í™˜ ë¡œì§ (ê¸°ì¡´ ë™ì¼)
        if not llm_response:
            augmented_keywords = [question]
        else:
            keywords_text = llm_response.strip()
            # ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬ ë° ì •ë¦¬
            augmented_keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
            # ì› ì§ˆë¬¸ ì¶”ê°€
            augmented_keywords.insert(0, question)
            # ì¤‘ë³µ ì œê±° ë° 20ê°œ ì œí•œ
            augmented_keywords = list(dict.fromkeys(augmented_keywords))[:20]

        print(f"[inform]: LLMì„ í†µí•´ ìƒì„±ëœ í‚¤ì›Œë“œ: {len(augmented_keywords)}ê°œ")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "B",
                "completed",
                {
                    "message": "RAG ê²€ìƒ‰ ì™„ë£Œ",
                    "documents_count": len(candidates_total),
                    "documents": candidates_total,
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ3")
                        for candidate in candidates_total
                    ],
                },
            )
        
        yield {
            "question": state['question'],
            "keyword": augmented_keywords,
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": generator_id
        }
    except Exception as e:
        print("[error]: node_rc_keyword")
        raise RuntimeError(f"[error]: node_rc_keyword: {str(e)}")

async def node_rc_rag(state: SearchState) -> SearchState:
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("[inform]: node_rc_rag")
    try:
        # ë²¡í„° DB ì„¤ì •
        ip, port, collection = QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION
        
        # ì§ì ‘ ê²€ìƒ‰ ìˆ˜í–‰
        candidates_each = []
        
        # questionìœ¼ë¡œ ê²€ìƒ‰
        if state.get('question'):
            try:
                question_results = await direct_document_search('question', 5, [state['question']], ip, port, collection)
                candidates_each.extend(question_results)
            except Exception as e:
                print(f"Question ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # keywordë¡œ ê²€ìƒ‰ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬)
        if state.get('keyword'):
            try:
                # keywordê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                if isinstance(state['keyword'], list):
                    keywords = state['keyword']
                else:
                    keywords = [state['keyword']]
                
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ í•„í„°ë§
                keywords = [k for k in keywords if k and isinstance(k, str) and k.strip()]
                
                if keywords:
                    keyword_results = await direct_document_search('keyword', 3, keywords, ip, port, collection)
                    candidates_each.extend(keyword_results)
            except Exception as e:
                print(f"Keyword ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í•˜ë“œì½”ë”© ì œê±°)
        if not candidates_each:
            print("[RAG] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ë™ì  ì ìˆ˜ ì§‘ê³„ (í•˜ë“œì½”ë”© ì œê±°)
        aggregated_scores = defaultdict(float)
        payloads = {}
        
        # candidates_eachê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if candidates_each:
            for item in candidates_each:
                try:
                    res_id = item.get('res_id')
                    score = item.get('res_score', 0.0)
                    
                    if res_id is not None and score > 0:
                        # ë‹¨ìˆœ ì ìˆ˜ í•©ì‚° (ê°€ì¤‘ì¹˜ ì œê±°)
                        aggregated_scores[res_id] += score
                        if res_id not in payloads:
                            payloads[res_id] = item.get('res_payload', {})
                except Exception as e:
                    print(f"ê°œë³„ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ì„ íƒ (ê³ ì • ê°œìˆ˜ ì œê±°)
        candidates_total = sorted(
            [{'res_id': res_id, 'res_score': aggregated_scores[res_id], 'res_payload': payloads[res_id]}
             for res_id in aggregated_scores],
            key=lambda x: x['res_score'],
            reverse=True
        )
        
        # ë™ì ìœ¼ë¡œ ê²°ê³¼ ê°œìˆ˜ ê²°ì • (ìµœì†Œ 1ê°œ, ìµœëŒ€ 10ê°œ)
        if candidates_total:
            max_results = min(len(candidates_total), max(1, min(10, len(candidates_total))))
            candidates_total = candidates_total[:max_results]
        
        print(f"[RAG] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 5ê±´):")
        for i, candidate in enumerate(candidates_total):
            title = candidate.get('res_payload', {}).get('document_name', 'ì œëª© ì—†ìŒ')
            vector_data = candidate.get("vector", {})
            # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
            summary = vector_data.get("summary_result") if isinstance(vector_data, dict) else None
            score = candidate.get('res_score', 0)
            print(f"  {i+1}. {title} - {summary} (ìœ ì‚¬ë„: {score:.4f})")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "C",
                "completed",
                {
                    "message": "RAG ê²€ìƒ‰ ì™„ë£Œ",
                    "documents_count": len(candidates_total),
                    "documents": candidates_total,
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ3")
                        for candidate in candidates_total
                    ],
                },
            )
        
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_total": candidates_total,
            "response": [],
            "generator_id": state.get('generator_id')
        }
    except Exception as e:
        print(f"[error]: node_rc_rag: {str(e)}")
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ìƒíƒœ ë°˜í™˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê³„ì† ì§„í–‰
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": state.get('generator_id')
        }

async def node_rc_rerank(state: SearchState) -> SearchState:
    """ë™ì  ì¬ìˆœìœ„ ë…¸ë“œ (í•˜ë“œì½”ë”© ì œê±°)"""
    print("[inform]: node_rc_rerank")
    try:
        candidates_top = state['candidates_total']
        
        if not candidates_top:
            print("[RERANK] ì¬ìˆœìœ„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "question": state['question'],
                "keyword": state["keyword"],
                "candidates_total": state["candidates_total"],
                "response": [],
                "generator_id": state.get('generator_id')
            }
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ë™ì  ì¬ìˆœìœ„ (í•˜ë“œì½”ë”©ëœ 0.1 ê°ì†Œ ì œê±°)
        similarity_calc = DirectSimilarityCalculator()
        question = state['question']
        
        for candidate in candidates_top:
            try:
                # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                payload = candidate.get('res_payload', {})
                doc_title = payload.get('document_name', '')
                vector_data = payload.get("vector", {})
                # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
                doc_content = vector_data.get("text") if isinstance(vector_data, dict) else None

                doc_text = f"{doc_title} {doc_content}"
                
                # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê°„ ì§ì ‘ ìœ ì‚¬ë„ ê³„ì‚°
                relevance_score = similarity_calc.calculate_combined_similarity(question, doc_text)
                
                # ê¸°ì¡´ ê²€ìƒ‰ ì ìˆ˜ì™€ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê²°í•©
                original_score = candidate.get('res_score', 0.0)
                combined_score = (original_score * 0.6) + (relevance_score * 0.4)
                
                candidate.update({
                    'res_relevance': relevance_score,
                    'combined_score': combined_score
                })
                
            except Exception as e:
                print(f"[RERANK] ê°œë³„ ë¬¸ì„œ ì¬ìˆœìœ„ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                candidate.update({
                    'res_relevance': candidate.get('res_score', 0.0),
                    'combined_score': candidate.get('res_score', 0.0)
                })
        
        # ê²°í•© ì ìˆ˜ë¡œ ì •ë ¬ (í•˜ë“œì½”ë”©ëœ ì •ë ¬ ë°©ì‹ ì œê±°)
        sorted_candidates_top = sorted(candidates_top, key=lambda x: x.get('combined_score', 0), reverse=True)
        
        print(f"[RERANK] ì¬ìˆœìœ„ ì™„ë£Œ: {len(sorted_candidates_top)}ê°œ ë¬¸ì„œ")
        for i, candidate in enumerate(sorted_candidates_top[:3]):
            title = candidate.get('res_payload', {}).get('document_name', 'ì œëª©ì—†ìŒ')
            combined_score = candidate.get('combined_score', 0)
            relevance = candidate.get('res_relevance', 0)
            print(f"[RERANK]   {i+1}. {title} (ê²°í•©ì ìˆ˜: {combined_score:.4f}, ê´€ë ¨ì„±: {relevance:.4f})")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "C",
                "completed",
                {
                    "message": "RAG ê²€ìƒ‰ ì™„ë£Œ",
                    "documents_count": len(candidates_total),
                    "documents": candidates_total,
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ3")
                        for candidate in candidates_total
                    ],
                },
            )
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": sorted_candidates_top,
            "generator_id": state.get('generator_id')
        }
    except Exception as e:
        print("[error]: node_rc_rerank")
        raise RuntimeError(f"[error]: node_rc_rerank: {str(e)}")

async def node_rc_answer(state: SearchState) -> SearchState:
    """ë‹µë³€ ìƒì„± ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_answer ì‹¤í–‰")
    
    try:
        candidates_top = state['response']
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if candidates_top:
            print(f"[Answer] âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. LLM API í˜¸ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ìƒìœ„ 1ê±´ì˜ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            top_result = candidates_top[0]
            top_payload = top_result.get('res_payload', {})
            
            # ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
            document_title = top_payload.get('document_name', 'ì œëª© ì—†ìŒ')
            vector_data = top_payload.get("vector", {})

            # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
            document_content = vector_data.get("text") if isinstance(vector_data, dict) else None
            
            print(f"[Answer] ğŸ“„ RAG ë¬¸ì„œ ì •ë³´:")
            print(f"[Answer] ì œëª©: {document_title}")
            print(f"[Answer] ë‚´ìš© ê¸¸ì´: {len(document_content)} ë¬¸ì")
            print(f"[Answer] ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {str(document_content)[:200]}...")
            
            # LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {str(document_content)[:1000]}...

[ì§ˆë¬¸]
{state['question']}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- í˜•ì‹ì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
            """.strip()
            
            print(f"[Answer] ğŸ“ LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸:")
            print(f"[Answer] {prompt}")
            print(f"[Answer] ğŸ“Š í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            
            # OpenAI API í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„± - ìƒˆë¡œìš´ LLM ë°©ì‹
            llm_answer = ""
            try:
                if OPENAI_API_KEY:
                    print(f"[Answer] ğŸš€ LLM API í˜¸ì¶œ ì‹œì‘...")
                    
                    messages = [{"role": "user", "content": prompt}]
                    
                    # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
                    httpx_client = httpx.AsyncClient(verify=False, timeout=None)

                    # print(f"[messages í™•ì¸] {messages}")               

                    # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    client = AsyncOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        http_client=httpx_client,
                        default_headers={
                            "x-dep-ticket": OPENAI_API_KEY,
                            "Send-System-Name": "ds2llm",
                            "User-Id": "c.seunghoon",
                            "User-Type": "AD_ID",
                            "Prompt-Msg-Id": str(uuid.uuid4()),
                            "Completion-Msg-Id": str(uuid.uuid4()),
                        }
                    )
                    
                    # ë¹„ë™ê¸° í˜¸ì¶œ
                    response = await client.chat.completions.create(
                        model="openai/gpt-oss-120b",
                        messages=messages,
                        stream=True,
                    )
                    async for chunk in response:              # ì´ì œ chunkëŠ” OpenAIObject
                        delta = chunk.choices[0].delta
                        content = delta.content
                        # print(content)
                    # for chunk in response:
                    #     if chunk.choices[0].delta.get("content"):
                    #         content = chunk.choices[0].delta.content
                        try:
                            # ë¹„-ASCII ë¬¸ì í—ˆìš©, UTF-8 bytes ë¡œ ì¦‰ì‹œ ì „ì†¡
                            payload = json.dumps({'content': content}, ensure_ascii=False)
                            yield (f"data: {payload}\n\n").encode("utf-8")

                            await asyncio.sleep(0.01)
                            # ì²­í¬ ì‚¬ì´ì— ì§€ì—° ì¶”ê°€í•˜ì—¬ ë‹¤ë¥¸ API ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•¨
                            await asyncio.sleep(0.01)
                        except (ConnectionResetError, BrokenPipeError, OSError, ConnectionAbortedError, ConnectionError) as e:
                            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
                            print(f"Client disconnected during streaming lv2: {type(e).__name__}")
                            return
                        except Exception as e:
                            print(f"Unexpected error during streaming lv1: {str(e)}")
                            return
                else:
                    print(f"[Answer] âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {str(document_content)[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
                    
            except Exception as e:
                print(f"[Answer] âŒ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                import traceback
                print(f"[Answer] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€ ìƒì„±
                llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {str(document_content)[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"""
            
            print(f"[Answer] ğŸ¯ ìµœì¢… ìƒì„±ëœ ë‹µë³€:")
            print(f"[Answer] {llm_answer}")
            
            # ì´ë¯¸ì§€ URL ìƒì„± (ì²« ë²ˆì§¸ ë¬¸ì„œ ê¸°ë°˜)
            image_url = None
            if top_result and top_payload:
                # ë¬¸ì„œ ì œëª©ê³¼ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ URL ìƒì„±
                doc_title = top_payload.get('document_name', '')
                doc_index = top_result.get('res_id', 0)
                
                if doc_title and doc_index:
                    # ì„¤ì • ê°€ëŠ¥í•œ ê¸°ë³¸ URLì— RAG ë¬¸ì„œ ì •ë³´ë¥¼ ì¶”ê°€í•œ í˜•ì‹
                    # URL ì•ˆì „í•œ ë¬¸ìì—´ë¡œ ë³€í™˜
                    import urllib.parse
                    safe_title = urllib.parse.quote(doc_title, safe='')
                    base = os.path.splitext(safe_title)[0]
                    # ìƒˆ í™•ì¥ì ì¶”ê°€
                    new_filename = f"{base}_whole.jpg"
                    image_url = f"{IMAGE_BASE_URL}{IMAGE_PATH_PREFIX}/{safe_title}"
                    print(f"[Answer] ğŸ–¼ï¸ ìƒì„±ëœ ì´ë¯¸ì§€ URL: {image_url}")
                    print(f"[Answer] ğŸ”§ ì´ë¯¸ì§€ URL êµ¬ì„± - ê¸°ë³¸URL: {IMAGE_BASE_URL}, ê²½ë¡œ: {IMAGE_PATH_PREFIX}, ì œëª©: {safe_title}, ì¸ë±ìŠ¤: {doc_index}")
                else:
                    print(f"[Answer] âš ï¸ ì´ë¯¸ì§€ URL ìƒì„± ì‹¤íŒ¨ - doc_title: {doc_title}, doc_index: {doc_index}")

            # LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ ìœ„í•œ ì™„ì „í•œ ì‘ë‹µ êµ¬ì¡°
            response = {
                "res_id": [rest['res_id'] for rest in candidates_top],
                "answer": llm_answer,  # LLMìœ¼ë¡œ ìƒì„±ëœ ì‹¤ì œ ë‹µë³€
                "q_mode": "search",  # ë­ê·¸ë˜í”„ëŠ” í•­ìƒ search ëª¨ë“œ
                "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
                "db_search_title": [item.get('res_payload', {}).get('document_name', 'ì œëª© ì—†ìŒ') for item in candidates_top],  # ê²€ìƒ‰ëœ ë¬¸ì„œ ì œëª©ë“¤
                "top_document": top_result,
                "analysis_image_url": image_url  # ë­ê·¸ë˜í”„ 4ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ ì´ë¯¸ì§€ URL
            }
        else:
            print(f"[Answer] âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            response = {
                "res_id": [],
                "answer": f"ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "q_mode": "search",
                "keyword": state["keyword"],
                "db_search_title": []
            }
        
        print(f"[Answer] ğŸ“¤ ìµœì¢… ì‘ë‹µ êµ¬ì¡°:")
        print(f"[Answer] {response}")
        
        # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
        # await save_langgraph_result_to_db(state['question'], response, state["keyword"], state["candidates_total"])
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "D",
                "completed",
                {
                    "message": "ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ",
                    "answer": response.get("answer", ""),
                    "analysis_image_url": response.get("analysis_image_url"),
                    "keywords": response.get("keyword", state.get("keyword", [])),
                    "document_titles": response.get("db_search_title", []),
                    "search_results": candidates_top,
                    "top_document": response.get("top_document"),
                },
            )
        
        print(f"[Answer] âœ… node_rc_answer í•¨ìˆ˜ ì™„ë£Œ")
        
        yield  {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": response,
            "generator_id": state.get('generator_id')
        }
        
    except Exception as e:
        print("[error]: node_rc_answer")
        import traceback
        print(f"[error] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise RuntimeError(f"[error]: node_rc_answer: {str(e)}")

async def node_rc_plain_answer(state: SearchState) -> SearchState:
    """ê¸°ë³¸ ë‹µë³€ ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_plain_answer ì‹¤í–‰")
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë” êµ¬ì²´ì ì¸ ì•ˆë‚´ ë©”ì‹œì§€ ìƒì„±
    question = state['question']
    keywords = state.get('keyword', [])
    
    if isinstance(keywords, list) and len(keywords) > 1:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}"
    else:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}"
    
    detailed_answer = f"""ğŸ” **ë¶„ì„ ê²°ê³¼ ìš”ì•½**

**ì…ë ¥ ì§ˆë¬¸**: {question}

**í‚¤ì›Œë“œ ì¦ê°•**: {keyword_info}

**ê²€ìƒ‰ ê²°ê³¼**: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ê°œì„  ì œì•ˆ**:
1. **ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±**í•´ì£¼ì„¸ìš”
   - ì˜ˆ: "ì„±ê³¼ ê°œì„ " â†’ "2024ë…„ 1ë¶„ê¸° ì˜ì—…íŒ€ ì„±ê³¼ ê°œì„  ë°©ì•ˆ"
   - ì˜ˆ: "ì „ëµ ìˆ˜ë¦½" â†’ "ì‹ ì œí’ˆ ì¶œì‹œë¥¼ ìœ„í•œ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½"

2. **ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ê°€**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ í‚¤ì›Œë“œ: {keywords[:3] if isinstance(keywords, list) else keywords}
   - ì¶”ê°€ í‚¤ì›Œë“œ ì˜ˆì‹œ: êµ¬ì²´ì ì¸ ì—…ë¬´ ì˜ì—­, ê¸°ê°„, ë¶€ì„œëª… ë“±

3. **ë°ì´í„°ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ ì„¤ì •ëœ ë²¡í„° DB: {QDRANT_HOST}:{QDRANT_PORT}
   - ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}

ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤."""
    
    # Redisë¥¼ í†µí•´ ì™„ë£Œ ìƒíƒœ ë°œí–‰ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ DB ì €ì¥ì„ ìœ„í•´)
    complete_result = {
        "res_id": [], 
        "answer": detailed_answer,
        "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
        "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
        "db_search_title": []  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    }
    
    # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
    # await save_langgraph_result_to_db(state['question'], complete_result, state["keyword"], state["candidates_total"])
    
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
                generator_id,
                "D",
                "completed",
                {
                    "message": "ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ",
                    "answer": response.get("answer", ""),
                    "analysis_image_url": response.get("analysis_image_url"),
                    "keywords": response.get("keyword", state.get("keyword", [])),
                    "document_titles": response.get("db_search_title", []),
                    "search_results": candidates_top,
                    "top_document": response.get("top_document"),
                },
            )
    
    return {
        "question": state['question'],
        "keyword": state["keyword"],
        "candidates_total": state["candidates_total"],
        "response": {
            "res_id": [], 
            "answer": detailed_answer,
            "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
            "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
            "db_search_title": []
        },
        "generator_id": state.get('generator_id')
    }

def judge_rc_ragscore(state: SearchState) -> str:
    """ë™ì  RAG ì ìˆ˜ íŒë‹¨ (í•˜ë“œì½”ë”©ëœ ì„ê³„ê°’ ì œê±°)"""
    candidates_total = state["candidates_total"]
    
    if not candidates_total:
        return "N"
    
    # ë™ì  ì„ê³„ê°’ ê³„ì‚°: í‰ê·  ì ìˆ˜ì˜ 70%ë¥¼ ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš©
    scores = [candidate.get("res_score", 0) for candidate in candidates_total]
    valid_scores = [score for score in scores if score > 0]
    
    if not valid_scores:
        return "N"
    
    avg_score = sum(valid_scores) / len(valid_scores)
    dynamic_threshold = avg_score * 0.7
    
    # ìµœì†Œ ì„ê³„ê°’ ì„¤ì • (ë„ˆë¬´ ë‚®ì€ ì ìˆ˜ëŠ” ì œì™¸)
    min_threshold = 0.1
    threshold = max(dynamic_threshold, min_threshold)
    
    has_good_results = any(candidate.get("res_score", 0) >= threshold for candidate in candidates_total)
    
    print(f"[JUDGE] ë™ì  ì„ê³„ê°’: {threshold:.4f} (í‰ê· : {avg_score:.4f})")
    print(f"[JUDGE] íŒë‹¨ ê²°ê³¼: {'Y' if has_good_results else 'N'}")
    
    return "Y" if has_good_results else "N"
