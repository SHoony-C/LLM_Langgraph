import openai
from openai import AsyncOpenAI
import asyncio
import json
import httpx
import uuid
from fastapi import APIRouter, HTTPException, Response, Depends, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from langgraph.graph import END, StateGraph
from collections import defaultdict
from qdrant_client import QdrantClient
from app.utils.config import (
    OPENAI_API_KEY, OPENAI_BASE_URL,
    QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION,
    IMAGE_BASE_URL, IMAGE_PATH_PREFIX
)
from app.database import get_db
from app.models import Conversation, Message
from sqlalchemy.orm import Session

from datetime import datetime

# Create router 
router = APIRouter()

# Redis ì œê±°ë¨ - SSE ë°©ì‹ ì‚¬ìš©

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”© í™•ì¸
print(f"[Config] OpenAI API Key: {'ì„¤ì •ë¨' if OPENAI_API_KEY else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
print(f"[Config] Qdrant: {QDRANT_HOST}:{QDRANT_PORT} (ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'})")

# ì§ì ‘ êµ¬í˜„í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° í´ë˜ìŠ¤
class DirectSimilarityCalculator:
    def __init__(self):
        self.stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 'í•œ', 'í•˜ëŠ”', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê·¸', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ'}
        
    def preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”"""
        import re
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text.lower())
        # í† í°í™”
        tokens = text.split()
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ 2 ì´ìƒ í† í°ë§Œ ìœ ì§€
        tokens = [token for token in tokens if token not in self.stopwords and len(token) >= 2]
        return tokens
    
    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        tokens1 = set(self.preprocess_text(text1))
        tokens2 = set(self.preprocess_text(text2))
        
        if not tokens1 and not tokens2:
            return 1.0
        if not tokens1 or not tokens2:
            return 0.0
            
        intersection = len(tokens1.intersection(tokens2))
        union = len(tokens1.union(tokens2))
        
        return intersection / union if union > 0 else 0.0
    
    def calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (TF ê¸°ë°˜)"""
        tokens1 = self.preprocess_text(text1)
        tokens2 = self.preprocess_text(text2)
        
        # TF ê³„ì‚°
        tf1 = {}
        tf2 = {}
        
        for token in tokens1:
            tf1[token] = tf1.get(token, 0) + 1
        for token in tokens2:
            tf2[token] = tf2.get(token, 0) + 1
        
        # ì „ì²´ ë‹¨ì–´ ì§‘í•©
        all_tokens = set(tokens1 + tokens2)
        
        if not all_tokens:
            return 1.0
        
        # ë²¡í„° ìƒì„±
        vector1 = [tf1.get(token, 0) for token in all_tokens]
        vector2 = [tf2.get(token, 0) for token in all_tokens]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        dot_product = sum(a * b for a, b in zip(vector1, vector2))
        magnitude1 = sum(a * a for a in vector1) ** 0.5
        magnitude2 = sum(b * b for b in vector2) ** 0.5
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
            
        return dot_product / (magnitude1 * magnitude2)
    
    def calculate_combined_similarity(self, text1: str, text2: str) -> float:
        """ìì¹´ë“œì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ ìµœì¢… ìœ ì‚¬ë„"""
        jaccard_sim = self.calculate_jaccard_similarity(text1, text2)
        cosine_sim = self.calculate_cosine_similarity(text1, text2)
        
        # ê°€ì¤‘ í‰ê·  (ìì¹´ë“œ 0.4, ì½”ì‚¬ì¸ 0.6)
        combined_sim = (jaccard_sim * 0.4) + (cosine_sim * 0.6)
        return combined_sim
    
    def find_similar_documents(self, query: str, documents: List[dict], top_k: int = 5) -> List[dict]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ì°¾ì•„ ë°˜í™˜"""
        results = []
        
        for doc in documents:
            # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì œëª© + ë‚´ìš©)
            doc_title = doc.get('ppt_title', '')
            doc_content = doc.get('ppt_content', doc.get('ppt_summary', ''))
            doc_text = f"{doc_title} {doc_content}"
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = self.calculate_combined_similarity(query, doc_text)
            
            results.append({
                'document': doc,
                'similarity': similarity,
                'matched_text': doc_text[:200] + '...' if len(doc_text) > 200 else doc_text
            })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]

# ì§ì ‘ êµ¬í˜„í•œ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
async def direct_document_search(question_type: str, limit: int, queries: List[str], 
                               ip: str, port: int, collection: str) -> List[dict]:
    """ì§ì ‘ êµ¬í˜„í•œ ë¬¸ì„œ ê²€ìƒ‰ (ìœ ì‚¬ë„ ê¸°ë°˜)"""
    try:
        print(f"[DIRECT_SEARCH] ì§ì ‘ ê²€ìƒ‰ ì‹œì‘: {len(queries)}ê°œ ì¿¼ë¦¬")
        
        # Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        client = QdrantClient(host=ip, port=port)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        try:
            collections = client.get_collections()
            print(f"[DIRECT_SEARCH] ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {[col.name for col in collections.collections]}")
            
            if not collection or collection not in [col.name for col in collections.collections]:
                print(f"[DIRECT_SEARCH] ì»¬ë ‰ì…˜ '{collection}' ì‚¬ìš© ë¶ˆê°€")
                return []
                
        except Exception as e:
            print(f"[DIRECT_SEARCH] Qdrant ì—°ê²° ì˜¤ë¥˜: {e}")
            return []
        
        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë¡¤ ë°©ì‹)
        all_documents = []
        try:
            scroll_result = client.scroll(
                collection_name=collection,
                limit=1000,  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜
                with_payload=True
            )
            
            for point in scroll_result[0]:
                if point.payload:
                    all_documents.append({
                        'id': point.id,
                        'payload': point.payload
                    })
            
            print(f"[DIRECT_SEARCH] ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ë¡œë“œ")
            
        except Exception as e:
            print(f"[DIRECT_SEARCH] ë¬¸ì„œ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return []
        
        if not all_documents:
            print(f"[DIRECT_SEARCH] ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        # ìœ ì‚¬ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        similarity_calc = DirectSimilarityCalculator()
        
        # ê° ì¿¼ë¦¬ì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
        all_results = []
        for query in queries:
            print(f"[DIRECT_SEARCH] ì¿¼ë¦¬ ì²˜ë¦¬: {query}")
            
            # ë¬¸ì„œë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
            query_results = similarity_calc.find_similar_documents(
                query=query,
                documents=[doc['payload'] for doc in all_documents],
                top_k=limit
            )
            
            # ê²°ê³¼ ë³€í™˜
            for i, result in enumerate(query_results):
                all_results.append({
                    'res_id': all_documents[i]['id'] if i < len(all_documents) else f"doc_{i}",
                    'res_score': result['similarity'],
                    'type_question': question_type,
                    'type_vector': 'direct_similarity',
                    'res_payload': result['document']
                })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        all_results.sort(key=lambda x: x['res_score'], reverse=True)
        final_results = all_results[:limit]
        
        print(f"[DIRECT_SEARCH] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(final_results)}ê±´")
        for i, result in enumerate(final_results[:3]):
            title = result['res_payload'].get('ppt_title', 'ì œëª©ì—†ìŒ')
            score = result['res_score']
            print(f"[DIRECT_SEARCH]   {i+1}. {title} (ìœ ì‚¬ë„: {score:.4f})")
        
        return final_results
        
    except Exception as e:
        print(f"[DIRECT_SEARCH] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜ë“¤ ì œê±° - ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´




# ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì„ ìœ„í•œ í´ë˜ìŠ¤
class StreamRequest(BaseModel):
    question: str
    conversation_id: Optional[int] = None
    generate_image: Optional[bool] = False  # ì´ë¯¸ì§€ ìƒì„± í”Œë˜ê·¸ ì¶”ê°€
    # LangGraph ì»¨í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ê°€
    langgraph_context: Optional[dict] = None
    include_langgraph_context: Optional[bool] = False

# ì´ë¯¸ì§€ URL (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
# SAMPLE_IMAGE_URLS = []

# LangGraph ìƒíƒœ ì •ì˜
class SearchState(dict):
    question: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    keyword: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    candidates_each: List[dict] 
    candidates_total: List[dict] 
    response: List[dict]     # LLMì´ ìƒì„±í•œ ì‘ë‹µ

# SSE ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì œë„ˆë ˆì´í„° í´ë˜ìŠ¤
class SSEGenerator:
    def __init__(self, generator_id: str):
        self.generator_id = generator_id
        self.message_queue = asyncio.Queue()
        self.is_active = True
        
    async def send_message(self, message: dict):
        """ë©”ì‹œì§€ë¥¼ íì— ì¶”ê°€"""
        if self.is_active:
            await self.message_queue.put(message)
    
    async def close(self):
        """ì œë„ˆë ˆì´í„° ì¢…ë£Œ"""
        self.is_active = False
        await self.message_queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸

# SSE ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
sse_generators = {}

# SSE ìƒíƒœ ë°œí–‰ í•¨ìˆ˜
async def yield_node_status(generator_id: str, node_name: str, status: str, data: dict):
    """SSEë¥¼ í†µí•´ ë…¸ë“œ ìƒíƒœë¥¼ ë°œí–‰"""
    if generator_id not in sse_generators:
        print(f"[SSE] âŒ ì œë„ˆë ˆì´í„° IDê°€ ì—†ìŒ - {node_name}: {status}")
        return
        
    try:
        message = {
            "stage": node_name,
            "status": status,
            "result": data,
            "timestamp": asyncio.get_event_loop().time()
        }
        
        # SSE ì œë„ˆë ˆì´í„°ì— ë©”ì‹œì§€ ì „ì†¡
        generator = sse_generators.get(generator_id)
        if generator and generator.is_active:
            await generator.send_message(message)
            print(f"[SSE] âœ… ë©”ì‹œì§€ ì „ì†¡: {node_name}:{status}")
        else:
            print(f"[SSE] âŒ ì œë„ˆë ˆì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {generator_id}")
    except Exception as e:
        print(f"[SSE] âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {node_name}:{status} - ì˜¤ë¥˜: {e}")
        import traceback
        print(f"[SSE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        pass

# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
async def node_rc_init(state: SearchState) -> SearchState:
    """ì´ˆê¸°í™” ë…¸ë“œ"""
    print("[inform]: node_rc_init")
    try: 
        question = state['question']
        generator_id = state.get('generator_id')
        
        if generator_id:
            await yield_node_status(generator_id, "A", "completed", {"message": "ì…ë ¥ ì •ë¦¬ ì™„ë£Œ", "question": question})
        
        return {
            "question": question,
            "keyword": "",
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": generator_id
        }
    except Exception as e:
        print("[error]: node_rc_init")
        raise RuntimeError(f"[error]: node_rc_init: {str(e)}")

async def node_rc_keyword(state: SearchState) -> SearchState:
    """í‚¤ì›Œë“œ ì¦ê°• ë…¸ë“œ - LLMì„ ì‚¬ìš©í•œ ë™ì  í‚¤ì›Œë“œ ìƒì„±"""
    print("[inform]: node_rc_keyword")
    try:
        question = state['question']
        
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your-openai-api-key-here":
            print("[error]: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            base_keywords = [question]
            generator_id = state.get('generator_id')
            if generator_id:
                await yield_node_status(generator_id, "B", "completed", {"message": "í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ", "keywords": base_keywords})
            
            return {
                "question": state['question'],
                "keyword": base_keywords,
                "candidates_each": [],
                "candidates_total": [],
                "response": [],
                "generator_id": generator_id
            }
        
        try:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¦ê°• - ìƒˆë¡œìš´ LLM ë°©ì‹
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ì „ë¬¸ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³ , ìµœëŒ€ 15ê°œê¹Œì§€ ìƒì„±í•˜ì„¸ìš”."},
                {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”: {question}"}
            ]
            
            # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            httpx_client = httpx.AsyncClient(verify=False, timeout=None)
            
            # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "langgraph_keyword",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
            # ë¹„ë™ê¸° í˜¸ì¶œ
            response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            llm_response = response.choices[0].message.content
            
            # LLM ì‘ë‹µì„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            keywords_text = llm_response.strip()
            # ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³  ê° í‚¤ì›Œë“œ ì •ë¦¬
            augmented_keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            # ì›ë³¸ ì§ˆë¬¸ë„ í¬í•¨
            augmented_keywords.insert(0, question)
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 20ê°œë¡œ ì œí•œ
            augmented_keywords = list(dict.fromkeys(augmented_keywords))[:20]
            
            print(f"[inform]: LLMì„ í†µí•´ ìƒì„±ëœ í‚¤ì›Œë“œ: {len(augmented_keywords)}ê°œ")
            
        except Exception as llm_error:
            print(f"[error]: LLM í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {llm_error}")
            # LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©
            augmented_keywords = [question]
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(generator_id, "B", "completed", {"message": "LLM í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ", "keywords": augmented_keywords})
        
        return {
            "question": state['question'],
            "keyword": augmented_keywords,
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": generator_id
        }
    except Exception as e:
        print("[error]: node_rc_keyword")
        raise RuntimeError(f"[error]: node_rc_keyword: {str(e)}")

async def node_rc_rag(state: SearchState) -> SearchState:
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("[inform]: node_rc_rag")
    try:
        # ë²¡í„° DB ì„¤ì •
        ip, port, collection = QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION
        
        # ì§ì ‘ ê²€ìƒ‰ ìˆ˜í–‰
        candidates_each = []
        
        # questionìœ¼ë¡œ ê²€ìƒ‰
        if state.get('question'):
            try:
                question_results = await direct_document_search('question', 5, [state['question']], ip, port, collection)
                candidates_each.extend(question_results)
            except Exception as e:
                print(f"Question ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # keywordë¡œ ê²€ìƒ‰ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬)
        if state.get('keyword'):
            try:
                # keywordê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                if isinstance(state['keyword'], list):
                    keywords = state['keyword']
                else:
                    keywords = [state['keyword']]
                
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ í•„í„°ë§
                keywords = [k for k in keywords if k and isinstance(k, str) and k.strip()]
                
                if keywords:
                    keyword_results = await direct_document_search('keyword', 3, keywords, ip, port, collection)
                    candidates_each.extend(keyword_results)
            except Exception as e:
                print(f"Keyword ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í•˜ë“œì½”ë”© ì œê±°)
        if not candidates_each:
            print("[RAG] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ë™ì  ì ìˆ˜ ì§‘ê³„ (í•˜ë“œì½”ë”© ì œê±°)
        aggregated_scores = defaultdict(float)
        payloads = {}
        
        # candidates_eachê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if candidates_each:
            for item in candidates_each:
                try:
                    res_id = item.get('res_id')
                    score = item.get('res_score', 0.0)
                    
                    if res_id is not None and score > 0:
                        # ë‹¨ìˆœ ì ìˆ˜ í•©ì‚° (ê°€ì¤‘ì¹˜ ì œê±°)
                        aggregated_scores[res_id] += score
                        if res_id not in payloads:
                            payloads[res_id] = item.get('res_payload', {})
                except Exception as e:
                    print(f"ê°œë³„ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ì„ íƒ (ê³ ì • ê°œìˆ˜ ì œê±°)
        candidates_total = sorted(
            [{'res_id': res_id, 'res_score': aggregated_scores[res_id], 'res_payload': payloads[res_id]}
             for res_id in aggregated_scores],
            key=lambda x: x['res_score'],
            reverse=True
        )
        
        # ë™ì ìœ¼ë¡œ ê²°ê³¼ ê°œìˆ˜ ê²°ì • (ìµœì†Œ 1ê°œ, ìµœëŒ€ 10ê°œ)
        if candidates_total:
            max_results = min(len(candidates_total), max(1, min(10, len(candidates_total))))
            candidates_total = candidates_total[:max_results]
        
        print(f"[RAG] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 5ê±´):")
        for i, candidate in enumerate(candidates_total):
            title = candidate.get('res_payload', {}).get('ppt_title', 'ì œëª© ì—†ìŒ')
            summary = candidate.get('res_payload', {}).get('ppt_summary', 'ìš”ì•½ ì—†ìŒ')
            score = candidate.get('res_score', 0)
            print(f"  {i+1}. {title} - {summary} (ìœ ì‚¬ë„: {score:.4f})")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(generator_id, "C", "completed", {"message": "RAG ê²€ìƒ‰ ì™„ë£Œ", "documents_count": len(candidates_total)})
        
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_total": candidates_total,
            "response": [],
            "generator_id": state.get('generator_id')
        }
    except Exception as e:
        print(f"[error]: node_rc_rag: {str(e)}")
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ìƒíƒœ ë°˜í™˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê³„ì† ì§„í–‰
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": state.get('generator_id')
        }

async def node_rc_rerank(state: SearchState) -> SearchState:
    """ë™ì  ì¬ìˆœìœ„ ë…¸ë“œ (í•˜ë“œì½”ë”© ì œê±°)"""
    print("[inform]: node_rc_rerank")
    try:
        candidates_top = state['candidates_total']
        
        if not candidates_top:
            print("[RERANK] ì¬ìˆœìœ„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "question": state['question'],
                "keyword": state["keyword"],
                "candidates_total": state["candidates_total"],
                "response": [],
                "generator_id": state.get('generator_id')
            }
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ë™ì  ì¬ìˆœìœ„ (í•˜ë“œì½”ë”©ëœ 0.1 ê°ì†Œ ì œê±°)
        similarity_calc = DirectSimilarityCalculator()
        question = state['question']
        
        for candidate in candidates_top:
            try:
                # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                payload = candidate.get('res_payload', {})
                doc_title = payload.get('ppt_title', '')
                doc_content = payload.get('ppt_content', payload.get('ppt_summary', ''))
                doc_text = f"{doc_title} {doc_content}"
                
                # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê°„ ì§ì ‘ ìœ ì‚¬ë„ ê³„ì‚°
                relevance_score = similarity_calc.calculate_combined_similarity(question, doc_text)
                
                # ê¸°ì¡´ ê²€ìƒ‰ ì ìˆ˜ì™€ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê²°í•©
                original_score = candidate.get('res_score', 0.0)
                combined_score = (original_score * 0.6) + (relevance_score * 0.4)
                
                candidate.update({
                    'res_relevance': relevance_score,
                    'combined_score': combined_score
                })
                
            except Exception as e:
                print(f"[RERANK] ê°œë³„ ë¬¸ì„œ ì¬ìˆœìœ„ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                candidate.update({
                    'res_relevance': candidate.get('res_score', 0.0),
                    'combined_score': candidate.get('res_score', 0.0)
                })
        
        # ê²°í•© ì ìˆ˜ë¡œ ì •ë ¬ (í•˜ë“œì½”ë”©ëœ ì •ë ¬ ë°©ì‹ ì œê±°)
        sorted_candidates_top = sorted(candidates_top, key=lambda x: x.get('combined_score', 0), reverse=True)
        
        print(f"[RERANK] ì¬ìˆœìœ„ ì™„ë£Œ: {len(sorted_candidates_top)}ê°œ ë¬¸ì„œ")
        for i, candidate in enumerate(sorted_candidates_top[:3]):
            title = candidate.get('res_payload', {}).get('ppt_title', 'ì œëª©ì—†ìŒ')
            combined_score = candidate.get('combined_score', 0)
            relevance = candidate.get('res_relevance', 0)
            print(f"[RERANK]   {i+1}. {title} (ê²°í•©ì ìˆ˜: {combined_score:.4f}, ê´€ë ¨ì„±: {relevance:.4f})")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(generator_id, "C", "completed", {"message": "ë™ì  ì¬ìˆœìœ„ ì™„ë£Œ", "top_documents": len(sorted_candidates_top)})
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": sorted_candidates_top,
            "generator_id": state.get('generator_id')
        }
    except Exception as e:
        print("[error]: node_rc_rerank")
        raise RuntimeError(f"[error]: node_rc_rerank: {str(e)}")

async def node_rc_answer(state: SearchState) -> SearchState:
    """ë‹µë³€ ìƒì„± ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_answer ì‹¤í–‰")
    
    try:
        candidates_top = state['response']
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if candidates_top:
            print(f"[Answer] âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. LLM API í˜¸ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ìƒìœ„ 1ê±´ì˜ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            top_result = candidates_top[0]
            top_payload = top_result.get('res_payload', {})
            
            # ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
            document_title = top_payload.get('ppt_title', 'ì œëª© ì—†ìŒ')
            document_content = top_payload.get('ppt_content', top_payload.get('ppt_summary', 'ë‚´ìš© ì—†ìŒ'))
            
            print(f"[Answer] ğŸ“„ RAG ë¬¸ì„œ ì •ë³´:")
            print(f"[Answer] ì œëª©: {document_title}")
            print(f"[Answer] ë‚´ìš© ê¸¸ì´: {len(document_content)} ë¬¸ì")
            print(f"[Answer] ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {document_content[:200]}...")
            
            # LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {document_content[:1000]}...

[ì§ˆë¬¸]
{state['question']}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- í˜•ì‹ì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
            """.strip()
            
            print(f"[Answer] ğŸ“ LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸:")
            print(f"[Answer] {prompt}")
            print(f"[Answer] ğŸ“Š í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            
            # OpenAI API í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„± - ìƒˆë¡œìš´ LLM ë°©ì‹
            llm_answer = ""
            try:
                if OPENAI_API_KEY:
                    print(f"[Answer] ğŸš€ LLM API í˜¸ì¶œ ì‹œì‘...")
                    
                    messages = [{"role": "user", "content": prompt}]
                    
                    # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
                    httpx_client = httpx.AsyncClient(verify=False, timeout=None)
                    
                    # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    client = AsyncOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        http_client=httpx_client,
                        default_headers={
                            "x-dep-ticket": OPENAI_API_KEY,
                            "Send-System-Name": "ds2llm",
                            "User-Id": "langgraph_answer",
                            "User-Type": "AD_ID",
                            "Prompt-Msg-Id": str(uuid.uuid4()),
                            "Completion-Msg-Id": str(uuid.uuid4()),
                        }
                    )
                    
                    # ë¹„ë™ê¸° í˜¸ì¶œ
                    response = await client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=messages,
                        max_tokens=1000,
                        temperature=0.7
                    )
                    raw_llm_answer = response.choices[0].message.content
                    print(f"[Answer] âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                    print(f"[Answer] ğŸ“¥ LLM ì›ì‹œ ì‘ë‹µ:")
                    print(f"[Answer] {raw_llm_answer}")
                    print(f"[Answer] ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(raw_llm_answer)} ë¬¸ì")
                    print(f"[Answer] ì‚¬ìš©ëœ í† í°: {response.usage.total_tokens if response.usage else 'N/A'}")
                    
                    # LLMì—ì„œ ë°›ì€ ê¹”ë”í•œ ë‹µë³€ì„ ë°”ë¡œ ì‚¬ìš©
                    llm_answer = raw_llm_answer.strip()
                else:
                    print(f"[Answer] âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {document_content[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
                    
            except Exception as e:
                print(f"[Answer] âŒ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                import traceback
                print(f"[Answer] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€ ìƒì„±
                llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {document_content[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"""
            
            print(f"[Answer] ğŸ¯ ìµœì¢… ìƒì„±ëœ ë‹µë³€:")
            print(f"[Answer] {llm_answer}")
            
            # ì´ë¯¸ì§€ URL ìƒì„± (ì²« ë²ˆì§¸ ë¬¸ì„œ ê¸°ë°˜)
            image_url = None
            if top_result and top_payload:
                # ë¬¸ì„œ ì œëª©ê³¼ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ URL ìƒì„±
                doc_title = top_payload.get('ppt_title', '')
                doc_index = top_result.get('res_id', 0)
                
                if doc_title and doc_index:
                    # ì„¤ì • ê°€ëŠ¥í•œ ê¸°ë³¸ URLì— RAG ë¬¸ì„œ ì •ë³´ë¥¼ ì¶”ê°€í•œ í˜•ì‹
                    # URL ì•ˆì „í•œ ë¬¸ìì—´ë¡œ ë³€í™˜
                    import urllib.parse
                    safe_title = urllib.parse.quote(doc_title, safe='')
                    image_url = f"{IMAGE_BASE_URL}{IMAGE_PATH_PREFIX}/{safe_title}/{doc_index}.jpg"
                    print(f"[Answer] ğŸ–¼ï¸ ìƒì„±ëœ ì´ë¯¸ì§€ URL: {image_url}")
                    print(f"[Answer] ğŸ”§ ì´ë¯¸ì§€ URL êµ¬ì„± - ê¸°ë³¸URL: {IMAGE_BASE_URL}, ê²½ë¡œ: {IMAGE_PATH_PREFIX}, ì œëª©: {safe_title}, ì¸ë±ìŠ¤: {doc_index}")
                else:
                    print(f"[Answer] âš ï¸ ì´ë¯¸ì§€ URL ìƒì„± ì‹¤íŒ¨ - doc_title: {doc_title}, doc_index: {doc_index}")

            # LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ ìœ„í•œ ì™„ì „í•œ ì‘ë‹µ êµ¬ì¡°
            response = {
                "res_id": [rest['res_id'] for rest in candidates_top],
                "answer": llm_answer,  # LLMìœ¼ë¡œ ìƒì„±ëœ ì‹¤ì œ ë‹µë³€
                "q_mode": "search",  # ë­ê·¸ë˜í”„ëŠ” í•­ìƒ search ëª¨ë“œ
                "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
                "db_search_title": [item.get('res_payload', {}).get('ppt_title', 'ì œëª© ì—†ìŒ') for item in candidates_top],  # ê²€ìƒ‰ëœ ë¬¸ì„œ ì œëª©ë“¤
                "top_document": top_result,
                "analysis_image_url": image_url  # ë­ê·¸ë˜í”„ 4ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ ì´ë¯¸ì§€ URL
            }
        else:
            print(f"[Answer] âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            response = {
                "res_id": [],
                "answer": f"ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "q_mode": "search",
                "keyword": state["keyword"],
                "db_search_title": []
            }
        
        print(f"[Answer] ğŸ“¤ ìµœì¢… ì‘ë‹µ êµ¬ì¡°:")
        print(f"[Answer] {response}")
        
        # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
        # await save_langgraph_result_to_db(state['question'], response, state["keyword"], state["candidates_total"])
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(generator_id, "D", "completed", {
                "message": "ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ", 
                "answer": response.get('answer', '')[:100],
                "analysis_image_url": response.get('analysis_image_url')  # ì´ë¯¸ì§€ URL í¬í•¨
            })
        
        print(f"[Answer] âœ… node_rc_answer í•¨ìˆ˜ ì™„ë£Œ")
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": response,
            "generator_id": state.get('generator_id')
        }
        
    except Exception as e:
        print("[error]: node_rc_answer")
        import traceback
        print(f"[error] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise RuntimeError(f"[error]: node_rc_answer: {str(e)}")

async def node_rc_plain_answer(state: SearchState) -> SearchState:
    """ê¸°ë³¸ ë‹µë³€ ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_plain_answer ì‹¤í–‰")
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë” êµ¬ì²´ì ì¸ ì•ˆë‚´ ë©”ì‹œì§€ ìƒì„±
    question = state['question']
    keywords = state.get('keyword', [])
    
    if isinstance(keywords, list) and len(keywords) > 1:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}"
    else:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}"
    
    detailed_answer = f"""ğŸ” **ë¶„ì„ ê²°ê³¼ ìš”ì•½**

**ì…ë ¥ ì§ˆë¬¸**: {question}

**í‚¤ì›Œë“œ ì¦ê°•**: {keyword_info}

**ê²€ìƒ‰ ê²°ê³¼**: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ê°œì„  ì œì•ˆ**:
1. **ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±**í•´ì£¼ì„¸ìš”
   - ì˜ˆ: "ì„±ê³¼ ê°œì„ " â†’ "2024ë…„ 1ë¶„ê¸° ì˜ì—…íŒ€ ì„±ê³¼ ê°œì„  ë°©ì•ˆ"
   - ì˜ˆ: "ì „ëµ ìˆ˜ë¦½" â†’ "ì‹ ì œí’ˆ ì¶œì‹œë¥¼ ìœ„í•œ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½"

2. **ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ê°€**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ í‚¤ì›Œë“œ: {keywords[:3] if isinstance(keywords, list) else keywords}
   - ì¶”ê°€ í‚¤ì›Œë“œ ì˜ˆì‹œ: êµ¬ì²´ì ì¸ ì—…ë¬´ ì˜ì—­, ê¸°ê°„, ë¶€ì„œëª… ë“±

3. **ë°ì´í„°ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ ì„¤ì •ëœ ë²¡í„° DB: {QDRANT_HOST}:{QDRANT_PORT}
   - ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}

ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤."""
    
    # Redisë¥¼ í†µí•´ ì™„ë£Œ ìƒíƒœ ë°œí–‰ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ DB ì €ì¥ì„ ìœ„í•´)
    complete_result = {
        "res_id": [], 
        "answer": detailed_answer,
        "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
        "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
        "db_search_title": []  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    }
    
    # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
    # await save_langgraph_result_to_db(state['question'], complete_result, state["keyword"], state["candidates_total"])
    
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(generator_id, "D", "completed", {"message": "ê¸°ë³¸ ë‹µë³€ ìƒì„± ì™„ë£Œ", "answer": complete_result.get('answer', '')[:100]})
    
    return {
        "question": state['question'],
        "keyword": state["keyword"],
        "candidates_total": state["candidates_total"],
        "response": {
            "res_id": [], 
            "answer": detailed_answer,
            "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
            "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
            "db_search_title": []
        },
        "generator_id": state.get('generator_id')
    }

def judge_rc_ragscore(state: SearchState) -> str:
    """ë™ì  RAG ì ìˆ˜ íŒë‹¨ (í•˜ë“œì½”ë”©ëœ ì„ê³„ê°’ ì œê±°)"""
    candidates_total = state["candidates_total"]
    
    if not candidates_total:
        return "N"
    
    # ë™ì  ì„ê³„ê°’ ê³„ì‚°: í‰ê·  ì ìˆ˜ì˜ 70%ë¥¼ ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš©
    scores = [candidate.get("res_score", 0) for candidate in candidates_total]
    valid_scores = [score for score in scores if score > 0]
    
    if not valid_scores:
        return "N"
    
    avg_score = sum(valid_scores) / len(valid_scores)
    dynamic_threshold = avg_score * 0.7
    
    # ìµœì†Œ ì„ê³„ê°’ ì„¤ì • (ë„ˆë¬´ ë‚®ì€ ì ìˆ˜ëŠ” ì œì™¸)
    min_threshold = 0.1
    threshold = max(dynamic_threshold, min_threshold)
    
    has_good_results = any(candidate.get("res_score", 0) >= threshold for candidate in candidates_total)
    
    print(f"[JUDGE] ë™ì  ì„ê³„ê°’: {threshold:.4f} (í‰ê· : {avg_score:.4f})")
    print(f"[JUDGE] íŒë‹¨ ê²°ê³¼: {'Y' if has_good_results else 'N'}")
    
    return "Y" if has_good_results else "N"

async def save_langgraph_result_to_db(question: str, response: dict, keywords: list, candidates_total: list, image_url: str = None):
    """LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ DBì— ì§ì ‘ ì €ì¥ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    try:
        print(f"[DB_SAVE] LangGraph ê²°ê³¼ DB ì €ì¥ ì‹œì‘ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ì§ˆë¬¸: {question}")
        print(f"[DB_SAVE] ì‘ë‹µ: {response.get('answer', '')[:100]}...")
        print(f"[DB_SAVE] í‚¤ì›Œë“œ: {keywords}")
        print(f"[DB_SAVE] ë¬¸ì„œ: {len(candidates_total)}ê±´")
        print(f"[DB_SAVE] ì´ë¯¸ì§€ URL: {image_url}")
        
        # ìƒˆ ëŒ€í™” ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì°¾ê¸°
        db = next(get_db())
        
        # ìƒˆ ëŒ€í™” ìƒì„±
        from datetime import datetime
        title = question[:50] + "..." if len(question) > 50 else question
        conversation = Conversation(
            title=title,
            user_id=1,  # ê¸°ë³¸ ì‚¬ìš©ì ID (ì‹¤ì œë¡œëŠ” ì¸ì¦ëœ ì‚¬ìš©ì ID ì‚¬ìš©)
            last_updated=datetime.utcnow()
        )
        db.add(conversation)
        db.commit()
        db.refresh(conversation)
        
        # ë©”ì‹œì§€ ì €ì¥ (q_mode: 'search' - ë­ê·¸ë˜í”„ ì „ìš©)
        message = Message(
            conversation_id=conversation.id,
            role="user",
            question=question,
            ans=response.get('answer', ''),
            q_mode='search',  # ë­ê·¸ë˜í”„ ì „ìš© ëª¨ë“œ
            keyword=str(keywords) if keywords else None,
            db_search_title=str([item.get('res_payload', {}).get('ppt_title', '') for item in candidates_total[:5]]) if candidates_total else None,
            image=image_url  # ì´ë¯¸ì§€ URL ì¶”ê°€
            # user_name í•„ë“œ ì œê±° - í•˜ë“œì½”ë”© ë°©ì§€
        )
        
        db.add(message)
        db.commit()
        
        print(f"[DB_SAVE] âœ… LangGraph ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ëŒ€í™” ID: {conversation.id}")
        print(f"[DB_SAVE] ë©”ì‹œì§€ ID: {message.id}")
        print(f"[DB_SAVE] q_mode: {message.q_mode} (ë­ê·¸ë˜í”„ ì „ìš©)")
        
    except Exception as e:
        print(f"[DB_SAVE] âŒ LangGraph ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"[DB_SAVE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")


# LangGraph êµ¬ì„±
def create_langgraph():
    """LangGraph ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("node_rc_init", node_rc_init)
    workflow.add_node("node_rc_keyword", node_rc_keyword)
    workflow.add_node("node_rc_rag", node_rc_rag)
    workflow.add_node("node_rc_rerank", node_rc_rerank)
    workflow.add_node("node_rc_answer", node_rc_answer)
    workflow.add_node("node_rc_plain_answer", node_rc_plain_answer)
    
    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("node_rc_init")
    workflow.add_edge("node_rc_init", "node_rc_keyword")
    workflow.add_edge("node_rc_keyword", "node_rc_rag")
    workflow.add_conditional_edges(
        "node_rc_rag",
        judge_rc_ragscore,
        {
            "Y": "node_rc_rerank",
            "N": "node_rc_plain_answer"
        }
    )
    workflow.add_edge("node_rc_rerank", "node_rc_answer")
    workflow.add_edge("node_rc_answer", END)
    workflow.add_edge("node_rc_plain_answer", END)
    
    return workflow.compile()

# LangGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
try:
    langgraph_instance = create_langgraph()
    print("[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨: {e}")
    langgraph_instance = None

# ê°„ë‹¨í•œ LLM ì‘ë‹µ í•¨ìˆ˜ (conversations.pyì—ì„œ ì‚¬ìš©)
async def get_llm_response(question: str) -> str:
    """ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    try:
        if not OPENAI_API_KEY:
            return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        print(f"[LLM_RESPONSE] ì§ˆë¬¸: {question}")
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": question}
        ]
        
        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        httpx_client = httpx.AsyncClient(verify=False, timeout=None)
        
        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL,
            http_client=httpx_client,
            default_headers={
                "x-dep-ticket": OPENAI_API_KEY,
                "Send-System-Name": "ds2llm",
                "User-Id": "conversation_api",
                "User-Type": "AD_ID",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            }
        )
        
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=1000,
            temperature=0.7
        )
        
        answer = response.choices[0].message.content
        print(f"[LLM_RESPONSE] ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(answer)}ì")
        return answer
        
    except Exception as e:
        print(f"[LLM_RESPONSE] ì˜¤ë¥˜: {str(e)}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"




# ì¼ë°˜ LLM ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (streaming ì§€ì›)
@router.post("/chat/stream")
async def stream_chat_with_llm(request: StreamRequest, http_request: Request, db: Session = Depends(get_db)):
    """Stream a response from general LLM chat using async method"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[Chat Stream] ========== LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‹œì‘ ==========")
        print(f"[Chat Stream] ìš”ì²­ ì •ë³´:")
        print(f"[Chat Stream] - ì§ˆë¬¸: {request.question}")
        print(f"[Chat Stream] - ëŒ€í™” ID: {request.conversation_id}")
        print(f"[Chat Stream] - conversation_id íƒ€ì…: {type(request.conversation_id)}")
        print(f"[Chat Stream] - conversation_idê°€ Noneì¸ê°€?: {request.conversation_id is None}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [{"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."}]
        
        if request.conversation_id:
            try:
                # í•´ë‹¹ ëŒ€í™”ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 10ê°œë§Œ)
                conversation_messages = db.query(Message).filter(
                    Message.conversation_id == request.conversation_id
                ).order_by(Message.created_at.asc()).limit(10).all()
                
                print(f"[Chat Stream] ì´ì „ ëŒ€í™” ë©”ì‹œì§€ {len(conversation_messages)}ê°œ ë¡œë“œ")
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ==========")
                
                # ì´ì „ ëŒ€í™”ë¥¼ messagesì— ì¶”ê°€
                for i, msg in enumerate(conversation_messages):
                    print(f"[Chat Stream] DB ë©”ì‹œì§€ {i+1}: ID={msg.id}, role={msg.role}, created_at={msg.created_at}")
                    if msg.question:
                        print(f"[Chat Stream] ì§ˆë¬¸: {msg.question}")
                        messages.append({"role": "user", "content": msg.question})
                    if msg.ans:
                        print(f"[Chat Stream] ë‹µë³€: {msg.ans}")
                        messages.append({"role": "assistant", "content": msg.ans})
                    print(f"[Chat Stream] ----------------------------------------")
                
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ ==========")
                        
            except Exception as e:
                print(f"[Chat Stream] ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[Chat Stream] ì „ì†¡í•  ë©”ì‹œì§€ ê°œìˆ˜: {len(messages)}")
        print(f"[Chat Stream] ========== OpenAI APIì— ì „ì†¡í•  ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ==========")
        print(f"[Chat Stream] ==========ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© : ")
        print(f"{messages}")
        
        print(f"[Chat Stream] ========== ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ë ==========")
        
        # ìƒˆë¡œìš´ GPT-3.5-turbo ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©
        return StreamingResponse(
            get_streaming_response_async(messages, http_request, request.generate_image or False),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[Chat Stream] LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[Chat Stream] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")



# ì§ˆë¬¸ ìœ í˜• íŒë³„ í•¨ìˆ˜
def is_first_question_in_conversation(conversation_id: int, db: Session) -> bool:
    """ëŒ€í™”ì—ì„œ ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
    try:
        message_count = db.query(Message).filter(Message.conversation_id == conversation_id).count()
        print(f"[QUESTION_TYPE] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ ìˆ˜: {message_count}")
        return message_count == 0
    except Exception as e:
        print(f"[QUESTION_TYPE] ë©”ì‹œì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ ì‹œ ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼

def get_conversation_context(conversation_id: int, db: Session) -> dict:
    """ëŒ€í™”ì˜ ì»¨í…ìŠ¤íŠ¸ì™€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # í•´ë‹¹ ëŒ€í™”ì˜ ëª¨ë“  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ìˆœ ì •ë ¬)
        messages = db.query(Message).filter(
            Message.conversation_id == conversation_id
        ).order_by(Message.created_at.asc()).all()
        
        print(f"[CONTEXT] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ {len(messages)}ê°œ ë¡œë“œ")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´
        if len(messages) > 0:
            print(f"[CONTEXT] ë©”ì‹œì§€ ìƒì„¸:")
            for i, msg in enumerate(messages):
                print(f"[CONTEXT]   {i+1}. ID: {msg.id}, q_mode: {msg.q_mode}, role: {msg.role}, question: {msg.question[:50] if msg.question else 'None'}...")
        else:
            print(f"[CONTEXT] âš ï¸ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€í™” ID {conversation_id} í™•ì¸ í•„ìš”")
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ì°¾ê¸° (q_modeê°€ "search"ì¸ ë©”ì‹œì§€)
        first_message = None
        for msg in messages:
            if msg.q_mode == "search":
                first_message = msg
                print(f"[CONTEXT] ì²« ë²ˆì§¸ ì§ˆë¬¸ ë°œê²¬: ë©”ì‹œì§€ ID {msg.id}")
                break
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation_history = []
        for msg in messages:
            if msg.question:
                conversation_history.append({"role": "user", "content": msg.question})
            if msg.ans:
                conversation_history.append({"role": "assistant", "content": msg.ans})
        
        return {
            "first_message": first_message,
            "conversation_history": conversation_history,
            "message_count": len(messages)
        }
        
    except Exception as e:
        print(f"[CONTEXT] ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return {
            "first_message": None,
            "conversation_history": [],
            "message_count": 0
        }

# SSE ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©)
@router.post("/langgraph/stream")
async def execute_langgraph_stream(request: StreamRequest, db: Session = Depends(get_db)):
    """LangGraph SSE ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    
    async def generate_sse():
        generator_id = str(uuid.uuid4())
        generator = SSEGenerator(generator_id)
        sse_generators[generator_id] = generator
        
        try:
            # OpenAI API í‚¤ í™•ì¸
            if not OPENAI_API_KEY:
                yield f"data: {json.dumps({'error': 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'})}\n\n"
                return
            
            print(f"[SSE] ğŸš€ LangGraph SSE ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {request.question}")
            
            # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸
            if request.conversation_id:
                is_first = is_first_question_in_conversation(request.conversation_id, db)
                if not is_first:
                    yield f"data: {json.dumps({'error': 'ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”'})}\n\n"
                    return
            
            # ì›Œí¬í”Œë¡œìš° í™•ì¸
            if langgraph_instance is None:
                yield f"data: {json.dumps({'error': 'LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'})}\n\n"
                return
            
            # ì´ˆê¸° ìƒíƒœì— generator_id ì¶”ê°€
            initial_state = {
                "question": request.question,
                "generator_id": generator_id
            }
            
            print(f"[SSE] LangGraph ì‹¤í–‰ ì‹œì‘: {request.question}")
            
            # LangGraph ì‹¤í–‰ì„ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            async def run_langgraph():
                try:
                    result = await langgraph_instance.ainvoke(initial_state)
                    # DONE ë©”ì‹œì§€ì— ì „ì²´ LangGraph ê²°ê³¼ í¬í•¨
                    done_message = {
                        "stage": "DONE", 
                        "result": result,  # ì „ì²´ LangGraph ê²°ê³¼ í¬í•¨
                        "keyword": result.get('keyword', []),
                        "candidates_total": result.get('candidates_total', [])
                    }
                    await generator.send_message(done_message)
                except Exception as e:
                    await generator.send_message({"stage": "ERROR", "error": str(e)})
                finally:
                    await generator.close()
            
            # LangGraph ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘
            langgraph_task = asyncio.create_task(run_langgraph())
            
            # SSE ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
            while generator.is_active:
                try:
                    # íƒ€ì„ì•„ì›ƒì„ ì§§ê²Œ ì„¤ì •í•˜ì—¬ ì‘ë‹µì„± í–¥ìƒ
                    message = await asyncio.wait_for(generator.message_queue.get(), timeout=0.1)
                    
                    if message is None:  # ì¢…ë£Œ ì‹ í˜¸
                        break
                    
                    # SSE í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡
                    yield f"data: {json.dumps(message)}\n\n"
                    
                except asyncio.TimeoutError:
                    # íƒ€ì„ì•„ì›ƒ ì‹œ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡
                    yield f"data: {json.dumps({'heartbeat': True})}\n\n"
                    continue
                except Exception as e:
                    print(f"[SSE] ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    break
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            yield f"data: [DONE]\n\n"
            
        except Exception as e:
            print(f"[SSE] ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        finally:
            # ì •ë¦¬
            if generator_id in sse_generators:
                del sse_generators[generator_id]
            print(f"[SSE] ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ: {generator_id}")
    
    return StreamingResponse(
        generate_sse(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

# LangGraph ì§ì ‘ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©) - ê¸°ì¡´ ìœ ì§€
@router.post("/langgraph")
async def execute_langgraph(request: StreamRequest, db: Session = Depends(get_db)):
    """LangGraphë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë°˜í™˜ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"[LangGraph] ğŸš€ ë­ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸
        if request.conversation_id:
            is_first = is_first_question_in_conversation(request.conversation_id, db)
            if not is_first:
                print(f"[LangGraph] âš ï¸ ì¶”ê°€ ì§ˆë¬¸ ê°ì§€ë¨ - LangGraph ì‹¤í–‰ ì°¨ë‹¨")
                raise HTTPException(
                    status_code=400, 
                    detail="ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
                )
        
        # ì›Œí¬í”Œë¡œìš° í™•ì¸
        if langgraph_instance is None:
            raise HTTPException(status_code=500, detail="LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        initial_state = {"question": request.question}
        print(f"[LangGraph] ì‹¤í–‰ ì‹œì‘: {request.question}")
        print(f"[LangGraph] ì´ˆê¸° ìƒíƒœ: {initial_state}")
        
        result = await langgraph_instance.ainvoke(initial_state)
        
        print(f"[LangGraph] âœ… ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ì—ì„œ íƒœê·¸ì™€ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
        tags = None
        db_search_title = None
        
        if isinstance(result, dict):
            # í‚¤ì›Œë“œ ì •ë³´ì—ì„œ íƒœê·¸ ì¶”ì¶œ
            if 'keyword' in result and result['keyword']:
                if isinstance(result['keyword'], list):
                    tags = ', '.join(result['keyword'])
                else:
                    tags = str(result['keyword'])
                print(f"[LangGraph] í‚¤ì›Œë“œ: {len(result['keyword'])}ê°œ")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
            if 'candidates_total' in result and result['candidates_total']:
                db_search_title = f"{len(result['candidates_total'])}ê±´"
                print(f"[LangGraph] ë¬¸ì„œ: {db_search_title}")
            
            # ì‘ë‹µ ì •ë³´ í™•ì¸
            if 'response' in result and result['response']:
                response_text = result['response'].get('answer', '')[:50] if isinstance(result['response'], dict) else str(result['response'])[:50]
                print(f"[LangGraph] ì‘ë‹µ: {response_text}...")
        
        print(f"[LangGraph] ìš”ì•½: í‚¤ì›Œë“œ {len(result.get('keyword', []))}ê°œ, ë¬¸ì„œ {len(result.get('candidates_total', []))}ê±´")
        
        return {
            "status": "success",
            "result": result,
            "tags": tags,
            "db_search_title": db_search_title,
            "message": "LangGraph ì‹¤í–‰ ì™„ë£Œ (ì²« ë²ˆì§¸ ì§ˆë¬¸)"
        }
        
    except Exception as e:
        print(f"[LangGraph] ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[LangGraph] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")


# ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.post("/langgraph/followup/stream")
async def execute_followup_question_stream(request: StreamRequest, http_request: Request, db: Session = Depends(get_db)):
    """ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ - ê¸°ì¡´ RAG ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œìš©"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[FOLLOWUP_STREAM] ğŸ”„ LLM ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” ID í™•ì¸
        if not request.conversation_id:
            return Response(content="Error: ì¶”ê°€ ì§ˆë¬¸ì€ conversation_idê°€ í•„ìš”í•©ë‹ˆë‹¤", media_type="text/plain")
        
        # LangGraph ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ ê²½ìš°)
        langgraph_context = getattr(request, 'langgraph_context', None)
        include_langgraph_context = getattr(request, 'include_langgraph_context', False)
        
        if include_langgraph_context and langgraph_context:
            print(f"[FOLLOWUP_STREAM] ğŸ”¬ LangGraph ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©")
            print(f"[FOLLOWUP_STREAM] ì›ë³¸ ì§ˆë¬¸: {langgraph_context.get('original_question', 'N/A')}")
            print(f"[FOLLOWUP_STREAM] í‚¤ì›Œë“œ: {langgraph_context.get('keywords', 'N/A')}")
            print(f"[FOLLOWUP_STREAM] ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(langgraph_context.get('search_results', []))}")
            
            # LangGraph ì»¨í…ìŠ¤íŠ¸ë¡œ ë¬¸ì„œ ì •ë³´ êµ¬ì„±
            document_title = "LangGraph ê²€ìƒ‰ ê²°ê³¼"
            search_results = langgraph_context.get('search_results', [])
            keywords = langgraph_context.get('keywords', [])
            previous_answer = langgraph_context.get('previous_answer', '')
            original_question = langgraph_context.get('original_question', '')
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±
            search_content = ""
            for i, result in enumerate(search_results[:3], 1):
                if isinstance(result, dict) and 'res_payload' in result:
                    title = result['res_payload'].get('ppt_title', f'ë¬¸ì„œ {i}')
                    content = result['res_payload'].get('ppt_content', 'ë‚´ìš© ì—†ìŒ')
                    search_content += f"\n[ë¬¸ì„œ {i}] {title}: {content}"
            
            document_content = f"""
[ì²« ë²ˆì§¸ ì§ˆë¬¸] {original_question}

[ì¶”ì¶œëœ í‚¤ì›Œë“œ] {', '.join(keywords) if isinstance(keywords, list) else keywords}

[ê²€ìƒ‰ëœ ë¬¸ì„œë“¤]{search_content}

[ì´ì „ ë‹µë³€] {previous_answer[:500]}...
"""
        else:
            # ê¸°ì¡´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
            context = get_conversation_context(request.conversation_id, db)
            
            if not context["first_message"]:
                print(f"[FOLLOWUP_STREAM] âš ï¸ ì²« ë²ˆì§¸ ì§ˆë¬¸ ì—†ìŒ - ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬")
                document_title = "ì¼ë°˜ ëŒ€í™”"
                document_content = "ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            else:
                # ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œì™€ ë¬¸ì„œ ì •ë³´ í™œìš©
                first_message = context["first_message"]
                document_title = first_message.db_search_title or "ê´€ë ¨ ë¬¸ì„œ"
                document_content = f"í‚¤ì›Œë“œ: {first_message.keyword}\nê²€ìƒ‰ ê²°ê³¼: {first_message.db_search_title}\nì²« ë²ˆì§¸ ì§ˆë¬¸: {first_message.question}\nì²« ë²ˆì§¸ ë‹µë³€: {first_message.ans[:500] if first_message.ans else 'ë‹µë³€ ì—†ìŒ'}..."
        
        print(f"[FOLLOWUP_STREAM] ğŸ“„ ì¬ì‚¬ìš©í•  RAG ë¬¸ì„œ:")
        print(f"[FOLLOWUP_STREAM] ì œëª©: {document_title}")
        print(f"[FOLLOWUP_STREAM] ë‚´ìš© ê¸¸ì´: {len(document_content)} ë¬¸ì")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        if include_langgraph_context and langgraph_context:
            # LangGraph ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì‹œ ê¸°ë³¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ë§Œ ê°€ì ¸ì˜¤ê¸°
            context = get_conversation_context(request.conversation_id, db)
            conversation_history = context["conversation_history"]
        else:
            # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            conversation_history = context["conversation_history"]
        
        print(f"[FOLLOWUP_STREAM] ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {document_content[:1500]}...

ìœ„ ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""
        
        # LLM API í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ)
        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
        messages.extend(recent_history)
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[FOLLOWUP_STREAM] ğŸ“¤ LLMì— ì „ì†¡í•  ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        print(f"[FOLLOWUP_STREAM] ğŸ“ í˜„ì¬ ì§ˆë¬¸: {request.question}")
        
        # ìƒˆë¡œìš´ GPT-3.5-turbo ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©
        return StreamingResponse(
            get_streaming_response_async(messages, http_request, request.generate_image or False),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[FOLLOWUP_STREAM] LLM ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[FOLLOWUP_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")

async def generate_image(prompt: str) -> str:
    """Generate an image using OpenAI DALL-E API"""
    try:
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ë¶€ë¶„ì—ì„œ OpenAI DALL-E API í˜¸ì¶œ
        # ì˜ˆì‹œ: 
        # client = OpenAI(api_key=OPENAI_API_KEY)
        # response = client.images.generate(prompt=prompt, n=1, size="1024x1024")
        # image_url = response.data[0].url
        
        # ì´ë¯¸ì§€ URL ë°˜í™˜ (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
        return None
    except Exception as e:
        print(f"Error generating image: {str(e)}")
        return None

async def get_streaming_response_async(messages: List[Dict], request: Request, generate_image: bool = False):
    """Stream a response from LLM using AsyncOpenAI with custom headers"""
    try:
        print(f"[LLM_STREAM] ğŸš€ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
        
        # ì´ë¯¸ì§€ URL (ì´ë¯¸ì§€ ìƒì„±ì´ ìš”ì²­ëœ ê²½ìš°)
        image_url = None
        if generate_image:
            # ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ë¡œì§ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            image_url = await generate_image(messages[-1]["content"] if messages else "")
        
        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        httpx_client = httpx.AsyncClient(verify=False, timeout=None)
        
        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL,
            http_client=httpx_client,
            default_headers={
                "x-dep-ticket": OPENAI_API_KEY,
                "Send-System-Name": "ds2llm",
                "User-Id": getattr(request, 'username', 'anonymous'),
                "User-Type": "AD_ID",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            }
        )
        
        print(f"[LLM_STREAM] ğŸ“¤ ë©”ì‹œì§€ ì „ì†¡: {len(messages)}ê°œ")
        
        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            stream=True
        )
        
        print(f"[LLM_STREAM] ğŸ“¥ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
        
        text_response = ""
        async for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                text_response += content
                
                try:
                    # ë¹„-ASCII ë¬¸ì í—ˆìš©, UTF-8 bytesë¡œ ì¦‰ì‹œ ì „ì†¡
                    payload = json.dumps({'content': content}, ensure_ascii=False)
                    yield (f"data: {payload}\n\n").encode("utf-8")
                    await asyncio.sleep(0.01)  # ì²­í¬ ì‚¬ì´ì— ì§€ì—° ì¶”ê°€í•˜ì—¬ ë‹¤ë¥¸ API ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•¨
                except (ConnectionResetError, BrokenPipeError, OSError, ConnectionAbortedError, ConnectionError) as e:
                    # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
                    print(f"[LLM_STREAM] Client disconnected during streaming lv2: {type(e).__name__}")
                    return
                except Exception as e:
                    print(f"[LLM_STREAM] Unexpected error during streaming lv1: {str(e)}")
                    return
        
        # í…ìŠ¤íŠ¸ ì‘ë‹µì´ ì™„ë£Œëœ í›„ ì´ë¯¸ì§€ URLì´ ìˆìœ¼ë©´ ì „ì†¡
        if image_url:
            try:
                response_data = {
                    "text": text_response,
                    "image_url": image_url
                }
                payload = json.dumps(response_data, ensure_ascii=False)
                yield (f"data: {payload}\n\n").encode("utf-8")
            except Exception as e:
                print(f"[LLM_STREAM] Error sending image data: {str(e)}")
        
        print(f"[LLM_STREAM] âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        yield "data: [DONE]\n\n".encode("utf-8")
        
    except Exception as e:
        print(f"[LLM_STREAM] Error in streaming response: {str(e)}")
        import traceback
        print(f"[LLM_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        try:
            error_payload = json.dumps({'error': str(e)}, ensure_ascii=False)
            yield (f"data: {error_payload}\n\n").encode("utf-8")
            yield "data: [DONE]\n\n".encode("utf-8")
        except Exception:
            # ì—ëŸ¬ ì „ì†¡ë„ ì‹¤íŒ¨í•œ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
            return
