import openai
from openai import AsyncOpenAI
import asyncio
import json
import httpx
import uuid
from fastapi import APIRouter, HTTPException, Response, Depends, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import redis.asyncio as aioredis
from langgraph.graph import END, StateGraph
from collections import defaultdict
from qdrant_client import QdrantClient
from app.utils.config import (
    OPENAI_API_KEY,
    REDIS_HOST, REDIS_PORT, REDIS_CHANNEL,
    QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION
)
from app.database import get_db
from app.models import Conversation, Message
from sqlalchemy.orm import Session

from datetime import datetime

# Create router 
router = APIRouter()

# Redis ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    redis_client = aioredis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
    print(f"[Redis] Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {REDIS_HOST}:{REDIS_PORT}")
except Exception as e:
    print(f"[Redis] Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    redis_client = None

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”© í™•ì¸
print(f"[Config] OpenAI API Key: {'ì„¤ì •ë¨' if OPENAI_API_KEY else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
print(f"[Config] Qdrant: {QDRANT_HOST}:{QDRANT_PORT} (ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'})")

# ì„ë² ë”© ëª¨ë¸ (10ì°¨ì› ë²¡í„° ìƒì„±)
class SimpleEmbeddings:
    def __init__(self):
        self.dimension = 10  # Qdrant ì»¬ë ‰ì…˜ ì°¨ì›ì— ë§ì¶¤
        self.api_key = OPENAI_API_KEY
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """10ì°¨ì› ë²¡í„° ìƒì„± (Qdrant ì»¬ë ‰ì…˜ì— ë§ì¶¤)"""
        try:
            embeddings = []
            for text in texts:
                # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ 10ì°¨ì› ë²¡í„° ìƒì„±
                import hashlib
                import struct
                
                # í…ìŠ¤íŠ¸ë¥¼ í•´ì‹œë¡œ ë³€í™˜
                text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                
                # í•´ì‹œë¥¼ 10ê°œì˜ floatë¡œ ë³€í™˜
                vector = []
                for i in range(0, 40, 4):  # MD5ëŠ” 32ë°”ì´íŠ¸, 4ë°”ì´íŠ¸ì”© 8ê°œ ê·¸ë£¹
                    if i + 4 <= len(text_hash):
                        hex_group = text_hash[i:i+4]
                        # 16ì§„ìˆ˜ë¥¼ floatë¡œ ë³€í™˜ (0-1 ë²”ìœ„ë¡œ ì •ê·œí™”)
                        float_val = float(int(hex_group, 16)) / 65535.0
                        vector.append(float_val)
                
                # 10ì°¨ì›ì´ ë˜ë„ë¡ íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
                while len(vector) < 10:
                    vector.append(0.0)
                vector = vector[:10]
                
                embeddings.append(vector)
                
            print(f"[Embeddings] {len(texts)}ê°œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ {self.dimension}ì°¨ì› ë²¡í„° ìƒì„± ì™„ë£Œ")
            return embeddings
            
        except Exception as e:
            print(f"[Embeddings] ë²¡í„° ìƒì„± ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ 10ì°¨ì› ë²¡í„° ë°˜í™˜
            return [[0.0] * self.dimension for _ in texts]

# ë²¡í„° DB ê²€ìƒ‰ í•¨ìˆ˜ë“¤
async def rag_multivector(question_type: str, limit: int, queries: List[str], query_vectors: List[List[float]], 
                          ip: str, port: int, collection: str) -> List[dict]:
    """ë©€í‹°ë²¡í„° ê²€ìƒ‰ (Qdrant)"""
    try:
        print(f"[RAG] Qdrant ì—°ê²° ì‹œë„: {ip}:{port}, ì»¬ë ‰ì…˜: {collection}")
        
        # ë²¡í„° DB ì—°ê²° ì‹œë„
        client = QdrantClient(host=ip, port=port)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        try:
            collections = client.get_collections()
            print(f"[RAG] ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {[col.name for col in collections.collections]}")
            
            if not collection:
                print(f"[RAG] ì»¬ë ‰ì…˜ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
                
            if collection not in [col.name for col in collections.collections]:
                print(f"[RAG] ì»¬ë ‰ì…˜ '{collection}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return []
                
            print(f"[RAG] ì»¬ë ‰ì…˜ '{collection}' ì—°ê²° ì„±ê³µ")
            
        except Exception as e:
            print(f"[RAG] Qdrant ì»¬ë ‰ì…˜ í™•ì¸ ì˜¤ë¥˜: {e}")
            return []
        
        results = []
        for i, (query, vector) in enumerate(zip(queries, query_vectors)):
            try:
                print(f"[RAG] ë²¡í„° ê²€ìƒ‰ ì‹œì‘: query {i+1}/{len(queries)}")
                
                # ë²¡í„° ê²€ìƒ‰
                search_result = client.search(
                    collection_name=collection,
                    query_vector=vector,
                    limit=limit,
                    with_payload=True
                )
                
                print(f"[RAG] ê²€ìƒ‰ ê²°ê³¼: {len(search_result)}ê±´")
                
                for item in search_result:
                    results.append({
                        'res_id': item.id,
                        'res_score': item.score,
                        'type_question': question_type,
                        'type_vector': '',  # ë²¡í„° íƒ€ì…
                        'res_payload': item.payload
                    })
            except Exception as e:
                print(f"[RAG] ê°œë³„ ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜ (query {i}): {e}")
                continue
        
        print(f"[RAG] ì´ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´")
        return results
        
    except Exception as e:
        print(f"[RAG] Qdrant ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

async def rag_vector_qdrant(question_type: str, limit: int, queries: List[str], query_vectors: List[List[float]], 
                           ip: str, port: int, collection: str) -> List[dict]:
    """Qdrant ë²¡í„° ê²€ìƒ‰"""
    return await rag_multivector(question_type, limit, queries, query_vectors, ip, port, collection)

async def rag_payload_qdrant(question_type: str, limit: int, queries: List[str], query_vectors: List[List[float]], 
                             ip: str, port: int, collection: str) -> List[dict]:
    """Qdrant í˜ì´ë¡œë“œ ê²€ìƒ‰"""
    return await rag_multivector(question_type, limit, queries, query_vectors, ip, port, collection)




# ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì„ ìœ„í•œ í´ë˜ìŠ¤
class StreamRequest(BaseModel):
    question: str
    conversation_id: Optional[int] = None
    generate_image: Optional[bool] = False  # ì´ë¯¸ì§€ ìƒì„± í”Œë˜ê·¸ ì¶”ê°€

# ì´ë¯¸ì§€ URL (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
# SAMPLE_IMAGE_URLS = []

# LangGraph ìƒíƒœ ì •ì˜
class SearchState(dict):
    question: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    keyword: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    candidates_each: List[dict] 
    candidates_total: List[dict] 
    response: List[dict]     # LLMì´ ìƒì„±í•œ ì‘ë‹µ

# Redis ìƒíƒœ ë°œí–‰ í•¨ìˆ˜
async def publish_node_status(node_name: str, status: str, data: dict):
    """Redisë¥¼ í†µí•´ ë…¸ë“œ ìƒíƒœë¥¼ ë°œí–‰"""
    if redis_client is None:
        print(f"[Redis] âŒ Redis í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - {node_name}: {status}")
        return
        
    try:
        message = {
            "node": node_name,
            "status": status,
            "data": data,
            "timestamp": asyncio.get_event_loop().time()
        }
        message_json = json.dumps(message)
        print(f"[Redis] ğŸ“¤ ë°œí–‰ ì‹œë„: {node_name}:{status} â†’ ì±„ë„: {REDIS_CHANNEL}")
        print(f"[Redis] ğŸ“„ ë©”ì‹œì§€ ë‚´ìš©: {message_json[:200]}...")
        
        result = await redis_client.publish(REDIS_CHANNEL, message_json)
        print(f"[Redis] âœ… ë°œí–‰ ì™„ë£Œ: {node_name}:{status} (êµ¬ë…ì {result}ëª…)")
    except Exception as e:
        print(f"[Redis] âŒ ë°œí–‰ ì‹¤íŒ¨: {node_name}:{status} - ì˜¤ë¥˜: {e}")
        import traceback
        print(f"[Redis] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        # Redis ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì›Œí¬í”Œë¡œìš°ëŠ” ê³„ì† ì§„í–‰
        pass

# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
async def node_rc_init(state: SearchState) -> SearchState:
    """ì´ˆê¸°í™” ë…¸ë“œ"""
    print("[inform]: node_rc_init")
    try: 
        question = state['question']
        await publish_node_status("node_init", "completed", {"result": question})
        return {
            "question": question,
            "keyword": "",
            "candidates_each": [],
            "candidates_total": [],
            "response": []
        }
    except Exception as e:
        print("[error]: node_rc_init")
        raise RuntimeError(f"[error]: node_rc_init: {str(e)}")

async def node_rc_keyword(state: SearchState) -> SearchState:
    """í‚¤ì›Œë“œ ì¦ê°• ë…¸ë“œ - LLMì„ ì‚¬ìš©í•œ ë™ì  í‚¤ì›Œë“œ ìƒì„±"""
    print("[inform]: node_rc_keyword")
    try:
        question = state['question']
        
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your-openai-api-key-here":
            print("[error]: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            base_keywords = [question]
            await publish_node_status("node_rc_keyword", "completed", {"result": base_keywords})
            return {
                "question": state['question'],
                "keyword": base_keywords,
                "candidates_each": [],
                "candidates_total": [],
                "response": []
            }
        
        try:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¦ê°• - ìƒˆë¡œìš´ LLM ë°©ì‹
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ì „ë¬¸ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³ , ìµœëŒ€ 15ê°œê¹Œì§€ ìƒì„±í•˜ì„¸ìš”."},
                {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”: {question}"}
            ]
            
            # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            httpx_client = httpx.AsyncClient(verify=False, timeout=None)
            
            # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url="https://api.openai.com/v1",
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "langgraph_keyword",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
            # ë¹„ë™ê¸° í˜¸ì¶œ
            response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            llm_response = response.choices[0].message.content
            
            # LLM ì‘ë‹µì„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            keywords_text = llm_response.strip()
            # ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³  ê° í‚¤ì›Œë“œ ì •ë¦¬
            augmented_keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            # ì›ë³¸ ì§ˆë¬¸ë„ í¬í•¨
            augmented_keywords.insert(0, question)
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 20ê°œë¡œ ì œí•œ
            augmented_keywords = list(dict.fromkeys(augmented_keywords))[:20]
            
            print(f"[inform]: LLMì„ í†µí•´ ìƒì„±ëœ í‚¤ì›Œë“œ: {len(augmented_keywords)}ê°œ")
            
        except Exception as llm_error:
            print(f"[error]: LLM í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {llm_error}")
            # LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©
            augmented_keywords = [question]
        
        await publish_node_status("node_rc_keyword", "completed", {"result": augmented_keywords})
        
        return {
            "question": state['question'],
            "keyword": augmented_keywords,
            "candidates_each": [],
            "candidates_total": [],
            "response": []
        }
    except Exception as e:
        print("[error]: node_rc_keyword")
        raise RuntimeError(f"[error]: node_rc_keyword: {str(e)}")

async def node_rc_rag(state: SearchState) -> SearchState:
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("[inform]: node_rc_rag")
    try:
        # ë²¡í„° DB ì„¤ì •
        ip, port, collection = QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        try:
            embeddings = SimpleEmbeddings()
        except Exception as e:
            print(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë²¡í„° ì‚¬ìš©
            embeddings = None
        
        # questionìœ¼ë¡œ ê²€ìƒ‰
        candidates_each = []
        if state.get('question') and embeddings:
            try:
                query_vectors = embeddings.embed_documents([state['question']])
                candidates_each.extend(await rag_multivector('question', 5, [state['question']], query_vectors, ip, port, collection))
            except Exception as e:
                print(f"Question ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # keywordë¡œ ê²€ìƒ‰ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬)
        if state.get('keyword') and embeddings:
            try:
                # keywordê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                if isinstance(state['keyword'], list):
                    keywords = state['keyword']
                else:
                    keywords = [state['keyword']]
                
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ í•„í„°ë§
                keywords = [k for k in keywords if k and isinstance(k, str) and k.strip()]
                
                if keywords:
                    query_vectors = embeddings.embed_documents(keywords)
                    candidates_each.extend(await rag_multivector('keyword', 2, keywords, query_vectors, ip, port, collection))
            except Exception as e:
                print(f"Keyword ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ë²¡í„° ê²€ìƒ‰ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        if not candidates_each:
            print("[RAG] ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ë°ì´í„° ìƒì„±
            candidates_each = [{
                'res_id': 'no_results',
                'res_score': 0.0,
                'res_payload': {
                    'title': 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ',
                    'content': 'ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”.',
                    'type': 'no_results'
                }
            }]
            print(f"[RAG] ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(candidates_each)}ê±´")

        # ê°€ì¤‘ í‰ê·  summation (ê¸°ì¡´ ë­ê·¸ë˜í”„ ì½”ë“œì™€ ë™ì¼)
        w_question = {
            'question': 1,
            'keyword': 1
        }
        w_vector = {
            'text': 1,
            'summary_purpose': 0.5,
            'summary_result': 0.5,
            'summary_fb': 0.5,
        }
        
        aggregated_scores = defaultdict(float)
        payloads = {}
        
        # candidates_eachê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if candidates_each:
            for item in candidates_each:
                try:
                    res_id = item.get('res_id')
                    score = item.get('res_score', 0.0)
                    type_question = item.get('type_question', '')
                    type_vector = item.get('type_vector', '')
                    
                    if res_id is not None:
                        aggregated_scores[res_id] += score * w_question.get(type_question, 1.0) * w_vector.get(type_vector, 1.0)
                        if res_id not in payloads:
                            payloads[res_id] = item.get('res_payload', {})
                except Exception as e:
                    print(f"ê°œë³„ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
        # ì •ë ¬ëœ ê²°ê³¼ ìƒì„± (list of dicts í˜•íƒœ) - ìƒìœ„ 5ê±´ë§Œ
        candidates_total = sorted(
            [{'res_id': res_id, 'res_score': aggregated_scores[res_id], 'res_payload': payloads[res_id]}
             for res_id in aggregated_scores],
            key=lambda x: x['res_score'],
            reverse=True
        )[:5]  # ìƒìœ„ 5ê±´ë§Œ ì„ íƒ
        
        print(f"[RAG] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 5ê±´):")
        for i, candidate in enumerate(candidates_total):
            title = candidate.get('res_payload', {}).get('ppt_title', 'ì œëª© ì—†ìŒ')
            summary = candidate.get('res_payload', {}).get('ppt_summary', 'ìš”ì•½ ì—†ìŒ')
            score = candidate.get('res_score', 0)
            print(f"  {i+1}. {title} - {summary} (ìœ ì‚¬ë„: {score:.4f})")
        
        await publish_node_status("node_rc_rag", "completed", {"result": candidates_total})
        
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_total": candidates_total,
            "response": []
        }
    except Exception as e:
        print(f"[error]: node_rc_rag: {str(e)}")
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ìƒíƒœ ë°˜í™˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê³„ì† ì§„í–‰
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_each": [],
            "candidates_total": [],
            "response": []
        }

async def node_rc_rerank(state: SearchState) -> SearchState:
    """ì¬ìˆœìœ„ ë…¸ë“œ"""
    print("[inform]: node_rc_rerank")
    try:
        cnt_result = 5
        candidates_top = state['candidates_total'][:cnt_result]
        
        # ì¬ìˆœìœ„ ì²˜ë¦¬ (ì‹¤ì œë¡œëŠ” LLMì„ ì‚¬ìš©í•œ ì¬ìˆœìœ„)
        for idx in range(len(candidates_top)):
            candidates_top[idx].update({'res_relevance': 1.0 - (idx * 0.1)})
        
        # ì ìˆ˜ì™€ ê´€ë ¨ì„±ìœ¼ë¡œ ì •ë ¬
        sorted_candidates_top = sorted(candidates_top, key=lambda x: (-x['res_relevance'], -x['res_score']))
        
        await publish_node_status("node_rc_rerank", "completed", {"result": sorted_candidates_top})
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": sorted_candidates_top
        }
    except Exception as e:
        print("[error]: node_rc_rerank")
        raise RuntimeError(f"[error]: node_rc_rerank: {str(e)}")

async def node_rc_answer(state: SearchState) -> SearchState:
    """ë‹µë³€ ìƒì„± ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_answer ì‹¤í–‰")
    
    try:
        cnt_result = 5
        candidates_top = state['response'][:cnt_result]
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if candidates_top:
            print(f"[Answer] âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. LLM API í˜¸ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ìƒìœ„ 1ê±´ì˜ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            top_result = candidates_top[0]
            top_payload = top_result.get('res_payload', {})
            
            # ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
            document_title = top_payload.get('ppt_title', 'ì œëª© ì—†ìŒ')
            document_content = top_payload.get('ppt_content', top_payload.get('ppt_summary', 'ë‚´ìš© ì—†ìŒ'))
            
            print(f"[Answer] ğŸ“„ RAG ë¬¸ì„œ ì •ë³´:")
            print(f"[Answer] ì œëª©: {document_title}")
            print(f"[Answer] ë‚´ìš© ê¸¸ì´: {len(document_content)} ë¬¸ì")
            print(f"[Answer] ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {document_content[:200]}...")
            
            # LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {document_content[:1000]}...

[ì§ˆë¬¸]
{state['question']}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- í˜•ì‹ì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
            """.strip()
            
            print(f"[Answer] ğŸ“ LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸:")
            print(f"[Answer] {prompt}")
            print(f"[Answer] ğŸ“Š í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            
            # OpenAI API í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„± - ìƒˆë¡œìš´ LLM ë°©ì‹
            llm_answer = ""
            try:
                if OPENAI_API_KEY:
                    print(f"[Answer] ğŸš€ LLM API í˜¸ì¶œ ì‹œì‘...")
                    
                    messages = [{"role": "user", "content": prompt}]
                    
                    # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
                    httpx_client = httpx.AsyncClient(verify=False, timeout=None)
                    
                    # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    client = AsyncOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url="https://api.openai.com/v1",
                        http_client=httpx_client,
                        default_headers={
                            "x-dep-ticket": OPENAI_API_KEY,
                            "Send-System-Name": "ds2llm",
                            "User-Id": "langgraph_answer",
                            "User-Type": "AD_ID",
                            "Prompt-Msg-Id": str(uuid.uuid4()),
                            "Completion-Msg-Id": str(uuid.uuid4()),
                        }
                    )
                    
                    # ë¹„ë™ê¸° í˜¸ì¶œ
                    response = await client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=messages,
                        max_tokens=1000,
                        temperature=0.7
                    )
                    raw_llm_answer = response.choices[0].message.content
                    print(f"[Answer] âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                    print(f"[Answer] ğŸ“¥ LLM ì›ì‹œ ì‘ë‹µ:")
                    print(f"[Answer] {raw_llm_answer}")
                    print(f"[Answer] ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(raw_llm_answer)} ë¬¸ì")
                    print(f"[Answer] ì‚¬ìš©ëœ í† í°: {response.usage.total_tokens if response.usage else 'N/A'}")
                    
                    # LLMì—ì„œ ë°›ì€ ê¹”ë”í•œ ë‹µë³€ì„ ë°”ë¡œ ì‚¬ìš©
                    llm_answer = raw_llm_answer.strip()
                else:
                    print(f"[Answer] âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {document_content[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
                    
            except Exception as e:
                print(f"[Answer] âŒ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                import traceback
                print(f"[Answer] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€ ìƒì„±
                llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {document_content[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"""
            
            print(f"[Answer] ğŸ¯ ìµœì¢… ìƒì„±ëœ ë‹µë³€:")
            print(f"[Answer] {llm_answer}")
            
            # LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ ìœ„í•œ ì™„ì „í•œ ì‘ë‹µ êµ¬ì¡°
            response = {
                "res_id": [rest['res_id'] for rest in candidates_top],
                "answer": llm_answer,  # LLMìœ¼ë¡œ ìƒì„±ëœ ì‹¤ì œ ë‹µë³€
                "q_mode": "search",  # ë­ê·¸ë˜í”„ëŠ” í•­ìƒ search ëª¨ë“œ
                "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
                "db_search_title": [item.get('res_payload', {}).get('ppt_title', 'ì œëª© ì—†ìŒ') for item in candidates_top],  # ê²€ìƒ‰ëœ ë¬¸ì„œ ì œëª©ë“¤
                "top_document": top_result
            }
        else:
            print(f"[Answer] âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            response = {
                "res_id": [],
                "answer": f"ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "q_mode": "search",
                "keyword": state["keyword"],
                "db_search_title": []
            }
        
        print(f"[Answer] ğŸ“¤ ìµœì¢… ì‘ë‹µ êµ¬ì¡°:")
        print(f"[Answer] {response}")
        
        # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
        # await save_langgraph_result_to_db(state['question'], response, state["keyword"], state["candidates_total"])
        
        await publish_node_status("node_rc_answer", "completed", {"result": response})
        
        print(f"[Answer] âœ… node_rc_answer í•¨ìˆ˜ ì™„ë£Œ")
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": response
        }
        
    except Exception as e:
        print("[error]: node_rc_answer")
        import traceback
        print(f"[error] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise RuntimeError(f"[error]: node_rc_answer: {str(e)}")

async def node_rc_plain_answer(state: SearchState) -> SearchState:
    """ê¸°ë³¸ ë‹µë³€ ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_plain_answer ì‹¤í–‰")
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë” êµ¬ì²´ì ì¸ ì•ˆë‚´ ë©”ì‹œì§€ ìƒì„±
    question = state['question']
    keywords = state.get('keyword', [])
    
    if isinstance(keywords, list) and len(keywords) > 1:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}"
    else:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}"
    
    detailed_answer = f"""ğŸ” **ë¶„ì„ ê²°ê³¼ ìš”ì•½**

**ì…ë ¥ ì§ˆë¬¸**: {question}

**í‚¤ì›Œë“œ ì¦ê°•**: {keyword_info}

**ê²€ìƒ‰ ê²°ê³¼**: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ê°œì„  ì œì•ˆ**:
1. **ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±**í•´ì£¼ì„¸ìš”
   - ì˜ˆ: "ì„±ê³¼ ê°œì„ " â†’ "2024ë…„ 1ë¶„ê¸° ì˜ì—…íŒ€ ì„±ê³¼ ê°œì„  ë°©ì•ˆ"
   - ì˜ˆ: "ì „ëµ ìˆ˜ë¦½" â†’ "ì‹ ì œí’ˆ ì¶œì‹œë¥¼ ìœ„í•œ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½"

2. **ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ê°€**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ í‚¤ì›Œë“œ: {keywords[:3] if isinstance(keywords, list) else keywords}
   - ì¶”ê°€ í‚¤ì›Œë“œ ì˜ˆì‹œ: êµ¬ì²´ì ì¸ ì—…ë¬´ ì˜ì—­, ê¸°ê°„, ë¶€ì„œëª… ë“±

3. **ë°ì´í„°ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ ì„¤ì •ëœ ë²¡í„° DB: {QDRANT_HOST}:{QDRANT_PORT}
   - ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}

ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤."""
    
    # Redisë¥¼ í†µí•´ ì™„ë£Œ ìƒíƒœ ë°œí–‰ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ DB ì €ì¥ì„ ìœ„í•´)
    complete_result = {
        "res_id": [], 
        "answer": detailed_answer,
        "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
        "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
        "db_search_title": []  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    }
    
    # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
    # await save_langgraph_result_to_db(state['question'], complete_result, state["keyword"], state["candidates_total"])
    
    await publish_node_status("node_rc_plain_answer", "completed", {"result": complete_result})
    
    return {
        "question": state['question'],
        "keyword": state["keyword"],
        "candidates_total": state["candidates_total"],
        "response": {
            "res_id": [], 
            "answer": detailed_answer,
            "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
            "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
            "db_search_title": []
        }
    }

def judge_rc_ragscore(state: SearchState) -> str:
    """RAG ì ìˆ˜ íŒë‹¨"""
    candidates_total = state["candidates_total"]
    return "Y" if any(candidate.get("res_score", 0) >= 0.5 for candidate in candidates_total) else "N"

async def save_langgraph_result_to_db(question: str, response: dict, keywords: list, candidates_total: list):
    """LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ DBì— ì§ì ‘ ì €ì¥ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    try:
        print(f"[DB_SAVE] LangGraph ê²°ê³¼ DB ì €ì¥ ì‹œì‘ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ì§ˆë¬¸: {question}")
        print(f"[DB_SAVE] ì‘ë‹µ: {response.get('answer', '')[:100]}...")
        print(f"[DB_SAVE] í‚¤ì›Œë“œ: {keywords}")
        print(f"[DB_SAVE] ë¬¸ì„œ: {len(candidates_total)}ê±´")
        
        # ìƒˆ ëŒ€í™” ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì°¾ê¸°
        db = next(get_db())
        
        # ìƒˆ ëŒ€í™” ìƒì„±
        conversation = Conversation(
            title=question[:50] + "..." if len(question) > 50 else question,
            user_id=1  # ê¸°ë³¸ ì‚¬ìš©ì ID (ì‹¤ì œë¡œëŠ” ì¸ì¦ëœ ì‚¬ìš©ì ID ì‚¬ìš©)
        )
        db.add(conversation)
        db.commit()
        db.refresh(conversation)
        
        # ë©”ì‹œì§€ ì €ì¥ (q_mode: 'search' - ë­ê·¸ë˜í”„ ì „ìš©)
        message = Message(
            conversation_id=conversation.id,
            role="user",
            question=question,
            ans=response.get('answer', ''),
            q_mode='search',  # ë­ê·¸ë˜í”„ ì „ìš© ëª¨ë“œ
            keyword=str(keywords) if keywords else None,
            db_search_title=str([item.get('res_payload', {}).get('ppt_title', '') for item in candidates_total[:5]]) if candidates_total else None
            # user_name í•„ë“œ ì œê±° - í•˜ë“œì½”ë”© ë°©ì§€
        )
        
        db.add(message)
        db.commit()
        
        print(f"[DB_SAVE] âœ… LangGraph ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ëŒ€í™” ID: {conversation.id}")
        print(f"[DB_SAVE] ë©”ì‹œì§€ ID: {message.id}")
        print(f"[DB_SAVE] q_mode: {message.q_mode} (ë­ê·¸ë˜í”„ ì „ìš©)")
        
    except Exception as e:
        print(f"[DB_SAVE] âŒ LangGraph ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"[DB_SAVE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")


# LangGraph êµ¬ì„±
def create_langgraph():
    """LangGraph ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("node_rc_init", node_rc_init)
    workflow.add_node("node_rc_keyword", node_rc_keyword)
    workflow.add_node("node_rc_rag", node_rc_rag)
    workflow.add_node("node_rc_rerank", node_rc_rerank)
    workflow.add_node("node_rc_answer", node_rc_answer)
    workflow.add_node("node_rc_plain_answer", node_rc_plain_answer)
    
    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("node_rc_init")
    workflow.add_edge("node_rc_init", "node_rc_keyword")
    workflow.add_edge("node_rc_keyword", "node_rc_rag")
    workflow.add_conditional_edges(
        "node_rc_rag",
        judge_rc_ragscore,
        {
            "Y": "node_rc_rerank",
            "N": "node_rc_plain_answer"
        }
    )
    workflow.add_edge("node_rc_rerank", "node_rc_answer")
    workflow.add_edge("node_rc_answer", END)
    workflow.add_edge("node_rc_plain_answer", END)
    
    return workflow.compile()

# LangGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
try:
    langgraph_instance = create_langgraph()
    print("[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨: {e}")
    langgraph_instance = None

# ê°„ë‹¨í•œ LLM ì‘ë‹µ í•¨ìˆ˜ (conversations.pyì—ì„œ ì‚¬ìš©)
async def get_llm_response(question: str) -> str:
    """ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    try:
        if not OPENAI_API_KEY:
            return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        print(f"[LLM_RESPONSE] ì§ˆë¬¸: {question}")
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": question}
        ]
        
        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        httpx_client = httpx.AsyncClient(verify=False, timeout=None)
        
        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url="https://api.openai.com/v1",
            http_client=httpx_client,
            default_headers={
                "x-dep-ticket": OPENAI_API_KEY,
                "Send-System-Name": "ds2llm",
                "User-Id": "conversation_api",
                "User-Type": "AD_ID",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            }
        )
        
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=1000,
            temperature=0.7
        )
        
        answer = response.choices[0].message.content
        print(f"[LLM_RESPONSE] ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(answer)}ì")
        return answer
        
    except Exception as e:
        print(f"[LLM_RESPONSE] ì˜¤ë¥˜: {str(e)}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"




# ì¼ë°˜ LLM ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (streaming ì§€ì›)
@router.post("/chat/stream")
async def stream_chat_with_llm(request: StreamRequest, http_request: Request, db: Session = Depends(get_db)):
    """Stream a response from general LLM chat using async method"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[Chat Stream] ========== LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‹œì‘ ==========")
        print(f"[Chat Stream] ìš”ì²­ ì •ë³´:")
        print(f"[Chat Stream] - ì§ˆë¬¸: {request.question}")
        print(f"[Chat Stream] - ëŒ€í™” ID: {request.conversation_id}")
        print(f"[Chat Stream] - conversation_id íƒ€ì…: {type(request.conversation_id)}")
        print(f"[Chat Stream] - conversation_idê°€ Noneì¸ê°€?: {request.conversation_id is None}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [{"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."}]
        
        if request.conversation_id:
            try:
                # í•´ë‹¹ ëŒ€í™”ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 10ê°œë§Œ)
                conversation_messages = db.query(Message).filter(
                    Message.conversation_id == request.conversation_id
                ).order_by(Message.created_at.asc()).limit(10).all()
                
                print(f"[Chat Stream] ì´ì „ ëŒ€í™” ë©”ì‹œì§€ {len(conversation_messages)}ê°œ ë¡œë“œ")
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ==========")
                
                # ì´ì „ ëŒ€í™”ë¥¼ messagesì— ì¶”ê°€
                for i, msg in enumerate(conversation_messages):
                    print(f"[Chat Stream] DB ë©”ì‹œì§€ {i+1}: ID={msg.id}, role={msg.role}, created_at={msg.created_at}")
                    if msg.question:
                        print(f"[Chat Stream] ì§ˆë¬¸: {msg.question}")
                        messages.append({"role": "user", "content": msg.question})
                    if msg.ans:
                        print(f"[Chat Stream] ë‹µë³€: {msg.ans}")
                        messages.append({"role": "assistant", "content": msg.ans})
                    print(f"[Chat Stream] ----------------------------------------")
                
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ ==========")
                        
            except Exception as e:
                print(f"[Chat Stream] ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[Chat Stream] ì „ì†¡í•  ë©”ì‹œì§€ ê°œìˆ˜: {len(messages)}")
        print(f"[Chat Stream] ========== OpenAI APIì— ì „ì†¡í•  ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ==========")
        print(f"[Chat Stream] ==========ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© : ")
        print(f"{messages}")
        
        print(f"[Chat Stream] ========== ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ë ==========")
        
        # ìƒˆë¡œìš´ GPT-3.5-turbo ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©
        return StreamingResponse(
            get_streaming_response_async(messages, http_request, request.generate_image or False),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[Chat Stream] LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[Chat Stream] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")



# ì§ˆë¬¸ ìœ í˜• íŒë³„ í•¨ìˆ˜
def is_first_question_in_conversation(conversation_id: int, db: Session) -> bool:
    """ëŒ€í™”ì—ì„œ ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
    try:
        message_count = db.query(Message).filter(Message.conversation_id == conversation_id).count()
        print(f"[QUESTION_TYPE] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ ìˆ˜: {message_count}")
        return message_count == 0
    except Exception as e:
        print(f"[QUESTION_TYPE] ë©”ì‹œì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ ì‹œ ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼

def get_conversation_context(conversation_id: int, db: Session) -> dict:
    """ëŒ€í™”ì˜ ì»¨í…ìŠ¤íŠ¸ì™€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # í•´ë‹¹ ëŒ€í™”ì˜ ëª¨ë“  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ìˆœ ì •ë ¬)
        messages = db.query(Message).filter(
            Message.conversation_id == conversation_id
        ).order_by(Message.created_at.asc()).all()
        
        print(f"[CONTEXT] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ {len(messages)}ê°œ ë¡œë“œ")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´
        if len(messages) > 0:
            print(f"[CONTEXT] ë©”ì‹œì§€ ìƒì„¸:")
            for i, msg in enumerate(messages):
                print(f"[CONTEXT]   {i+1}. ID: {msg.id}, q_mode: {msg.q_mode}, role: {msg.role}, question: {msg.question[:50] if msg.question else 'None'}...")
        else:
            print(f"[CONTEXT] âš ï¸ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€í™” ID {conversation_id} í™•ì¸ í•„ìš”")
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ì°¾ê¸° (q_modeê°€ "search"ì¸ ë©”ì‹œì§€)
        first_message = None
        for msg in messages:
            if msg.q_mode == "search":
                first_message = msg
                print(f"[CONTEXT] ì²« ë²ˆì§¸ ì§ˆë¬¸ ë°œê²¬: ë©”ì‹œì§€ ID {msg.id}")
                break
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation_history = []
        for msg in messages:
            if msg.question:
                conversation_history.append({"role": "user", "content": msg.question})
            if msg.ans:
                conversation_history.append({"role": "assistant", "content": msg.ans})
        
        return {
            "first_message": first_message,
            "conversation_history": conversation_history,
            "message_count": len(messages)
        }
        
    except Exception as e:
        print(f"[CONTEXT] ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return {
            "first_message": None,
            "conversation_history": [],
            "message_count": 0
        }

# LangGraph ì§ì ‘ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©)
@router.post("/langgraph")
async def execute_langgraph(request: StreamRequest, db: Session = Depends(get_db)):
    """LangGraphë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë°˜í™˜ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"[LangGraph] ğŸš€ ë­ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸
        if request.conversation_id:
            is_first = is_first_question_in_conversation(request.conversation_id, db)
            if not is_first:
                print(f"[LangGraph] âš ï¸ ì¶”ê°€ ì§ˆë¬¸ ê°ì§€ë¨ - LangGraph ì‹¤í–‰ ì°¨ë‹¨")
                raise HTTPException(
                    status_code=400, 
                    detail="ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
                )
        
        # ì›Œí¬í”Œë¡œìš° í™•ì¸
        if langgraph_instance is None:
            raise HTTPException(status_code=500, detail="LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        initial_state = {"question": request.question}
        print(f"[LangGraph] ì‹¤í–‰ ì‹œì‘: {request.question}")
        print(f"[LangGraph] ì´ˆê¸° ìƒíƒœ: {initial_state}")
        
        result = await langgraph_instance.ainvoke(initial_state)
        
        print(f"[LangGraph] âœ… ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ì—ì„œ íƒœê·¸ì™€ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
        tags = None
        db_search_title = None
        
        if isinstance(result, dict):
            # í‚¤ì›Œë“œ ì •ë³´ì—ì„œ íƒœê·¸ ì¶”ì¶œ
            if 'keyword' in result and result['keyword']:
                if isinstance(result['keyword'], list):
                    tags = ', '.join(result['keyword'])
                else:
                    tags = str(result['keyword'])
                print(f"[LangGraph] í‚¤ì›Œë“œ: {len(result['keyword'])}ê°œ")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
            if 'candidates_total' in result and result['candidates_total']:
                db_search_title = f"{len(result['candidates_total'])}ê±´"
                print(f"[LangGraph] ë¬¸ì„œ: {db_search_title}")
            
            # ì‘ë‹µ ì •ë³´ í™•ì¸
            if 'response' in result and result['response']:
                response_text = result['response'].get('answer', '')[:50] if isinstance(result['response'], dict) else str(result['response'])[:50]
                print(f"[LangGraph] ì‘ë‹µ: {response_text}...")
        
        print(f"[LangGraph] ìš”ì•½: í‚¤ì›Œë“œ {len(result.get('keyword', []))}ê°œ, ë¬¸ì„œ {len(result.get('candidates_total', []))}ê±´")
        
        return {
            "status": "success",
            "result": result,
            "tags": tags,
            "db_search_title": db_search_title,
            "message": "LangGraph ì‹¤í–‰ ì™„ë£Œ (ì²« ë²ˆì§¸ ì§ˆë¬¸)"
        }
        
    except Exception as e:
        print(f"[LangGraph] ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[LangGraph] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")


# ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.post("/langgraph/followup/stream")
async def execute_followup_question_stream(request: StreamRequest, http_request: Request, db: Session = Depends(get_db)):
    """ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ - ê¸°ì¡´ RAG ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œìš©"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[FOLLOWUP_STREAM] ğŸ”„ LLM ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” ID í™•ì¸
        if not request.conversation_id:
            return Response(content="Error: ì¶”ê°€ ì§ˆë¬¸ì€ conversation_idê°€ í•„ìš”í•©ë‹ˆë‹¤", media_type="text/plain")
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        context = get_conversation_context(request.conversation_id, db)
        
        if not context["first_message"]:
            print(f"[FOLLOWUP_STREAM] âš ï¸ ì²« ë²ˆì§¸ ì§ˆë¬¸ ì—†ìŒ - ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬")
            # ì²« ë²ˆì§¸ ì§ˆë¬¸ì´ ì—†ì–´ë„ ì¼ë°˜ì ì¸ ë‹µë³€ ì œê³µ
            document_title = "ì¼ë°˜ ëŒ€í™”"
            document_content = "ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        else:
            # ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œì™€ ë¬¸ì„œ ì •ë³´ í™œìš©
            first_message = context["first_message"]
            
            # ê¸°ë³¸ ë¬¸ì„œ ì •ë³´ (ì‹¤ì œ RAG ê²°ê³¼ê°€ ì—†ìœ¼ë¯€ë¡œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±)
            document_title = first_message.db_search_title or "ê´€ë ¨ ë¬¸ì„œ"
            document_content = f"í‚¤ì›Œë“œ: {first_message.keyword}\nê²€ìƒ‰ ê²°ê³¼: {first_message.db_search_title}\nì²« ë²ˆì§¸ ì§ˆë¬¸: {first_message.question}\nì²« ë²ˆì§¸ ë‹µë³€: {first_message.ans[:500] if first_message.ans else 'ë‹µë³€ ì—†ìŒ'}..."
        
        print(f"[FOLLOWUP_STREAM] ğŸ“„ ì¬ì‚¬ìš©í•  RAG ë¬¸ì„œ:")
        print(f"[FOLLOWUP_STREAM] ì œëª©: {document_title}")
        print(f"[FOLLOWUP_STREAM] ë‚´ìš© ê¸¸ì´: {len(document_content)} ë¬¸ì")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation_history = context["conversation_history"]
        print(f"[FOLLOWUP_STREAM] ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {document_content[:1500]}...

ìœ„ ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""
        
        # LLM API í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ)
        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
        messages.extend(recent_history)
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[FOLLOWUP_STREAM] ğŸ“¤ LLMì— ì „ì†¡í•  ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        print(f"[FOLLOWUP_STREAM] ğŸ“ í˜„ì¬ ì§ˆë¬¸: {request.question}")
        
        # ìƒˆë¡œìš´ GPT-3.5-turbo ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©
        return StreamingResponse(
            get_streaming_response_async(messages, http_request, request.generate_image or False),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[FOLLOWUP_STREAM] LLM ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[FOLLOWUP_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")

async def generate_image(prompt: str) -> str:
    """Generate an image using OpenAI DALL-E API"""
    try:
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ë¶€ë¶„ì—ì„œ OpenAI DALL-E API í˜¸ì¶œ
        # ì˜ˆì‹œ: 
        # client = OpenAI(api_key=OPENAI_API_KEY)
        # response = client.images.generate(prompt=prompt, n=1, size="1024x1024")
        # image_url = response.data[0].url
        
        # ì´ë¯¸ì§€ URL ë°˜í™˜ (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
        return None
    except Exception as e:
        print(f"Error generating image: {str(e)}")
        return None

async def get_streaming_response_async(messages: List[Dict], request: Request, generate_image: bool = False):
    """Stream a response from LLM using AsyncOpenAI with custom headers"""
    try:
        print(f"[LLM_STREAM] ğŸš€ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
        
        # ì´ë¯¸ì§€ URL (ì´ë¯¸ì§€ ìƒì„±ì´ ìš”ì²­ëœ ê²½ìš°)
        image_url = None
        if generate_image:
            # ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ë¡œì§ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            image_url = await generate_image(messages[-1]["content"] if messages else "")
        
        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        httpx_client = httpx.AsyncClient(verify=False, timeout=None)
        
        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url="https://api.openai.com/v1",
            http_client=httpx_client,
            default_headers={
                "x-dep-ticket": OPENAI_API_KEY,
                "Send-System-Name": "ds2llm",
                "User-Id": getattr(request, 'username', 'anonymous'),
                "User-Type": "AD_ID",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            }
        )
        
        print(f"[LLM_STREAM] ğŸ“¤ ë©”ì‹œì§€ ì „ì†¡: {len(messages)}ê°œ")
        
        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            stream=True
        )
        
        print(f"[LLM_STREAM] ğŸ“¥ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
        
        text_response = ""
        async for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                text_response += content
                
                try:
                    # ë¹„-ASCII ë¬¸ì í—ˆìš©, UTF-8 bytesë¡œ ì¦‰ì‹œ ì „ì†¡
                    payload = json.dumps({'content': content}, ensure_ascii=False)
                    yield (f"data: {payload}\n\n").encode("utf-8")
                    await asyncio.sleep(0.01)  # ì²­í¬ ì‚¬ì´ì— ì§€ì—° ì¶”ê°€í•˜ì—¬ ë‹¤ë¥¸ API ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•¨
                except (ConnectionResetError, BrokenPipeError, OSError, ConnectionAbortedError, ConnectionError) as e:
                    # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
                    print(f"[LLM_STREAM] Client disconnected during streaming lv2: {type(e).__name__}")
                    return
                except Exception as e:
                    print(f"[LLM_STREAM] Unexpected error during streaming lv1: {str(e)}")
                    return
        
        # í…ìŠ¤íŠ¸ ì‘ë‹µì´ ì™„ë£Œëœ í›„ ì´ë¯¸ì§€ URLì´ ìˆìœ¼ë©´ ì „ì†¡
        if image_url:
            try:
                response_data = {
                    "text": text_response,
                    "image_url": image_url
                }
                payload = json.dumps(response_data, ensure_ascii=False)
                yield (f"data: {payload}\n\n").encode("utf-8")
            except Exception as e:
                print(f"[LLM_STREAM] Error sending image data: {str(e)}")
        
        print(f"[LLM_STREAM] âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        yield "data: [DONE]\n\n".encode("utf-8")
        
    except Exception as e:
        print(f"[LLM_STREAM] Error in streaming response: {str(e)}")
        import traceback
        print(f"[LLM_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        try:
            error_payload = json.dumps({'error': str(e)}, ensure_ascii=False)
            yield (f"data: {error_payload}\n\n").encode("utf-8")
            yield "data: [DONE]\n\n".encode("utf-8")
        except Exception:
            # ì—ëŸ¬ ì „ì†¡ë„ ì‹¤íŒ¨í•œ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
            return
