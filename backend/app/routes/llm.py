import openai
import asyncio
import json
import base64
import random
import requests
from fastapi import APIRouter, HTTPException, Response, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, AsyncGenerator
import redis.asyncio as aioredis
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import END, StateGraph
from collections import defaultdict
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, SearchRequest, Filter
from app.utils.config import (
    OPENAI_API_KEY, CUSTOM_LLAMA_API_KEY, CUSTOM_LLAMA_API_ENDPOINT, CUSTOM_LLAMA_API_BASE,
    REDIS_HOST, REDIS_PORT, REDIS_CHANNEL,
    QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION
)
from app.database import get_db
from app.models import Conversation, Message
from sqlalchemy.orm import Session

from datetime import datetime

# Create router 
router = APIRouter()

# Redis ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    redis_client = aioredis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
    print(f"[Redis] Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {REDIS_HOST}:{REDIS_PORT}")
except Exception as e:
    print(f"[Redis] Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    redis_client = None

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”© í™•ì¸
print(f"[Config] OpenAI API Key: {'ì„¤ì •ë¨' if OPENAI_API_KEY else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
print(f"[Config] Qdrant: {QDRANT_HOST}:{QDRANT_PORT} (ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'})")

# ì„ë² ë”© ëª¨ë¸ (10ì°¨ì› ë²¡í„° ìƒì„±)
class SimpleEmbeddings:
    def __init__(self):
        self.dimension = 10  # Qdrant ì»¬ë ‰ì…˜ ì°¨ì›ì— ë§ì¶¤
        self.api_key = OPENAI_API_KEY
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """10ì°¨ì› ë²¡í„° ìƒì„± (Qdrant ì»¬ë ‰ì…˜ì— ë§ì¶¤)"""
        try:
            embeddings = []
            for text in texts:
                # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ 10ì°¨ì› ë²¡í„° ìƒì„±
                import hashlib
                import struct
                
                # í…ìŠ¤íŠ¸ë¥¼ í•´ì‹œë¡œ ë³€í™˜
                text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                
                # í•´ì‹œë¥¼ 10ê°œì˜ floatë¡œ ë³€í™˜
                vector = []
                for i in range(0, 40, 4):  # MD5ëŠ” 32ë°”ì´íŠ¸, 4ë°”ì´íŠ¸ì”© 8ê°œ ê·¸ë£¹
                    if i + 4 <= len(text_hash):
                        hex_group = text_hash[i:i+4]
                        # 16ì§„ìˆ˜ë¥¼ floatë¡œ ë³€í™˜ (0-1 ë²”ìœ„ë¡œ ì •ê·œí™”)
                        float_val = float(int(hex_group, 16)) / 65535.0
                        vector.append(float_val)
                
                # 10ì°¨ì›ì´ ë˜ë„ë¡ íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
                while len(vector) < 10:
                    vector.append(0.0)
                vector = vector[:10]
                
                embeddings.append(vector)
                
            print(f"[Embeddings] {len(texts)}ê°œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ {self.dimension}ì°¨ì› ë²¡í„° ìƒì„± ì™„ë£Œ")
            return embeddings
            
        except Exception as e:
            print(f"[Embeddings] ë²¡í„° ìƒì„± ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ 10ì°¨ì› ë²¡í„° ë°˜í™˜
            return [[0.0] * self.dimension for _ in texts]

# ë²¡í„° DB ê²€ìƒ‰ í•¨ìˆ˜ë“¤
async def rag_multivector(question_type: str, limit: int, queries: List[str], query_vectors: List[List[float]], 
                          ip: str, port: int, collection: str) -> List[dict]:
    """ë©€í‹°ë²¡í„° ê²€ìƒ‰ (Qdrant)"""
    try:
        print(f"[RAG] Qdrant ì—°ê²° ì‹œë„: {ip}:{port}, ì»¬ë ‰ì…˜: {collection}")
        
        # ë²¡í„° DB ì—°ê²° ì‹œë„
        client = QdrantClient(host=ip, port=port)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        try:
            collections = client.get_collections()
            print(f"[RAG] ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {[col.name for col in collections.collections]}")
            
            if not collection:
                print(f"[RAG] ì»¬ë ‰ì…˜ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
                
            if collection not in [col.name for col in collections.collections]:
                print(f"[RAG] ì»¬ë ‰ì…˜ '{collection}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return []
                
            print(f"[RAG] ì»¬ë ‰ì…˜ '{collection}' ì—°ê²° ì„±ê³µ")
            
        except Exception as e:
            print(f"[RAG] Qdrant ì»¬ë ‰ì…˜ í™•ì¸ ì˜¤ë¥˜: {e}")
            return []
        
        results = []
        for i, (query, vector) in enumerate(zip(queries, query_vectors)):
            try:
                print(f"[RAG] ë²¡í„° ê²€ìƒ‰ ì‹œì‘: query {i+1}/{len(queries)}")
                
                # ë²¡í„° ê²€ìƒ‰
                search_result = client.search(
                    collection_name=collection,
                    query_vector=vector,
                    limit=limit,
                    with_payload=True
                )
                
                print(f"[RAG] ê²€ìƒ‰ ê²°ê³¼: {len(search_result)}ê±´")
                
                for item in search_result:
                    results.append({
                        'res_id': item.id,
                        'res_score': item.score,
                        'type_question': question_type,
                        'type_vector': '',  # ë²¡í„° íƒ€ì…
                        'res_payload': item.payload
                    })
            except Exception as e:
                print(f"[RAG] ê°œë³„ ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜ (query {i}): {e}")
                continue
        
        print(f"[RAG] ì´ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´")
        return results
        
    except Exception as e:
        print(f"[RAG] Qdrant ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

async def rag_vector_qdrant(question_type: str, limit: int, queries: List[str], query_vectors: List[List[float]], 
                           ip: str, port: int, collection: str) -> List[dict]:
    """Qdrant ë²¡í„° ê²€ìƒ‰"""
    return await rag_multivector(question_type, limit, queries, query_vectors, ip, port, collection)

async def rag_payload_qdrant(question_type: str, limit: int, queries: List[str], query_vectors: List[List[float]], 
                             ip: str, port: int, collection: str) -> List[dict]:
    """Qdrant í˜ì´ë¡œë“œ ê²€ìƒ‰"""
    return await rag_multivector(question_type, limit, queries, query_vectors, ip, port, collection)



# Available models with added custom Llama models
AVAILABLE_MODELS = [
    {"name": "GPT-3.5 Turbo", "value": "gpt-3.5-turbo"},
    {"name": "GPT-4", "value": "gpt-4"},
    {"name": "Claude 3 Opus", "value": "claude-3-opus"},
    {"name": "Claude 3 Sonnet", "value": "claude-3-sonnet"},
    {"name": "Llama 4 Maverick", "value": "llama-4-maverick"}
]

# êµ¬ë²„ì „ OpenAI SDK ê°•ì œ ì‚¬ìš©
USING_NEW_SDK = False

# Custom API key update for Llama models only
class CustomApiKeyUpdate(BaseModel):
    api_key: str
    api_base: Optional[str] = None
    api_endpoint: Optional[str] = None

class Model(BaseModel):
    name: str
    value: str

# ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì„ ìœ„í•œ í´ë˜ìŠ¤
class StreamRequest(BaseModel):
    question: str
    conversation_id: Optional[int] = None
    generate_image: Optional[bool] = False  # ì´ë¯¸ì§€ ìƒì„± í”Œë˜ê·¸ ì¶”ê°€

# ì´ë¯¸ì§€ URL (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
# SAMPLE_IMAGE_URLS = []

# LangGraph ìƒíƒœ ì •ì˜
class SearchState(dict):
    question: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    keyword: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    candidates_each: List[dict] 
    candidates_total: List[dict] 
    response: List[dict]     # LLMì´ ìƒì„±í•œ ì‘ë‹µ

# Redis ìƒíƒœ ë°œí–‰ í•¨ìˆ˜
async def publish_node_status(node_name: str, status: str, data: dict):
    """Redisë¥¼ í†µí•´ ë…¸ë“œ ìƒíƒœë¥¼ ë°œí–‰"""
    if redis_client is None:
        print(f"[Redis] Redis í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - {node_name}: {status}")
        return
        
    try:
        message = {
            "node": node_name,
            "status": status,
            "data": data,
            "timestamp": asyncio.get_event_loop().time()
        }
        await redis_client.publish(REDIS_CHANNEL, json.dumps(message))
        print(f"[Redis] Published {node_name}: {status}")
    except Exception as e:
        print(f"[Redis] Error publishing status: {e}")
        # Redis ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì›Œí¬í”Œë¡œìš°ëŠ” ê³„ì† ì§„í–‰
        pass

# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
async def node_rc_init(state: SearchState) -> SearchState:
    """ì´ˆê¸°í™” ë…¸ë“œ"""
    print("[inform]: node_rc_init")
    try: 
        question = state['question']
        await publish_node_status("node_init", "completed", {"result": question})
        return {
            "question": question,
            "keyword": "",
            "candidates_each": [],
            "candidates_total": [],
            "response": []
        }
    except Exception as e:
        print("[error]: node_rc_init")
        raise RuntimeError(f"[error]: node_rc_init: {str(e)}")

async def node_rc_keyword(state: SearchState) -> SearchState:
    """í‚¤ì›Œë“œ ì¦ê°• ë…¸ë“œ - LLMì„ ì‚¬ìš©í•œ ë™ì  í‚¤ì›Œë“œ ìƒì„±"""
    print("[inform]: node_rc_keyword")
    try:
        question = state['question']
        
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your-openai-api-key-here":
            print("[error]: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            base_keywords = [question]
            await publish_node_status("node_rc_keyword", "completed", {"result": base_keywords})
            return {
                "question": state['question'],
                "keyword": base_keywords,
                "candidates_each": [],
                "candidates_total": [],
                "response": []
            }
        
        try:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¦ê°•
            # êµ¬ë²„ì „ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            openai.api_key = OPENAI_API_KEY
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ì „ë¬¸ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³ , ìµœëŒ€ 15ê°œê¹Œì§€ ìƒì„±í•˜ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”: {question}"}
                ],
                max_tokens=200,
                temperature=0.7
            )
            llm_response = response.choices[0].message.content
            
            # LLM ì‘ë‹µì„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            keywords_text = llm_response.strip()
            # ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³  ê° í‚¤ì›Œë“œ ì •ë¦¬
            augmented_keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            # ì›ë³¸ ì§ˆë¬¸ë„ í¬í•¨
            augmented_keywords.insert(0, question)
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 20ê°œë¡œ ì œí•œ
            augmented_keywords = list(dict.fromkeys(augmented_keywords))[:20]
            
            print(f"[inform]: LLMì„ í†µí•´ ìƒì„±ëœ í‚¤ì›Œë“œ: {len(augmented_keywords)}ê°œ")
            
        except Exception as llm_error:
            print(f"[error]: LLM í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {llm_error}")
            # LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©
            augmented_keywords = [question]
        
        await publish_node_status("node_rc_keyword", "completed", {"result": augmented_keywords})
        
        return {
            "question": state['question'],
            "keyword": augmented_keywords,
            "candidates_each": [],
            "candidates_total": [],
            "response": []
        }
    except Exception as e:
        print("[error]: node_rc_keyword")
        raise RuntimeError(f"[error]: node_rc_keyword: {str(e)}")

async def node_rc_rag(state: SearchState) -> SearchState:
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("[inform]: node_rc_rag")
    try:
        # ë²¡í„° DB ì„¤ì •
        ip, port, collection = QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        try:
            embeddings = SimpleEmbeddings()
        except Exception as e:
            print(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë²¡í„° ì‚¬ìš©
            embeddings = None
        
        # questionìœ¼ë¡œ ê²€ìƒ‰
        candidates_each = []
        if state.get('question') and embeddings:
            try:
                query_vectors = embeddings.embed_documents([state['question']])
                candidates_each.extend(await rag_multivector('question', 5, [state['question']], query_vectors, ip, port, collection))
            except Exception as e:
                print(f"Question ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # keywordë¡œ ê²€ìƒ‰ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬)
        if state.get('keyword') and embeddings:
            try:
                # keywordê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                if isinstance(state['keyword'], list):
                    keywords = state['keyword']
                else:
                    keywords = [state['keyword']]
                
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ í•„í„°ë§
                keywords = [k for k in keywords if k and isinstance(k, str) and k.strip()]
                
                if keywords:
                    query_vectors = embeddings.embed_documents(keywords)
                    candidates_each.extend(await rag_multivector('keyword', 2, keywords, query_vectors, ip, port, collection))
            except Exception as e:
                print(f"Keyword ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ë²¡í„° ê²€ìƒ‰ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        if not candidates_each:
            print("[RAG] ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ë°ì´í„° ìƒì„±
            candidates_each = [{
                'res_id': 'no_results',
                'res_score': 0.0,
                'res_payload': {
                    'title': 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ',
                    'content': 'ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”.',
                    'type': 'no_results'
                }
            }]
            print(f"[RAG] ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(candidates_each)}ê±´")

        # ê°€ì¤‘ í‰ê·  summation (ê¸°ì¡´ ë­ê·¸ë˜í”„ ì½”ë“œì™€ ë™ì¼)
        w_question = {
            'question': 1,
            'keyword': 1
        }
        w_vector = {
            'text': 1,
            'summary_purpose': 0.5,
            'summary_result': 0.5,
            'summary_fb': 0.5,
        }
        
        aggregated_scores = defaultdict(float)
        payloads = {}
        
        # candidates_eachê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if candidates_each:
            for item in candidates_each:
                try:
                    res_id = item.get('res_id')
                    score = item.get('res_score', 0.0)
                    type_question = item.get('type_question', '')
                    type_vector = item.get('type_vector', '')
                    
                    if res_id is not None:
                        aggregated_scores[res_id] += score * w_question.get(type_question, 1.0) * w_vector.get(type_vector, 1.0)
                        if res_id not in payloads:
                            payloads[res_id] = item.get('res_payload', {})
                except Exception as e:
                    print(f"ê°œë³„ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
        # ì •ë ¬ëœ ê²°ê³¼ ìƒì„± (list of dicts í˜•íƒœ) - ìƒìœ„ 5ê±´ë§Œ
        candidates_total = sorted(
            [{'res_id': res_id, 'res_score': aggregated_scores[res_id], 'res_payload': payloads[res_id]}
             for res_id in aggregated_scores],
            key=lambda x: x['res_score'],
            reverse=True
        )[:5]  # ìƒìœ„ 5ê±´ë§Œ ì„ íƒ
        
        print(f"[RAG] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 5ê±´):")
        for i, candidate in enumerate(candidates_total):
            title = candidate.get('res_payload', {}).get('ppt_title', 'ì œëª© ì—†ìŒ')
            summary = candidate.get('res_payload', {}).get('ppt_summary', 'ìš”ì•½ ì—†ìŒ')
            score = candidate.get('res_score', 0)
            print(f"  {i+1}. {title} - {summary} (ìœ ì‚¬ë„: {score:.4f})")
        
        await publish_node_status("node_rc_rag", "completed", {"result": candidates_total})
        
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_total": candidates_total,
            "response": []
        }
    except Exception as e:
        print(f"[error]: node_rc_rag: {str(e)}")
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ìƒíƒœ ë°˜í™˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê³„ì† ì§„í–‰
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_each": [],
            "candidates_total": [],
            "response": []
        }

async def node_rc_rerank(state: SearchState) -> SearchState:
    """ì¬ìˆœìœ„ ë…¸ë“œ"""
    print("[inform]: node_rc_rerank")
    try:
        cnt_result = 5
        candidates_top = state['candidates_total'][:cnt_result]
        
        # ì¬ìˆœìœ„ ì²˜ë¦¬ (ì‹¤ì œë¡œëŠ” LLMì„ ì‚¬ìš©í•œ ì¬ìˆœìœ„)
        for idx in range(len(candidates_top)):
            candidates_top[idx].update({'res_relevance': 1.0 - (idx * 0.1)})
        
        # ì ìˆ˜ì™€ ê´€ë ¨ì„±ìœ¼ë¡œ ì •ë ¬
        sorted_candidates_top = sorted(candidates_top, key=lambda x: (-x['res_relevance'], -x['res_score']))
        
        await publish_node_status("node_rc_rerank", "completed", {"result": sorted_candidates_top})
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": sorted_candidates_top
        }
    except Exception as e:
        print("[error]: node_rc_rerank")
        raise RuntimeError(f"[error]: node_rc_rerank: {str(e)}")

async def node_rc_answer(state: SearchState) -> SearchState:
    """ë‹µë³€ ìƒì„± ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_answer ì‹¤í–‰")
    
    try:
        cnt_result = 5
        candidates_top = state['response'][:cnt_result]
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if candidates_top:
            print(f"[Answer] âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. LLM API í˜¸ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ìƒìœ„ 1ê±´ì˜ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            top_result = candidates_top[0]
            top_payload = top_result.get('res_payload', {})
            
            # ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
            document_title = top_payload.get('ppt_title', 'ì œëª© ì—†ìŒ')
            document_content = top_payload.get('ppt_content', top_payload.get('ppt_summary', 'ë‚´ìš© ì—†ìŒ'))
            
            print(f"[Answer] ğŸ“„ RAG ë¬¸ì„œ ì •ë³´:")
            print(f"[Answer] ì œëª©: {document_title}")
            print(f"[Answer] ë‚´ìš© ê¸¸ì´: {len(document_content)} ë¬¸ì")
            print(f"[Answer] ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {document_content[:200]}...")
            
            # LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {document_content[:1000]}...

[ì§ˆë¬¸]
{state['question']}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- í˜•ì‹ì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
            """.strip()
            
            print(f"[Answer] ğŸ“ LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸:")
            print(f"[Answer] {prompt}")
            print(f"[Answer] ğŸ“Š í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            
            # OpenAI API í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±
            llm_answer = ""
            try:
                if OPENAI_API_KEY:
                    print(f"[Answer] ğŸš€ OpenAI API í˜¸ì¶œ ì‹œì‘...")
                    openai.api_key = OPENAI_API_KEY
                    response = openai.ChatCompletion.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=1000,
                        temperature=0.7
                    )
                    raw_llm_answer = response['choices'][0]['message']['content']
                    print(f"[Answer] âœ… OpenAI ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                    print(f"[Answer] ğŸ“¥ OpenAI ì›ì‹œ ì‘ë‹µ:")
                    print(f"[Answer] {raw_llm_answer}")
                    print(f"[Answer] ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(raw_llm_answer)} ë¬¸ì")
                    print(f"[Answer] ì‚¬ìš©ëœ í† í°: {response['usage']['total_tokens'] if response.get('usage') else 'N/A'}")
                    
                    # LLMì—ì„œ ë°›ì€ ê¹”ë”í•œ ë‹µë³€ì„ ë°”ë¡œ ì‚¬ìš©
                    llm_answer = raw_llm_answer.strip()
                else:
                    print(f"[Answer] âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {document_content[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
                    
            except Exception as e:
                print(f"[Answer] âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                import traceback
                print(f"[Answer] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€ ìƒì„±
                llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {document_content[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"""
            
            print(f"[Answer] ğŸ¯ ìµœì¢… ìƒì„±ëœ ë‹µë³€:")
            print(f"[Answer] {llm_answer}")
            
            # LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ ìœ„í•œ ì™„ì „í•œ ì‘ë‹µ êµ¬ì¡°
            response = {
                "res_id": [rest['res_id'] for rest in candidates_top],
                "answer": llm_answer,  # LLMìœ¼ë¡œ ìƒì„±ëœ ì‹¤ì œ ë‹µë³€
                "q_mode": "search",  # ë­ê·¸ë˜í”„ëŠ” í•­ìƒ search ëª¨ë“œ
                "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
                "db_search_title": [item.get('res_payload', {}).get('ppt_title', 'ì œëª© ì—†ìŒ') for item in candidates_top],  # ê²€ìƒ‰ëœ ë¬¸ì„œ ì œëª©ë“¤
                "top_document": top_result
            }
        else:
            print(f"[Answer] âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            response = {
                "res_id": [],
                "answer": f"ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "q_mode": "search",
                "keyword": state["keyword"],
                "db_search_title": []
            }
        
        print(f"[Answer] ğŸ“¤ ìµœì¢… ì‘ë‹µ êµ¬ì¡°:")
        print(f"[Answer] {response}")
        
        # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
        # await save_langgraph_result_to_db(state['question'], response, state["keyword"], state["candidates_total"])
        
        await publish_node_status("node_rc_answer", "completed", {"result": response})
        
        print(f"[Answer] âœ… node_rc_answer í•¨ìˆ˜ ì™„ë£Œ")
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": response
        }
        
    except Exception as e:
        print("[error]: node_rc_answer")
        import traceback
        print(f"[error] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise RuntimeError(f"[error]: node_rc_answer: {str(e)}")

async def node_rc_plain_answer(state: SearchState) -> SearchState:
    """ê¸°ë³¸ ë‹µë³€ ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_plain_answer ì‹¤í–‰")
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë” êµ¬ì²´ì ì¸ ì•ˆë‚´ ë©”ì‹œì§€ ìƒì„±
    question = state['question']
    keywords = state.get('keyword', [])
    
    if isinstance(keywords, list) and len(keywords) > 1:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}"
    else:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}"
    
    detailed_answer = f"""ğŸ” **ë¶„ì„ ê²°ê³¼ ìš”ì•½**

**ì…ë ¥ ì§ˆë¬¸**: {question}

**í‚¤ì›Œë“œ ì¦ê°•**: {keyword_info}

**ê²€ìƒ‰ ê²°ê³¼**: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ê°œì„  ì œì•ˆ**:
1. **ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±**í•´ì£¼ì„¸ìš”
   - ì˜ˆ: "ì„±ê³¼ ê°œì„ " â†’ "2024ë…„ 1ë¶„ê¸° ì˜ì—…íŒ€ ì„±ê³¼ ê°œì„  ë°©ì•ˆ"
   - ì˜ˆ: "ì „ëµ ìˆ˜ë¦½" â†’ "ì‹ ì œí’ˆ ì¶œì‹œë¥¼ ìœ„í•œ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½"

2. **ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ê°€**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ í‚¤ì›Œë“œ: {keywords[:3] if isinstance(keywords, list) else keywords}
   - ì¶”ê°€ í‚¤ì›Œë“œ ì˜ˆì‹œ: êµ¬ì²´ì ì¸ ì—…ë¬´ ì˜ì—­, ê¸°ê°„, ë¶€ì„œëª… ë“±

3. **ë°ì´í„°ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ ì„¤ì •ëœ ë²¡í„° DB: {QDRANT_HOST}:{QDRANT_PORT}
   - ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}

ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤."""
    
    # Redisë¥¼ í†µí•´ ì™„ë£Œ ìƒíƒœ ë°œí–‰ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ DB ì €ì¥ì„ ìœ„í•´)
    complete_result = {
        "res_id": [], 
        "answer": detailed_answer,
        "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
        "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
        "db_search_title": []  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    }
    
    # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
    # await save_langgraph_result_to_db(state['question'], complete_result, state["keyword"], state["candidates_total"])
    
    await publish_node_status("node_rc_plain_answer", "completed", {"result": complete_result})
    
    return {
        "question": state['question'],
        "keyword": state["keyword"],
        "candidates_total": state["candidates_total"],
        "response": {
            "res_id": [], 
            "answer": detailed_answer,
            "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
            "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
            "db_search_title": []
        }
    }

def judge_rc_ragscore(state: SearchState) -> str:
    """RAG ì ìˆ˜ íŒë‹¨"""
    candidates_total = state["candidates_total"]
    return "Y" if any(candidate.get("res_score", 0) >= 0.5 for candidate in candidates_total) else "N"

async def save_langgraph_result_to_db(question: str, response: dict, keywords: list, candidates_total: list):
    """LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ DBì— ì§ì ‘ ì €ì¥ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    try:
        print(f"[DB_SAVE] LangGraph ê²°ê³¼ DB ì €ì¥ ì‹œì‘ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ì§ˆë¬¸: {question}")
        print(f"[DB_SAVE] ì‘ë‹µ: {response.get('answer', '')[:100]}...")
        print(f"[DB_SAVE] í‚¤ì›Œë“œ: {keywords}")
        print(f"[DB_SAVE] ë¬¸ì„œ: {len(candidates_total)}ê±´")
        
        # ìƒˆ ëŒ€í™” ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì°¾ê¸°
        db = next(get_db())
        
        # ìƒˆ ëŒ€í™” ìƒì„±
        conversation = Conversation(
            title=question[:50] + "..." if len(question) > 50 else question,
            user_id=1  # ê¸°ë³¸ ì‚¬ìš©ì ID (ì‹¤ì œë¡œëŠ” ì¸ì¦ëœ ì‚¬ìš©ì ID ì‚¬ìš©)
        )
        db.add(conversation)
        db.commit()
        db.refresh(conversation)
        
        # ë©”ì‹œì§€ ì €ì¥ (q_mode: 'search' - ë­ê·¸ë˜í”„ ì „ìš©)
        message = Message(
            conversation_id=conversation.id,
            role="user",
            question=question,
            ans=response.get('answer', ''),
            q_mode='search',  # ë­ê·¸ë˜í”„ ì „ìš© ëª¨ë“œ
            keyword=str(keywords) if keywords else None,
            db_search_title=str([item.get('res_payload', {}).get('ppt_title', '') for item in candidates_total[:5]]) if candidates_total else None
            # user_name í•„ë“œ ì œê±° - í•˜ë“œì½”ë”© ë°©ì§€
        )
        
        db.add(message)
        db.commit()
        
        print(f"[DB_SAVE] âœ… LangGraph ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ëŒ€í™” ID: {conversation.id}")
        print(f"[DB_SAVE] ë©”ì‹œì§€ ID: {message.id}")
        print(f"[DB_SAVE] q_mode: {message.q_mode} (ë­ê·¸ë˜í”„ ì „ìš©)")
        
    except Exception as e:
        print(f"[DB_SAVE] âŒ LangGraph ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"[DB_SAVE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

async def generate_detailed_analysis(question: str, top_result: dict) -> str:
    """ìƒìœ„ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´ LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ë¶„ì„ ìƒì„±"""
    try:
        # ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ ì¶”ì¶œ
        doc_title = top_result.get('res_payload', {}).get('ppt_title', 'ì œëª© ì—†ìŒ')
        doc_summary = top_result.get('res_payload', {}).get('ppt_summary', 'ìš”ì•½ ì—†ìŒ')
        doc_content = top_result.get('res_payload', {}).get('ppt_content', 'ë‚´ìš© ì—†ìŒ')
        doc_score = top_result.get('res_score', 0)
        
        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸**: {question}

**ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´**:
- ì œëª©: {doc_title}
- ìš”ì•½: {doc_summary}
- ë‚´ìš©: {doc_content}
- ìœ ì‚¬ë„ ì ìˆ˜: {doc_score:.4f}

**ë‹µë³€ ìš”êµ¬ì‚¬í•­**:
1. ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€
2. êµ¬ì²´ì ì¸ ì •ë³´ì™€ ì˜ˆì‹œ í¬í•¨
3. ì‹¤ë¬´ì— ì ìš© ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
4. ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ê³ ë ¤ì‚¬í•­ ì œì‹œ

**ë‹µë³€ í˜•ì‹**:
- í•µì‹¬ ìš”ì•½ (2-3ì¤„)
- ìƒì„¸ ë¶„ì„ (5-7ì¤„)
- ì‹¤ë¬´ ì ìš© ë°©ì•ˆ (2-3ì¤„)
- ì¶”ê°€ ê³ ë ¤ì‚¬í•­ (1-2ì¤„)

ë¬¸ì„œ ë‚´ìš©ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        # OpenAI API í˜¸ì¶œ
        if OPENAI_API_KEY:
            response = await openai.ChatCompletion.acreate(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.7
            )
            
            detailed_answer = response.choices[0].message.content
            print(f"[LLM] ìƒì„¸ ë¶„ì„ ìƒì„± ì™„ë£Œ: {len(detailed_answer)}ì")
            return detailed_answer
        else:
            # OpenAI API í‚¤ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë‹µë³€ ìƒì„±
            basic_answer = f"""ğŸ” **ë¶„ì„ ê²°ê³¼ ìš”ì•½**

**ì…ë ¥ ì§ˆë¬¸**: {question}

**ê²€ìƒ‰ëœ ë¬¸ì„œ**: {doc_title}
**ìœ ì‚¬ë„ ì ìˆ˜**: {doc_score:.4f}

**ë¬¸ì„œ ìš”ì•½**: {doc_summary}

**ìƒì„¸ ë‚´ìš©**: {doc_content}

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
- ì´ ë¬¸ì„œëŠ” {doc_title}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤
- {doc_summary}ì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤
- {doc_content}ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤

**ì‹¤ë¬´ ì ìš© ë°©ì•ˆ**:
- ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {question}ì— ëŒ€í•œ í•´ê²°ì±…ì„ ëª¨ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- {doc_title} ì˜ì—­ì—ì„œì˜ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

**ì¶”ê°€ ê³ ë ¤ì‚¬í•­**:
- ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
- ê´€ë ¨ëœ ë‹¤ë¥¸ ë¬¸ì„œë‚˜ ìë£Œë„ í•¨ê»˜ ê²€í† í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤"""
            
            print(f"[LLM] ê¸°ë³¸ ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(basic_answer)}ì")
            return basic_answer
            
    except Exception as e:
        print(f"[LLM] ìƒì„¸ ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë‹µë³€ ë°˜í™˜
        return f"""ğŸ” **ë¶„ì„ ê²°ê³¼ ìš”ì•½**

**ì…ë ¥ ì§ˆë¬¸**: {question}

**ê²€ìƒ‰ëœ ë¬¸ì„œ**: {top_result.get('res_payload', {}).get('ppt_title', 'ì œëª© ì—†ìŒ')}
**ìœ ì‚¬ë„ ì ìˆ˜**: {top_result.get('res_score', 0):.4f}

**ë¬¸ì„œ ë‚´ìš©**: {top_result.get('res_payload', {}).get('ppt_content', 'ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')}

**ìƒì„¸ ë¶„ì„**: 
ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ìƒì„¸ ë¶„ì„ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 
ê¸°ë³¸ ì •ë³´ëŠ” ìœ„ì™€ ê°™ìœ¼ë©°, ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ë” êµ¬ì²´ì ì¸ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

# LangGraph êµ¬ì„±
def create_langgraph():
    """LangGraph ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("node_rc_init", node_rc_init)
    workflow.add_node("node_rc_keyword", node_rc_keyword)
    workflow.add_node("node_rc_rag", node_rc_rag)
    workflow.add_node("node_rc_rerank", node_rc_rerank)
    workflow.add_node("node_rc_answer", node_rc_answer)
    workflow.add_node("node_rc_plain_answer", node_rc_plain_answer)
    
    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("node_rc_init")
    workflow.add_edge("node_rc_init", "node_rc_keyword")
    workflow.add_edge("node_rc_keyword", "node_rc_rag")
    workflow.add_conditional_edges(
        "node_rc_rag",
        judge_rc_ragscore,
        {
            "Y": "node_rc_rerank",
            "N": "node_rc_plain_answer"
        }
    )
    workflow.add_edge("node_rc_rerank", "node_rc_answer")
    workflow.add_edge("node_rc_answer", END)
    workflow.add_edge("node_rc_plain_answer", END)
    
    return workflow.compile()

# LangGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
try:
    langgraph_instance = create_langgraph()
    print("[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨: {e}")
    langgraph_instance = None

@router.get("/models", response_model=List[Model])
def get_models():
    """Get list of available models"""
    return AVAILABLE_MODELS

@router.get("/test-embeddings")
async def test_embeddings():
    """ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    try:
        if not OPENAI_API_KEY:
            return {"status": "error", "message": "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        embeddings = SimpleEmbeddings()
        test_texts = ["í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸"]
        vectors = embeddings.embed_documents(test_texts)
        
        return {
            "status": "success", 
            "message": "ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ",
            "vector_dimension": len(vectors[0]) if vectors else 0
        }
    except Exception as e:
        return {"status": "error", "message": f"ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"}

@router.get("/test-vector-db")
async def test_vector_db():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        # Qdrant ì—°ê²° í…ŒìŠ¤íŠ¸
        qdrant_status = "ì—°ê²° ì•ˆë¨"
        try:
            client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
            collections = client.get_collections()
            qdrant_status = f"ì—°ê²°ë¨ (ì»¬ë ‰ì…˜ ìˆ˜: {len(collections.collections)})"
        except Exception as e:
            qdrant_status = f"ì—°ê²° ì‹¤íŒ¨: {str(e)}"
        
        
        return {
            "status": "success",
            "qdrant": {
                "host": QDRANT_HOST,
                "port": QDRANT_PORT,
                "collection": QDRANT_COLLECTION,
                "status": qdrant_status
            }
        }
    except Exception as e:
        return {"status": "error", "message": f"ë²¡í„° DB í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"}

@router.get("/test-langgraph")
async def test_langgraph():
    """LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    try:
        # ì›Œí¬í”Œë¡œìš° ìƒíƒœ í™•ì¸
        workflow_info = {
            "nodes": ["node_rc_init", "node_rc_keyword", "node_rc_rag", "node_rc_rerank", "node_rc_answer", "node_rc_plain_answer"],
            "entry_point": "node_rc_init",
            "end_points": ["END"]
        }
        
        # ì›Œí¬í”Œë¡œìš° í™•ì¸
        if langgraph_instance is None:
            return {
                "status": "error",
                "workflow_info": workflow_info,
                "workflow_status": "ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ",
                "result_summary": {}
            }
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_state = {"question": "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸"}
        try:
            result = await langgraph_instance.ainvoke(test_state)
            workflow_status = "ì‹¤í–‰ ì„±ê³µ"
            result_summary = {
                "question": result.get("question", ""),
                "keyword_count": len(result.get("keyword", [])) if isinstance(result.get("keyword"), list) else 1,
                "candidates_count": len(result.get("candidates_total", [])),
                "response_count": len(result.get("response", []))
            }
        except Exception as e:
            workflow_status = f"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            result_summary = {}
        
        return {
            "status": "success",
            "workflow_info": workflow_info,
            "workflow_status": workflow_status,
            "result_summary": result_summary
        }
    except Exception as e:
        return {"status": "error", "message": f"LangGraph í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"}

# OpenAI API í‚¤ëŠ” config.pyì—ì„œ ê³ ì •ìœ¼ë¡œ ì„¤ì •ë¨ - ëŸ°íƒ€ì„ ë³€ê²½ ë¶ˆê°€

@router.post("/set-custom-api-key")
def set_custom_api_key(key_data: CustomApiKeyUpdate):
    """Set the custom LLM API key and endpoints at runtime (Llama models only)"""
    global CUSTOM_LLAMA_API_KEY, CUSTOM_LLAMA_API_BASE, CUSTOM_LLAMA_API_ENDPOINT
    try:
        # Set the new API key for custom Llama models only
        CUSTOM_LLAMA_API_KEY = key_data.api_key
        
        # Optionally update API base and endpoint
        if key_data.api_base:
            CUSTOM_LLAMA_API_BASE = key_data.api_base
        if key_data.api_endpoint:
            CUSTOM_LLAMA_API_ENDPOINT = key_data.api_endpoint
        
        # We don't test the API key here as it might require special headers
        # that we don't know about in advance
        
        return {
            "status": "success", 
            "message": "Custom API key updated successfully (Llama models only)",
            "api_base": CUSTOM_LLAMA_API_BASE,
            "api_endpoint": CUSTOM_LLAMA_API_ENDPOINT
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error setting custom API key: {str(e)}")

def get_llm_response(prompt: str) -> str:
    """Get a response from the selected LLM model"""
    model = "gpt-3.5-turbo"  # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    
    # Check if API key is set
    if model.startswith("gpt") and not OPENAI_API_KEY:
        return "Error: OpenAI API key not set. Please set your API key in the settings."
    
    # Handle different model providers
    if model.startswith("gpt"):
        return get_openai_response(prompt, model)
    elif model.startswith("claude"):
        # Claude API í†µí•©ì´ í•„ìš”í•©ë‹ˆë‹¤
        return f"Error: Claude API integration not implemented yet. Model: {model}"
    elif model.startswith("llama"):
        return get_custom_llama_response(prompt, model)
    else:
        return f"Unknown model: {model}. Please select a supported model."

def get_custom_llama_response(prompt: str, model: str = "llama-4-maverick") -> str:
    """Get a response from custom Llama API"""
    try:
        # Construct the full API URL
        api_url = f"{CUSTOM_LLAMA_API_BASE}{CUSTOM_LLAMA_API_ENDPOINT}"
        
        # Prepare the headers
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {CUSTOM_LLAMA_API_KEY}"
        }
        
        # Prepare the request body
        # Note: Adjust this format according to your API's requirements
        data = {
            "prompt": prompt,
            "max_tokens": 500,
            "temperature": 0.7,
            "model": model,  # This might need adjustment based on your API
            "stream": False
        }
        
        # Make the API call
        response = requests.post(api_url, headers=headers, json=data, timeout=60)
        
        # Check for successful response
        if response.status_code == 200:
            result = response.json()
            # Extract the generated text (adjust key names based on your API response structure)
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0].get("text", "No response text found")
            return "No valid response from Llama API"
        else:
            return f"Error from Llama API: HTTP {response.status_code}"
            
    except Exception as e:
        print(f"Error calling Llama API: {str(e)}")
        return f"Error: {str(e)}"

def get_openai_response(prompt: str, model: str = "gpt-3.5-turbo") -> str:
    """Get a response from OpenAI API"""
    try:
        if USING_NEW_SDK:
            # New SDK style (>1.0)
            try:
                client = OpenAI(api_key=OPENAI_API_KEY)
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1000
                )
                return response.choices[0].message.content
            except Exception as e:
                print(f"New SDK error: {e}, trying old SDK...")
                # New SDK ì‹¤íŒ¨ ì‹œ old SDK ì‹œë„
                openai.api_key = OPENAI_API_KEY
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1000
                )
                return response.choices[0].message.content
        else:
            # Old SDK style (<1.0)
            openai.api_key = OPENAI_API_KEY
            response = openai.ChatCompletion.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000
            )
            return response.choices[0].message.content
    except Exception as e:
        print(f"Error calling OpenAI API: {str(e)}")
        return f"Error: {str(e)}"

# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ìƒˆë¡œìš´ ì—”ë“œí¬ì¸íŠ¸
@router.post("/stream")
async def stream_response(request: StreamRequest):
    """Stream a response from the LLM model using LangGraph"""
    try:
        # ì›Œí¬í”Œë¡œìš° í™•ì¸
        if langgraph_instance is None:
            return Response(content="Error: LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        # LangGraph ì‹¤í–‰
        initial_state = {"question": request.question}
        
        # ë¹„ë™ê¸°ë¡œ LangGraph ì‹¤í–‰
        result = await langgraph_instance.ainvoke(initial_state)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        async def stream_generator():
            # ê° ë…¸ë“œì˜ ì§„í–‰ ìƒí™©ì„ ìŠ¤íŠ¸ë¦¬ë°
            yield f"data: {json.dumps({'type': 'start', 'message': 'LangGraph ì‹¤í–‰ ì‹œì‘'})}\n\n"
            
            # ìµœì¢… ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
            if 'response' in result and 'answer' in result['response']:
                answer = result['response']['answer']
                # ë‹µë³€ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
                words = answer.split()
                for word in words:
                    yield f"data: {word} \n\n"
                    await asyncio.sleep(0.1)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼
            
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream"
        )
        
    except Exception as e:
        print(f"Error in LangGraph streaming: {str(e)}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")

# ì¼ë°˜ LLM ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (streaming ì§€ì›)
@router.post("/chat/stream")
async def stream_chat_with_llm(request: StreamRequest, db: Session = Depends(get_db)):
    """Stream a response from general LLM chat (without LangGraph)"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[Chat Stream] ========== ì¼ë°˜ LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‹œì‘ ==========")
        print(f"[Chat Stream] ìš”ì²­ ì •ë³´:")
        print(f"[Chat Stream] - ì§ˆë¬¸: {request.question}")
        print(f"[Chat Stream] - ëŒ€í™” ID: {request.conversation_id}")
        print(f"[Chat Stream] - conversation_id íƒ€ì…: {type(request.conversation_id)}")
        print(f"[Chat Stream] - conversation_idê°€ Noneì¸ê°€?: {request.conversation_id is None}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [{"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."}]
        
        if request.conversation_id:
            try:
                # í•´ë‹¹ ëŒ€í™”ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 10ê°œë§Œ)
                conversation_messages = db.query(Message).filter(
                    Message.conversation_id == request.conversation_id
                ).order_by(Message.created_at.asc()).limit(10).all()
                
                print(f"[Chat Stream] ì´ì „ ëŒ€í™” ë©”ì‹œì§€ {len(conversation_messages)}ê°œ ë¡œë“œ")
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ==========")
                
                # ì´ì „ ëŒ€í™”ë¥¼ messagesì— ì¶”ê°€
                for i, msg in enumerate(conversation_messages):
                    print(f"[Chat Stream] DB ë©”ì‹œì§€ {i+1}: ID={msg.id}, role={msg.role}, created_at={msg.created_at}")
                    if msg.question:
                        print(f"[Chat Stream] ì§ˆë¬¸: {msg.question}")
                        messages.append({"role": "user", "content": msg.question})
                    if msg.ans:
                        print(f"[Chat Stream] ë‹µë³€: {msg.ans}")
                        messages.append({"role": "assistant", "content": msg.ans})
                    print(f"[Chat Stream] ----------------------------------------")
                
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ ==========")
                        
            except Exception as e:
                print(f"[Chat Stream] ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[Chat Stream] ì „ì†¡í•  ë©”ì‹œì§€ ê°œìˆ˜: {len(messages)}")
        print(f"[Chat Stream] ========== OpenAI APIì— ì „ì†¡í•  ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ==========")
        print(f"[Chat Stream] ==========ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© : ")
        print(f"{messages}")
        
        print(f"[Chat Stream] ========== ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ë ==========")
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        async def stream_generator():
            try:
                print(f"[Chat Stream] OpenAI API ìŠ¤íŠ¸ë¦¼ ìƒì„± ì‹œì‘...")
                # Old SDK style (<1.0) ê°•ì œ ì‚¬ìš©
                openai.api_key = OPENAI_API_KEY
                stream = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=messages,
                    max_tokens=1000,
                    temperature=0.7,
                    stream=True
                )
                print(f"[Chat Stream] OpenAI API ìŠ¤íŠ¸ë¦¼ ìƒì„± ì™„ë£Œ, ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
                
                chunk_count = 0
                for chunk in stream:
                    chunk_count += 1
                    # print(f"[Chat Stream] ì²­í¬ {chunk_count} ìˆ˜ì‹ : {chunk}")
                    if chunk['choices'][0].get('delta', {}).get('content'):
                        content = chunk['choices'][0]['delta']['content']
                        # print(f"[Chat Stream] ì²­í¬ {chunk_count} ë‚´ìš©: '{content}'")
                        yield f"data: {content}\n\n"
                    await asyncio.sleep(0.01)  # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì§§ì€ ì§€ì—°
                
                # print(f"[Chat Stream] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ, ì´ {chunk_count}ê°œ ì²­í¬ ì²˜ë¦¬")
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                print(f"[Chat Stream] ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {str(e)}")
                import traceback
                print(f"[Chat Stream] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                yield f"data: Error: {str(e)}\n\n"
                yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[Chat Stream] ì¼ë°˜ LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[Chat Stream] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")

# ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@router.get("/chat/stream/test")
async def test_streaming():
    """ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    async def test_stream_generator():
        try:
            print("[Test Stream] í…ŒìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
            for i in range(5):
                message = f"í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ {i+1}"
                print(f"[Test Stream] ì „ì†¡: {message}")
                yield f"data: {message}\n\n"
                await asyncio.sleep(1)  # 1ì´ˆ ê°„ê²©ìœ¼ë¡œ ì „ì†¡
            
            print("[Test Stream] í…ŒìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
            yield "data: [DONE]\n\n"
        except Exception as e:
            print(f"[Test Stream] ì˜¤ë¥˜: {str(e)}")
            yield f"data: Error: {str(e)}\n\n"
            yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        test_stream_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type, Authorization",
        }
    )

# ê¸°ì¡´ ë¹„-ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
@router.post("/chat")
async def chat_with_llm(request: StreamRequest, db: Session = Depends(get_db)):
    """ì¼ë°˜ LLMì„ ì‚¬ìš©í•œ ì±„íŒ… ì‘ë‹µ (ë­ê·¸ë˜í”„ ì—†ì´)"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"[Chat] ========== ì¼ë°˜ LLM ì±„íŒ… ì‹œì‘ (ë¹„-ìŠ¤íŠ¸ë¦¬ë°) ==========")
        print(f"[Chat] ìš”ì²­ ì •ë³´:")
        print(f"[Chat] - ì§ˆë¬¸: {request.question}")
        print(f"[Chat] - ëŒ€í™” ID: {request.conversation_id}")
        print(f"[Chat] - conversation_id íƒ€ì…: {type(request.conversation_id)}")
        print(f"[Chat] - conversation_idê°€ Noneì¸ê°€?: {request.conversation_id is None}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [{"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."}]
        
        if request.conversation_id:
            try:
                # í•´ë‹¹ ëŒ€í™”ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 10ê°œë§Œ)
                conversation_messages = db.query(Message).filter(
                    Message.conversation_id == request.conversation_id
                ).order_by(Message.created_at.asc()).limit(10).all()
                
                print(f"[Chat] ì´ì „ ëŒ€í™” ë©”ì‹œì§€ {len(conversation_messages)}ê°œ ë¡œë“œ")
                print(f"[Chat] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ==========")
                
                # ì´ì „ ëŒ€í™”ë¥¼ messagesì— ì¶”ê°€
                for i, msg in enumerate(conversation_messages):
                    print(f"[Chat] DB ë©”ì‹œì§€ {i+1}: ID={msg.id}, role={msg.role}, created_at={msg.created_at}")
                    if msg.question:
                        print(f"[Chat] ì§ˆë¬¸: {msg.question}")
                        messages.append({"role": "user", "content": msg.question})
                    if msg.ans:
                        print(f"[Chat] ë‹µë³€: {msg.ans}")
                        messages.append({"role": "assistant", "content": msg.ans})
                    print(f"[Chat] ----------------------------------------")
                
                print(f"[Chat] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ ==========")
                        
            except Exception as e:
                print(f"[Chat] ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[Chat] ì „ì†¡í•  ë©”ì‹œì§€ ê°œìˆ˜: {len(messages)}")
        print(f"[Chat] ========== OpenAI APIì— ì „ì†¡í•  ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ==========")
        for i, msg in enumerate(messages):
            print(f"[Chat] ë©”ì‹œì§€ {i+1}/{len(messages)}: role={msg['role']}")
            print(f"[Chat] ë‚´ìš©: {msg['content']}")
            print(f"[Chat] ----------------------------------------")
        print(f"[Chat] ========== ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ë ==========")
        
        # OpenAI API í˜¸ì¶œ (êµ¬ë²„ì „ ì‚¬ìš©)
        openai.api_key = OPENAI_API_KEY
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=1000,
            temperature=0.7
        )
        answer = response.choices[0].message.content
        print(f"[Chat] âœ… êµ¬ë²„ì „ OpenAI ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        
        return {
            "status": "success",
            "response": answer,
            "message": "ì¼ë°˜ LLM ì±„íŒ… ì™„ë£Œ"
        }
        
    except Exception as e:
        print(f"[Chat] ì¼ë°˜ LLM ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[Chat] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ì¼ë°˜ LLM ì±„íŒ… ì˜¤ë¥˜: {str(e)}")

# ì§ˆë¬¸ ìœ í˜• íŒë³„ í•¨ìˆ˜
def is_first_question_in_conversation(conversation_id: int, db: Session) -> bool:
    """ëŒ€í™”ì—ì„œ ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
    try:
        message_count = db.query(Message).filter(Message.conversation_id == conversation_id).count()
        print(f"[QUESTION_TYPE] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ ìˆ˜: {message_count}")
        return message_count == 0
    except Exception as e:
        print(f"[QUESTION_TYPE] ë©”ì‹œì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ ì‹œ ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼

def get_conversation_context(conversation_id: int, db: Session) -> dict:
    """ëŒ€í™”ì˜ ì»¨í…ìŠ¤íŠ¸ì™€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # í•´ë‹¹ ëŒ€í™”ì˜ ëª¨ë“  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ìˆœ ì •ë ¬)
        messages = db.query(Message).filter(
            Message.conversation_id == conversation_id
        ).order_by(Message.created_at.asc()).all()
        
        print(f"[CONTEXT] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ {len(messages)}ê°œ ë¡œë“œ")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´
        if len(messages) > 0:
            print(f"[CONTEXT] ë©”ì‹œì§€ ìƒì„¸:")
            for i, msg in enumerate(messages):
                print(f"[CONTEXT]   {i+1}. ID: {msg.id}, q_mode: {msg.q_mode}, role: {msg.role}, question: {msg.question[:50] if msg.question else 'None'}...")
        else:
            print(f"[CONTEXT] âš ï¸ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€í™” ID {conversation_id} í™•ì¸ í•„ìš”")
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ì°¾ê¸° (q_modeê°€ "search"ì¸ ë©”ì‹œì§€)
        first_message = None
        for msg in messages:
            if msg.q_mode == "search":
                first_message = msg
                print(f"[CONTEXT] ì²« ë²ˆì§¸ ì§ˆë¬¸ ë°œê²¬: ë©”ì‹œì§€ ID {msg.id}")
                break
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation_history = []
        for msg in messages:
            if msg.question:
                conversation_history.append({"role": "user", "content": msg.question})
            if msg.ans:
                conversation_history.append({"role": "assistant", "content": msg.ans})
        
        return {
            "first_message": first_message,
            "conversation_history": conversation_history,
            "message_count": len(messages)
        }
        
    except Exception as e:
        print(f"[CONTEXT] ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return {
            "first_message": None,
            "conversation_history": [],
            "message_count": 0
        }

# LangGraph ì§ì ‘ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©)
@router.post("/langgraph")
async def execute_langgraph(request: StreamRequest, db: Session = Depends(get_db)):
    """LangGraphë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë°˜í™˜ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"[LangGraph] ğŸš€ ë­ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸
        if request.conversation_id:
            is_first = is_first_question_in_conversation(request.conversation_id, db)
            if not is_first:
                print(f"[LangGraph] âš ï¸ ì¶”ê°€ ì§ˆë¬¸ ê°ì§€ë¨ - LangGraph ì‹¤í–‰ ì°¨ë‹¨")
                raise HTTPException(
                    status_code=400, 
                    detail="ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
                )
        
        # ì›Œí¬í”Œë¡œìš° í™•ì¸
        if langgraph_instance is None:
            raise HTTPException(status_code=500, detail="LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        initial_state = {"question": request.question}
        print(f"[LangGraph] ì‹¤í–‰ ì‹œì‘: {request.question}")
        print(f"[LangGraph] ì´ˆê¸° ìƒíƒœ: {initial_state}")
        
        result = await langgraph_instance.ainvoke(initial_state)
        
        print(f"[LangGraph] âœ… ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ì—ì„œ íƒœê·¸ì™€ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
        tags = None
        db_search_title = None
        
        if isinstance(result, dict):
            # í‚¤ì›Œë“œ ì •ë³´ì—ì„œ íƒœê·¸ ì¶”ì¶œ
            if 'keyword' in result and result['keyword']:
                if isinstance(result['keyword'], list):
                    tags = ', '.join(result['keyword'])
                else:
                    tags = str(result['keyword'])
                print(f"[LangGraph] í‚¤ì›Œë“œ: {len(result['keyword'])}ê°œ")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
            if 'candidates_total' in result and result['candidates_total']:
                db_search_title = f"{len(result['candidates_total'])}ê±´"
                print(f"[LangGraph] ë¬¸ì„œ: {db_search_title}")
            
            # ì‘ë‹µ ì •ë³´ í™•ì¸
            if 'response' in result and result['response']:
                response_text = result['response'].get('answer', '')[:50] if isinstance(result['response'], dict) else str(result['response'])[:50]
                print(f"[LangGraph] ì‘ë‹µ: {response_text}...")
        
        print(f"[LangGraph] ìš”ì•½: í‚¤ì›Œë“œ {len(result.get('keyword', []))}ê°œ, ë¬¸ì„œ {len(result.get('candidates_total', []))}ê±´")
        
        return {
            "status": "success",
            "result": result,
            "tags": tags,
            "db_search_title": db_search_title,
            "message": "LangGraph ì‹¤í–‰ ì™„ë£Œ (ì²« ë²ˆì§¸ ì§ˆë¬¸)"
        }
        
    except Exception as e:
        print(f"[LangGraph] ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[LangGraph] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")

# ì¶”ê°€ ì§ˆë¬¸ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸ (RAG ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš©)
@router.post("/langgraph/followup")
async def execute_followup_question(request: StreamRequest, db: Session = Depends(get_db)):
    """ì¶”ê°€ ì§ˆë¬¸ ì²˜ë¦¬ - ê¸°ì¡´ RAG ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œìš©"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"[FOLLOWUP] ğŸ”„ ì¶”ê°€ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” ID í™•ì¸
        if not request.conversation_id:
            raise HTTPException(status_code=400, detail="ì¶”ê°€ ì§ˆë¬¸ì€ conversation_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ê²€ì¦ ì œê±° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì„¸ì…˜ ê¸°ë°˜ìœ¼ë¡œ ê´€ë¦¬)
        print(f"[FOLLOWUP] ğŸ“ ì¶”ê°€ ì§ˆë¬¸ ì²˜ë¦¬ (ì„¸ì…˜ ê¸°ë°˜ ê´€ë¦¬ë¡œ ì¸í•´ ë°±ì—”ë“œ ê²€ì¦ ìƒëµ)")
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        context = get_conversation_context(request.conversation_id, db)
        
        if not context["first_message"]:
            print(f"[FOLLOWUP] âš ï¸ ì²« ë²ˆì§¸ ì§ˆë¬¸ ì—†ìŒ - ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬")
            # ì²« ë²ˆì§¸ ì§ˆë¬¸ì´ ì—†ì–´ë„ ì¼ë°˜ì ì¸ ë‹µë³€ ì œê³µ
            document_title = "ì¼ë°˜ ëŒ€í™”"
            document_content = "ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        else:
            # ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œì™€ ë¬¸ì„œ ì •ë³´ í™œìš©
            first_message = context["first_message"]
            
            # ê¸°ë³¸ ë¬¸ì„œ ì •ë³´ (ì‹¤ì œ RAG ê²°ê³¼ê°€ ì—†ìœ¼ë¯€ë¡œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±)
            document_title = first_message.db_search_title or "ê´€ë ¨ ë¬¸ì„œ"
            document_content = f"í‚¤ì›Œë“œ: {first_message.keyword}\nê²€ìƒ‰ ê²°ê³¼: {first_message.db_search_title}\nì²« ë²ˆì§¸ ì§ˆë¬¸: {first_message.question}\nì²« ë²ˆì§¸ ë‹µë³€: {first_message.ans[:500] if first_message.ans else 'ë‹µë³€ ì—†ìŒ'}..."
        
        print(f"[FOLLOWUP] ğŸ“„ ì¬ì‚¬ìš©í•  RAG ë¬¸ì„œ:")
        print(f"[FOLLOWUP] ì œëª©: {document_title}")
        print(f"[FOLLOWUP] ë‚´ìš© ê¸¸ì´: {len(document_content)} ë¬¸ì")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation_history = context["conversation_history"]
        print(f"[FOLLOWUP] ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {document_content[:1500]}...

ìœ„ ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""
        
        # LLM API í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ)
        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
        messages.extend(recent_history)
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[FOLLOWUP] ğŸ“¤ LLMì— ì „ì†¡í•  ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        print(f"[FOLLOWUP] ğŸ“ í˜„ì¬ ì§ˆë¬¸: {request.question}")
        
        # OpenAI API í˜¸ì¶œ
        try:
            openai.api_key = OPENAI_API_KEY
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content
            print(f"[FOLLOWUP] âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(answer)} ë¬¸ì")
            
        except Exception as e:
            print(f"[FOLLOWUP] âŒ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì¶”ê°€ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        # ì‘ë‹µ êµ¬ì„± (ì¶”ê°€ ì§ˆë¬¸ ëª¨ë“œ)
        result = {
            "res_id": [],
            "answer": answer,
            "q_mode": "add",  # ì¶”ê°€ ì§ˆë¬¸ ëª¨ë“œ
            "keyword": context["first_message"].keyword if context["first_message"] else None,
            "db_search_title": context["first_message"].db_search_title if context["first_message"] else None,
            "conversation_context": {
                "message_count": context["message_count"],
                "reused_context": True
            }
        }
        
        print(f"[FOLLOWUP] ğŸ“¤ ìµœì¢… ì‘ë‹µ êµ¬ì¡°:")
        print(f"[FOLLOWUP] - q_mode: {result['q_mode']}")
        print(f"[FOLLOWUP] - ë‹µë³€ ê¸¸ì´: {len(result['answer'])} ë¬¸ì")
        print(f"[FOLLOWUP] - ì¬ì‚¬ìš©ëœ ë¬¸ì„œ: {document_title}")
        
        return {
            "status": "success",
            "result": result,
            "tags": context["first_message"].keyword if context["first_message"] else None,
            "db_search_title": context["first_message"].db_search_title if context["first_message"] else None,
            "message": "ì¶”ê°€ ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ (ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš©)"
        }
        
    except Exception as e:
        print(f"[FOLLOWUP] ì¶”ê°€ ì§ˆë¬¸ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[FOLLOWUP] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ì¶”ê°€ ì§ˆë¬¸ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

async def generate_image(prompt: str) -> str:
    """Generate an image using OpenAI DALL-E API"""
    try:
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ë¶€ë¶„ì—ì„œ OpenAI DALL-E API í˜¸ì¶œ
        # ì˜ˆì‹œ: 
        # client = OpenAI(api_key=OPENAI_API_KEY)
        # response = client.images.generate(prompt=prompt, n=1, size="1024x1024")
        # image_url = response.data[0].url
        
        # ì´ë¯¸ì§€ URL ë°˜í™˜ (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
        return None
    except Exception as e:
        print(f"Error generating image: {str(e)}")
        return None

async def get_streaming_response(prompt: str, model: str = "gpt-3.5-turbo", generate_image: bool = False):
    """Stream a response from OpenAI API, optionally with an image"""
    try:
        # ì´ë¯¸ì§€ URL (ì´ë¯¸ì§€ ìƒì„±ì´ ìš”ì²­ëœ ê²½ìš°)
        image_url = None
        if generate_image or "ì´ë¯¸ì§€" in prompt or "ê·¸ë¦¼" in prompt or "image" in prompt.lower() or "picture" in prompt.lower():
            image_url = await generate_image(prompt)
        
        if USING_NEW_SDK:
            # New SDK style (>1.0)
            client = OpenAI(api_key=OPENAI_API_KEY)
            stream = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                stream=True
            )
            
            text_response = ""
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    text_response += content
                    
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ìŠ¤íŠ¸ë¦¬ë°
                    yield f"data: {content}\n\n"
                await asyncio.sleep(0.01)  # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì§§ì€ ì§€ì—°
            
            # í…ìŠ¤íŠ¸ ì‘ë‹µì´ ì™„ë£Œëœ í›„ ì´ë¯¸ì§€ URLì´ ìˆìœ¼ë©´ ì „ì†¡
            if image_url:
                response_data = {
                    "text": text_response,
                    "image_url": image_url
                }
                yield f"data: {json.dumps(response_data)}\n\n"
            
            yield "data: [DONE]\n\n"
        else:
            # Old SDK style (<1.0)
            openai.api_key = OPENAI_API_KEY
            stream = openai.ChatCompletion.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                stream=True
            )
            
            text_response = ""
            for chunk in stream:
                if chunk['choices'][0].get('delta', {}).get('content'):
                    content = chunk['choices'][0]['delta']['content']
                    text_response += content
                    
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ìŠ¤íŠ¸ë¦¬ë°
                    yield f"data: {content}\n\n"
                await asyncio.sleep(0.01)  # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì§§ì€ ì§€ì—°
            
            # í…ìŠ¤íŠ¸ ì‘ë‹µì´ ì™„ë£Œëœ í›„ ì´ë¯¸ì§€ URLì´ ìˆìœ¼ë©´ ì „ì†¡
            if image_url:
                response_data = {
                    "text": text_response,
                    "image_url": image_url
                }
                yield f"data: {json.dumps(response_data)}\n\n"
            
            yield "data: [DONE]\n\n"
    except Exception as e:
        print(f"Error in streaming response: {str(e)}")
        yield f"data: Error: {str(e)}\n\n"
        yield "data: [DONE]\n\n" 