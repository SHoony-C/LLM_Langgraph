import os
import openai
from openai import AsyncOpenAI
import asyncio
import json
import httpx
import uuid
from fastapi import APIRouter, HTTPException, Response, Depends, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from langgraph.graph import END, StateGraph
from collections import defaultdict
from qdrant_client import QdrantClient
from app.utils.config import (
    OPENAI_API_KEY, OPENAI_BASE_URL,
    QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION,
    IMAGE_BASE_URL, IMAGE_PATH_PREFIX, IS_OPENAI_CONFIGURED
)
from app.database import get_db
from app.models import Conversation, Message, User
from app.utils.auth import get_current_user
from app.utils.questionJudge import (
    judge_question_type,
    get_conversation_langgraph_context,
    create_llm_context_for_followup,
    should_use_langgraph,
    get_processing_endpoint,
    log_question_processing
)
from app.routes.llm_class import (
    DirectSimilarityCalculator,
    StreamRequest,
    SearchState,
    SSEGenerator
)   

from sqlalchemy.orm import Session

from datetime import datetime


# Create router 
router = APIRouter()


# 4ì°¨ì› ë²¡í„° ìƒì„± í•¨ìˆ˜
def generate_4d_vector(text: str) -> List[float]:
    """í…ìŠ¤íŠ¸ë¥¼ 4ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜"""
    import hashlib
    import struct
    
    # í…ìŠ¤íŠ¸ë¥¼ í•´ì‹œë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ëœ ë²¡í„° ìƒì„±
    hash_obj = hashlib.md5(text.encode('utf-8'))
    hash_bytes = hash_obj.digest()
    
    # 4ê°œì˜ float ê°’ ìƒì„± (0.0 ~ 1.0 ë²”ìœ„)
    vector = []
    for i in range(4):
        # 4ë°”ì´íŠ¸ì”© ì½ì–´ì„œ floatë¡œ ë³€í™˜
        byte_chunk = hash_bytes[i*4:(i+1)*4]
        if len(byte_chunk) < 4:
            byte_chunk += b'\x00' * (4 - len(byte_chunk))
        
        # unsigned intë¡œ ë³€í™˜ í›„ ì •ê·œí™”
        uint_val = struct.unpack('>I', byte_chunk)[0]
        normalized_val = uint_val / (2**32 - 1)  # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì •ê·œí™”
        vector.append(normalized_val)
    
    return vector

# ë²¡í„° ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
async def direct_document_search(question_type: str, limit: int, queries: List[str], 
                               ip: str, port: int, collection: str) -> List[dict]:
    """ë²¡í„° ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
    try:
        print(f"[VECTOR_SEARCH] ë²¡í„° ê²€ìƒ‰ ì‹œì‘: {len(queries)}ê°œ ì¿¼ë¦¬")
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        if ip.startswith(('http://', 'https://')):
            qdrant_url = f"{ip.rstrip('/')}" + (f":{port}" if not ip.endswith(f":{port}") else "")
            client = QdrantClient(url=qdrant_url)
        else:
            client = QdrantClient(host=ip, port=port)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        try:
            collections = client.get_collections()
            if not collection or collection not in [col.name for col in collections.collections]:
                print(f"[VECTOR_SEARCH] ì»¬ë ‰ì…˜ '{collection}' ì‚¬ìš© ë¶ˆê°€")
                return []
        except Exception as e:
            print(f"[VECTOR_SEARCH] Qdrant ì—°ê²° ì˜¤ë¥˜: {e}")
            return []
        
        all_results = []
        
        for query in queries:
            try:
                # ìì²´ 4ì°¨ì› ë²¡í„° ìƒì„±
                query_vector = generate_4d_vector(query)
                
                # Qdrantì—ì„œ ë²¡í„° ê²€ìƒ‰
                search_result = client.search(
                    collection_name=collection,
                    query_vector=query_vector,
                    limit=limit,
                    with_payload=True,
                    with_vectors=False
                )
                
                # ê²°ê³¼ ë³€í™˜
                for hit in search_result:
                    all_results.append({
                        'res_id': hit.id,
                        'res_score': hit.score,
                        'type_question': question_type,
                        'type_vector': 'custom_4d_vector',
                        'res_payload': hit.payload
                    })
                
                print(f"[VECTOR_SEARCH] '{query}' ê²€ìƒ‰ ì™„ë£Œ: {len(search_result)}ê±´")
                
            except Exception as e:
                print(f"[VECTOR_SEARCH] ì¿¼ë¦¬ '{query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                # ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                fallback_results = await fallback_text_search([query], client, collection, limit)
                all_results.extend(fallback_results)
                continue
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        all_results.sort(key=lambda x: x['res_score'], reverse=True)
        final_results = all_results[:limit]
        
        print(f"[VECTOR_SEARCH] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(final_results)}ê±´")
        for i, result in enumerate(final_results[:3]):
            title = result['res_payload'].get('document_name', 'ì œëª©ì—†ìŒ')
            score = result['res_score']
            print(f"[VECTOR_SEARCH]   {i+1}. {title} (ë²¡í„° ìœ ì‚¬ë„: {score:.4f})")
        
        return final_results
        
    except Exception as e:
        print(f"[VECTOR_SEARCH] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# í´ë°±ìš© í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰
async def fallback_text_search(queries: List[str], client, collection: str, limit: int) -> List[dict]:
    """OpenAI API ë¯¸ì„¤ì • ì‹œ í´ë°±ìš© í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    try:
        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        scroll_result = client.scroll(
            collection_name=collection,
            limit=1000,
            with_payload=True
        )
        
        all_documents = []
        for point in scroll_result[0]:
            if point.payload:
                all_documents.append({
                    'id': point.id,
                    'payload': point.payload
                })
        
        if not all_documents:
            return []
        
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_calc = DirectSimilarityCalculator()
        all_results = []
        
        for query in queries:
            query_results = similarity_calc.find_similar_documents(
                query=query,
                documents=[doc['payload'] for doc in all_documents],
                top_k=limit
            )
            
            for i, result in enumerate(query_results):
                if result['similarity'] > 0:  # 0ì  ì œì™¸
                    all_results.append({
                        'res_id': all_documents[i]['id'] if i < len(all_documents) else f"doc_{i}",
                        'res_score': result['similarity'],
                        'type_question': 'fallback',
                        'type_vector': 'text_similarity',
                        'res_payload': result['document']
                    })
        
        all_results.sort(key=lambda x: x['res_score'], reverse=True)
        return all_results[:limit]
        
    except Exception as e:
        print(f"[FALLBACK_SEARCH] í´ë°± ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# ì´ë¯¸ì§€ URL (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
# SAMPLE_IMAGE_URLS = []

# SSE ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
sse_generators = {}

# SSE ìƒíƒœ ë°œí–‰ í•¨ìˆ˜
async def yield_node_status(generator_id: str, node_name: str, status: str, data: dict):
    """SSEë¥¼ í†µí•´ ë…¸ë“œ ìƒíƒœë¥¼ ë°œí–‰"""
    # print(f"[SSE] ğŸ”„ ë©”ì‹œì§€ ì „ì†¡ ì‹œë„: {node_name}:{status} (generator_id: {generator_id})")
    
    if generator_id not in sse_generators:
        print(f"[SSE] âŒ ì œë„ˆë ˆì´í„° IDê°€ ì—†ìŒ - {node_name}: {status}")
        print(f"[SSE] í˜„ì¬ í™œì„± ì œë„ˆë ˆì´í„°: {list(sse_generators.keys())}")
        return
        
    try:
        message = {
            "stage": node_name,
            "status": status,
            "result": data,
            "timestamp": asyncio.get_event_loop().time()
        }
        
        # print(f"[SSE] ğŸ“¤ ì „ì†¡í•  ë©”ì‹œì§€: {message}")
        
        # SSE ì œë„ˆë ˆì´í„°ì— ë©”ì‹œì§€ ì „ì†¡
        generator = sse_generators.get(generator_id)
        if generator and generator.is_active:
            await generator.send_message(message)
            # print(f"[SSE] âœ… ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ: {node_name}:{status}")
            
            # ë©”ì‹œì§€ê°€ íì— ì œëŒ€ë¡œ ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸
            queue_size = generator.message_queue.qsize()
            # print(f"[SSE] ğŸ“Š ë©”ì‹œì§€ í í¬ê¸°: {queue_size}")
            
            # ë©”ì‹œì§€ ì „ì†¡ í›„ ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
        else:
            print(f"[SSE] âŒ ì œë„ˆë ˆì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {generator_id}")
            print(f"[SSE] ì œë„ˆë ˆì´í„° ìƒíƒœ: active={generator.is_active if generator else 'None'}")
    except Exception as e:
        print(f"[SSE] âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {node_name}:{status} - ì˜¤ë¥˜: {e}")
        import traceback
        print(f"[SSE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        pass

# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
async def node_rc_init(state: SearchState) -> SearchState:
    """ì´ˆê¸°í™” ë…¸ë“œ"""
    print("[inform]: node_rc_init")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    print(f"[NODE_INIT] ğŸ” generator_id: {generator_id}")
    print(f"[NODE_INIT] ğŸ” sse_generators í‚¤: {list(sse_generators.keys())}")
    
    if generator_id:
        print(f"[NODE_INIT] ğŸ“¤ A:started ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
        await yield_node_status(
            generator_id,
            "A",
            "started",
            {
                "message": "ì´ˆê¸°í™” ë‹¨ê³„ ì‹œì‘",
                "step": "A. ì´ˆê¸°í™”"
            }
        )
        print(f"[NODE_INIT] âœ… A:started ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
    else:
        print(f"[NODE_INIT] âŒ generator_idê°€ ì—†ìŒ")
    
    try: 
        question = state['question']
        generator_id = state.get('generator_id')
        
        if generator_id:
            print(f"[NODE_INIT] ğŸ“¤ A:completed ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
            await yield_node_status(
                generator_id,
                "A",
                "completed",
                {
                    "message": "ì´ˆê¸°í™” ë‹¨ê³„ ì™„ë£Œ",
                    "step": "A. ì´ˆê¸°í™” ì™„ë£Œ",
                    "question": question
                }
            )
            print(f"[NODE_INIT] âœ… A:completed ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
        
        return {
            "question": question,
            "keyword": "",
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": state.get('generator_id')  # ìƒíƒœì—ì„œ generator_id ìœ ì§€
        }
    except Exception as e:
        print("[error]: node_rc_init")
        raise RuntimeError(f"[error]: node_rc_init: {str(e)}")

async def node_rc_keyword(state: SearchState) -> SearchState:
    """í‚¤ì›Œë“œ ì¦ê°• ë…¸ë“œ - LLMì„ ì‚¬ìš©í•œ ë™ì  í‚¤ì›Œë“œ ìƒì„±"""
    print("[inform]: node_rc_keyword")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    print(f"[NODE_KEYWORD] ğŸ” generator_id: {generator_id}")
    
    if generator_id:
        print(f"[NODE_KEYWORD] ğŸ“¤ B:started ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
        await yield_node_status(
            generator_id,
            "B",
            "started",
            {
                "message": "í‚¤ì›Œë“œ ì¦ê°• ë‹¨ê³„ ì‹œì‘",
                "step": "B. í‚¤ì›Œë“œ ì¦ê°•",
                "question": state['question']
            }
        )
        print(f"[NODE_KEYWORD] âœ… B:started ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
    else:
        print(f"[NODE_KEYWORD] âŒ generator_idê°€ ì—†ìŒ")
    
    full_text_parts = []
    try:
        question = state['question']
        
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your-openai-api-key-here":
            print("[error]: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            base_keywords = [question]
            generator_id = state.get('generator_id')
            if generator_id:
                await yield_node_status(
                generator_id,
                "B",
                "completed",
                {
                    "message": "í‚¤ì›Œë“œ ì¦ê°• ë‹¨ê³„ ì™„ë£Œ (ê¸°ë³¸ ëª¨ë“œ)",
                    "step": "B. í‚¤ì›Œë“œ ì¦ê°• ì™„ë£Œ",
                    "keywords": base_keywords,
                    "keywords_count": len(base_keywords),
                    "reason": "OpenAI API í‚¤ ë¯¸ì„¤ì •"
                }
            )
            
            return {
                "question": state['question'],
                "keyword": base_keywords,
                "candidates_each": [],
                "candidates_total": [],
                "response": [],
                "generator_id": state.get('generator_id')  # ìƒíƒœì—ì„œ generator_id ìœ ì§€
            }
        
        try:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¦ê°• - ìƒˆë¡œìš´ LLM ë°©ì‹
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ì „ë¬¸ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³ , ìµœëŒ€ 15ê°œê¹Œì§€ ìƒì„±í•˜ì„¸ìš”."},
                {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”: {question}"}
            ]
            
            # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            httpx_client = httpx.AsyncClient(verify=False, timeout=None)

            # print(f"[messages í™•ì¸] {messages}")              
            
            # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "c.seunghoon",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
            # ë¹„ë™ê¸° í˜¸ì¶œ
            response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                stream=True,
            )
            # ìŠ¤íŠ¸ë¦¼ ì²­í¬ ìˆ˜ì‹ 
            async for chunk in response:
                if not chunk.choices:
                    continue
                delta = chunk.choices[0].delta
                content = getattr(delta, "content", None)
                if not content:
                    continue

                # ë¶€ë¶„ ì‘ë‹µ ëˆ„ì 
                full_text_parts.append(content)

                # ë„ˆë¬´ ë¹¡ë¹¡í•œ ë£¨í”„ ë°©ì§€
                await asyncio.sleep(0)

        except Exception as e:
            print(f"[error] streaming failed: {type(e).__name__}: {e}")
            full_text_parts = []

        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ì „ì²´ ì‘ë‹µ ë¬¸ìì—´ ì¡°ë¦½
        llm_response = "".join(full_text_parts).strip()

        # âš™ï¸ í‚¤ì›Œë“œ ë³€í™˜ ë¡œì§ (ê¸°ì¡´ ë™ì¼)
        if not llm_response:
            augmented_keywords = [question]
        else:
            keywords_text = llm_response.strip()
            # ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬ ë° ì •ë¦¬
            augmented_keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
            # ì› ì§ˆë¬¸ ì¶”ê°€
            augmented_keywords.insert(0, question)
            # ì¤‘ë³µ ì œê±° ë° 20ê°œ ì œí•œ
            augmented_keywords = list(dict.fromkeys(augmented_keywords))[:20]

        print(f"[inform]: LLMì„ í†µí•´ ìƒì„±ëœ í‚¤ì›Œë“œ: {len(augmented_keywords)}ê°œ")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "B",
                "completed",
                {
                    "message": "í‚¤ì›Œë“œ ì¦ê°• ë‹¨ê³„ ì™„ë£Œ",
                    "step": "B. í‚¤ì›Œë“œ ì¦ê°• ì™„ë£Œ",
                    "keywords": augmented_keywords,
                    "keywords_count": len(augmented_keywords)
                }
            )
        
        return {
            "question": state['question'],
            "keyword": augmented_keywords,
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": state.get('generator_id')  # ìƒíƒœì—ì„œ generator_id ìœ ì§€
        }
    except Exception as e:
        print("[error]: node_rc_keyword")
        raise RuntimeError(f"[error]: node_rc_keyword: {str(e)}")

async def node_rc_rag(state: SearchState) -> SearchState:
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("[inform]: node_rc_rag")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
            generator_id,
            "C",
            "started",
            {
                "message": "RAG ê²€ìƒ‰ ë‹¨ê³„ ì‹œì‘",
                "step": "C. RAG ê²€ìƒ‰",
                "keywords": state.get('keyword', [])
            }
        )
    
    try:
        # ë²¡í„° DB ì„¤ì •
        ip, port, collection = QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION
        
        # ì§ì ‘ ê²€ìƒ‰ ìˆ˜í–‰
        candidates_each = []
        
        # questionìœ¼ë¡œ ê²€ìƒ‰
        if state.get('question'):
            try:
                question_results = await direct_document_search('question', 5, [state['question']], ip, port, collection)
                candidates_each.extend(question_results)
            except Exception as e:
                print(f"Question ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # keywordë¡œ ê²€ìƒ‰ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬)
        if state.get('keyword'):
            try:
                # keywordê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                if isinstance(state['keyword'], list):
                    keywords = state['keyword']
                else:
                    keywords = [state['keyword']]
                
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ í•„í„°ë§
                keywords = [k for k in keywords if k and isinstance(k, str) and k.strip()]
                
                if keywords:
                    keyword_results = await direct_document_search('keyword', 3, keywords, ip, port, collection)
                    candidates_each.extend(keyword_results)
            except Exception as e:
                print(f"Keyword ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í•˜ë“œì½”ë”© ì œê±°)
        if not candidates_each:
            print("[RAG] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ë™ì  ì ìˆ˜ ì§‘ê³„ (í•˜ë“œì½”ë”© ì œê±°)
        aggregated_scores = defaultdict(float)
        payloads = {}
        
        # candidates_eachê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if candidates_each:
            for item in candidates_each:
                try:
                    res_id = item.get('res_id')
                    score = item.get('res_score', 0.0)
                    
                    if res_id is not None and score > 0:
                        # ë‹¨ìˆœ ì ìˆ˜ í•©ì‚° (ê°€ì¤‘ì¹˜ ì œê±°)
                        aggregated_scores[res_id] += score
                        if res_id not in payloads:
                            payloads[res_id] = item.get('res_payload', {})
                except Exception as e:
                    print(f"ê°œë³„ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ì„ íƒ (ê³ ì • ê°œìˆ˜ ì œê±°)
        candidates_total = sorted(
            [{'res_id': res_id, 'res_score': aggregated_scores[res_id], 'res_payload': payloads[res_id]}
             for res_id in aggregated_scores],
            key=lambda x: x['res_score'],
            reverse=True
        )
        
        # ë™ì ìœ¼ë¡œ ê²°ê³¼ ê°œìˆ˜ ê²°ì • (ìµœì†Œ 1ê°œ, ìµœëŒ€ 10ê°œ)
        if candidates_total:
            max_results = min(len(candidates_total), max(1, min(10, len(candidates_total))))
            candidates_total = candidates_total[:max_results]
        
        print(f"[RAG] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 5ê±´):")
        for i, candidate in enumerate(candidates_total):
            title = candidate.get('res_payload', {}).get('document_name', 'ì œëª© ì—†ìŒ')
            vector_data = candidate.get("vector", {})
            # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
            summary = vector_data.get("summary_result") if isinstance(vector_data, dict) else None
            score = candidate.get('res_score', 0)
            print(f"  {i+1}. {title} - {summary} (ìœ ì‚¬ë„: {score:.4f})")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "C",
                "completed",
                {
                    "message": "RAG ê²€ìƒ‰ ë‹¨ê³„ ì™„ë£Œ",
                    "step": "C. RAG ê²€ìƒ‰ ì™„ë£Œ",
                    "documents_count": len(candidates_total),
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ")
                        for candidate in candidates_total
                    ],
                    "search_results": [
                        {
                            "title": candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ"),
                            "text": candidate.get("res_payload", {}).get("vector", {}).get("text", "ë‚´ìš© ì—†ìŒ"),
                            "summary": candidate.get("res_payload", {}).get("vector", {}).get("summary_result", "ìš”ì•½ ì—†ìŒ"),
                            "image_url": candidate.get("res_payload", {}).get("vector", {}).get("image_url", ""),
                            "score": candidate.get("res_score", 0)
                        }
                        for candidate in candidates_total[:5]  # ìƒìœ„ 5ê°œë§Œ
                    ]
                }
            )
        
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_total": candidates_total,
            "response": [],
            "generator_id": state.get('generator_id')
        }
    except Exception as e:
        print(f"[error]: node_rc_rag: {str(e)}")
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ìƒíƒœ ë°˜í™˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê³„ì† ì§„í–‰
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": state.get('generator_id')
        }

async def node_rc_rerank(state: SearchState) -> SearchState:
    """ë™ì  ì¬ìˆœìœ„ ë…¸ë“œ (í•˜ë“œì½”ë”© ì œê±°)"""
    print("[inform]: node_rc_rerank")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
            generator_id,
            "D",
            "started",
            {
                "message": "ë¬¸ì„œ ì¬ìˆœìœ„ ë‹¨ê³„ ì‹œì‘",
                "step": "D. ë¬¸ì„œ ì¬ìˆœìœ„",
                "documents_count": len(state['candidates_total'])
            }
        )
    
    try:
        candidates_top = state['candidates_total']
        
        if not candidates_top:
            print("[RERANK] ì¬ìˆœìœ„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "question": state['question'],
                "keyword": state["keyword"],
                "candidates_total": state["candidates_total"],
                "response": [],
                "generator_id": state.get('generator_id')
            }
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ë™ì  ì¬ìˆœìœ„ (í•˜ë“œì½”ë”©ëœ 0.1 ê°ì†Œ ì œê±°)
        similarity_calc = DirectSimilarityCalculator()
        question = state['question']
        
        for candidate in candidates_top:
            try:
                # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                payload = candidate.get('res_payload', {})
                doc_title = payload.get('document_name', '')
                vector_data = payload.get("vector", {})
                # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
                doc_content = vector_data.get("text") if isinstance(vector_data, dict) else None

                doc_text = f"{doc_title} {doc_content}"
                
                # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê°„ ì§ì ‘ ìœ ì‚¬ë„ ê³„ì‚°
                relevance_score = similarity_calc.calculate_combined_similarity(question, doc_text)
                
                # ê¸°ì¡´ ê²€ìƒ‰ ì ìˆ˜ì™€ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê²°í•©
                original_score = candidate.get('res_score', 0.0)
                combined_score = (original_score * 0.6) + (relevance_score * 0.4)
                
                candidate.update({
                    'res_relevance': relevance_score,
                    'combined_score': combined_score
                })
                
            except Exception as e:
                print(f"[RERANK] ê°œë³„ ë¬¸ì„œ ì¬ìˆœìœ„ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                candidate.update({
                    'res_relevance': candidate.get('res_score', 0.0),
                    'combined_score': candidate.get('res_score', 0.0)
                })
        
        # ê²°í•© ì ìˆ˜ë¡œ ì •ë ¬ (í•˜ë“œì½”ë”©ëœ ì •ë ¬ ë°©ì‹ ì œê±°)
        sorted_candidates_top = sorted(candidates_top, key=lambda x: x.get('combined_score', 0), reverse=True)
        
        print(f"[RERANK] ì¬ìˆœìœ„ ì™„ë£Œ: {len(sorted_candidates_top)}ê°œ ë¬¸ì„œ")
        for i, candidate in enumerate(sorted_candidates_top[:3]):
            title = candidate.get('res_payload', {}).get('document_name', 'ì œëª©ì—†ìŒ')
            combined_score = candidate.get('combined_score', 0)
            relevance = candidate.get('res_relevance', 0)
            print(f"[RERANK]   {i+1}. {title} (ê²°í•©ì ìˆ˜: {combined_score:.4f}, ê´€ë ¨ì„±: {relevance:.4f})")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "D",
                "completed",
                {
                    "message": "ë¬¸ì„œ ì¬ìˆœìœ„ ë‹¨ê³„ ì™„ë£Œ",
                    "step": "D. ë¬¸ì„œ ì¬ìˆœìœ„ ì™„ë£Œ",
                    "documents_count": len(sorted_candidates_top),
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª©ì—†ìŒ")
                        for candidate in sorted_candidates_top[:3]
                    ]
                }
            )
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": sorted_candidates_top,
            "generator_id": state.get('generator_id')
        }
    except Exception as e:
        print("[error]: node_rc_rerank")
        raise RuntimeError(f"[error]: node_rc_rerank: {str(e)}")

async def node_rc_answer(state: SearchState) -> SearchState:
    """ë‹µë³€ ìƒì„± ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©) - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›"""
    print("[inform]: node_rc_answer ì‹¤í–‰")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
            generator_id,
            "E",
            "started",
            {
                "message": "ë‹µë³€ ìƒì„± ë‹¨ê³„ ì‹œì‘",
                "step": "E. ë‹µë³€ ìƒì„±",
                "search_results_count": len(state['response'])
            }
        )
    
    try:
        candidates_top = state['response']
        generator_id = state.get('generator_id')
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if candidates_top:
            print(f"[Answer] âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. LLM API í˜¸ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ìƒìœ„ 1ê±´ì˜ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            top_result = candidates_top[0]
            top_payload = top_result.get('res_payload', {})
            
            # ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
            document_title = top_payload.get('document_name', 'ì œëª© ì—†ìŒ')
            vector_data = top_payload.get("vector", {})
            document_content = vector_data.get("text") if isinstance(vector_data, dict) else None
            
            # LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {str(document_content)[:1000]}...

[ì§ˆë¬¸]
{state['question']}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- í˜•ì‹ì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
            """.strip()
            
            # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
            llm_answer = ""
            try:
                if IS_OPENAI_CONFIGURED:
                    print(f"[Answer] ğŸš€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° LLM API í˜¸ì¶œ ì‹œì‘...")
                    
                    messages = [{"role": "user", "content": prompt}]
                    
                    # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)
                    httpx_client = httpx.AsyncClient(
                        verify=False, 
                        timeout=httpx.Timeout(30.0, connect=10.0)  # ì—°ê²° íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    )
                    
                    # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    client = AsyncOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        http_client=httpx_client,
                        default_headers={
                            "x-dep-ticket": OPENAI_API_KEY,
                            "Send-System-Name": "ds2llm",
                            "User-Id": "c.seunghoon",
                            "User-Type": "AD_ID",
                            "Prompt-Msg-Id": str(uuid.uuid4()),
                            "Completion-Msg-Id": str(uuid.uuid4()),
                        }
                    )
                    
                    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                    response = await client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=messages,
                        stream=True,
                    )
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ SSEë¡œ ì „ì†¡
                    async for chunk in response:
                        if chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            llm_answer += content
                            
                            # ì‹¤ì‹œê°„ìœ¼ë¡œ SSEë¥¼ í†µí•´ ë‹µë³€ ì²­í¬ ì „ì†¡
                            if generator_id:
                                await yield_node_status(
                                    generator_id,
                                    "D",
                                    "streaming",
                                    {
                                        "message": "ë‹µë³€ ìƒì„± ì¤‘...",
                                        "content": content,
                                        "accumulated_answer": llm_answer,
                                        "is_streaming": True
                                    }
                                )
                            
                            # ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
                    
                    print(f"[Answer] âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
                    
                else:
                    print(f"[Answer] âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {str(document_content)[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
                    
            except Exception as e:
                error_message = str(e)
                print(f"[Answer] âŒ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {error_message}")
                
                # ì—°ê²° ì˜¤ë¥˜ íƒ€ì…ë³„ ì²˜ë¦¬
                if "APIConnectionError" in error_message or "Connection error" in error_message:
                    error_desc = "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                elif "timeout" in error_message.lower():
                    error_desc = "AI ì„œë¹„ìŠ¤ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                elif "authentication" in error_message.lower() or "unauthorized" in error_message.lower():
                    error_desc = "AI ì„œë¹„ìŠ¤ ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                else:
                    error_desc = "AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                
                llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {str(document_content)[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

âš ï¸ {error_desc}"""
            
            # ì´ë¯¸ì§€ URL ìƒì„± (ì²« ë²ˆì§¸ ë¬¸ì„œ ê¸°ë°˜)
            image_url = None
            if top_result and top_payload:
                doc_title = top_payload.get('document_name', '')
                doc_index = top_result.get('res_id', 0)
                
                if doc_title and doc_index:
                    import urllib.parse
                    import os
                    # .txt í™•ì¥ì ì œê±°í•˜ê³  _whole.jpgë¡œ ëŒ€ì²´
                    if doc_title.endswith('.txt'):
                        doc_title_without_ext = doc_title[:-4]  # .txt ì œê±°
                        image_filename = f"{doc_title_without_ext}_whole.jpg"
                    else:
                        # í™•ì¥ìê°€ ì—†ê±°ë‚˜ ë‹¤ë¥¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        image_filename = doc_title
                    
                    safe_title = urllib.parse.quote(image_filename, safe='')
                    image_url = f"{IMAGE_BASE_URL}{IMAGE_PATH_PREFIX}/{safe_title}"
                    print(f"[Answer] ğŸ–¼ï¸ ìƒì„±ëœ ì´ë¯¸ì§€ URL: {image_url}")

            # LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ ìœ„í•œ ì™„ì „í•œ ì‘ë‹µ êµ¬ì¡°
            response = {
                "res_id": [rest['res_id'] for rest in candidates_top],
                "answer": llm_answer,  # ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±ëœ ì „ì²´ ë‹µë³€
                "q_mode": "search",
                "keyword": state["keyword"],
                "db_contents": [] ,
                "top_document": top_result,
                "analysis_image_url": image_url
            }
        else:
            print(f"[Answer] âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            response = {
                "res_id": [],
                "answer": f"ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "q_mode": "search",
                "keyword": state["keyword"],
            }
        
        # ìµœì¢… ì™„ë£Œ ìƒíƒœ ì „ì†¡
        if generator_id:
            await yield_node_status(
                generator_id,
                "E",
                "completed",
                {
                    "message": "ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ",
                    "step": "E. ë‹µë³€ ìƒì„± ì™„ë£Œ",
                    "answer": response.get("answer", ""),
                    "analysis_image_url": response.get("analysis_image_url"),
                    "keywords": response.get("keyword", state.get("keyword", [])),
                    "document_titles": [],  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (db_contents ì‚¬ìš©)
                    "top_document": response.get("top_document"),
                    "is_streaming": False
                },
            )
        
        print(f"[Answer] âœ… node_rc_answer í•¨ìˆ˜ ì™„ë£Œ")
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": response,
            "generator_id": state.get('generator_id')
        }
        
    except Exception as e:
        print("[error]: node_rc_answer")
        import traceback
        print(f"[error] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise RuntimeError(f"[error]: node_rc_answer: {str(e)}")

async def node_rc_plain_answer(state: SearchState) -> SearchState:
    """ê¸°ë³¸ ë‹µë³€ ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_plain_answer ì‹¤í–‰")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
            generator_id,
            "E",
            "started",
            {
                "message": "ê¸°ë³¸ ë‹µë³€ ìƒì„± ë‹¨ê³„ ì‹œì‘",
                "step": "E. ê¸°ë³¸ ë‹µë³€ ìƒì„±",
                "reason": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
            }
        )
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë” êµ¬ì²´ì ì¸ ì•ˆë‚´ ë©”ì‹œì§€ ìƒì„±
    question = state['question']
    keywords = state.get('keyword', [])
    
    if isinstance(keywords, list) and len(keywords) > 1:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}"
    else:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}"
    
    detailed_answer = f"""ğŸ” **ë¶„ì„ ê²°ê³¼ ìš”ì•½**

**ì…ë ¥ ì§ˆë¬¸**: {question}

**í‚¤ì›Œë“œ ì¦ê°•**: {keyword_info}

**ê²€ìƒ‰ ê²°ê³¼**: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ê°œì„  ì œì•ˆ**:
1. **ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±**í•´ì£¼ì„¸ìš”
   - ì˜ˆ: "ì„±ê³¼ ê°œì„ " â†’ "2024ë…„ 1ë¶„ê¸° ì˜ì—…íŒ€ ì„±ê³¼ ê°œì„  ë°©ì•ˆ"
   - ì˜ˆ: "ì „ëµ ìˆ˜ë¦½" â†’ "ì‹ ì œí’ˆ ì¶œì‹œë¥¼ ìœ„í•œ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½"

2. **ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ê°€**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ í‚¤ì›Œë“œ: {keywords[:3] if isinstance(keywords, list) else keywords}
   - ì¶”ê°€ í‚¤ì›Œë“œ ì˜ˆì‹œ: êµ¬ì²´ì ì¸ ì—…ë¬´ ì˜ì—­, ê¸°ê°„, ë¶€ì„œëª… ë“±

3. **ë°ì´í„°ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ ì„¤ì •ëœ ë²¡í„° DB: {QDRANT_HOST}:{QDRANT_PORT}
   - ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}

ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤."""
    
    # Redisë¥¼ í†µí•´ ì™„ë£Œ ìƒíƒœ ë°œí–‰ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ DB ì €ì¥ì„ ìœ„í•´)
    complete_result = {
        "res_id": [], 
        "answer": detailed_answer,
        "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
        "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
    }
    
    # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
    # await save_langgraph_result_to_db(state['question'], complete_result, state["keyword"], state["candidates_total"])
    
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
                generator_id,
                "E",
                "completed",
                {
                    "message": "ê¸°ë³¸ ë‹µë³€ ìƒì„± ë‹¨ê³„ ì™„ë£Œ",
                    "step": "E. ê¸°ë³¸ ë‹µë³€ ìƒì„± ì™„ë£Œ",
                    "answer": detailed_answer,
                    "keywords": state.get("keyword", []),
                    "reason": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                }
            )
    
    return {
        "question": state['question'],
        "keyword": state["keyword"],
        "candidates_total": state["candidates_total"],
        "response": {
            "res_id": [], 
            "answer": detailed_answer,
            "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
            "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
        },
        "generator_id": state.get('generator_id')
    }

def judge_rc_ragscore(state: SearchState) -> str:
    """ë²¡í„° ê¸°ë°˜ RAG ì ìˆ˜ íŒë‹¨"""
    candidates_total = state["candidates_total"]
    
    if not candidates_total:
        print(f"[JUDGE] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ -> N")
        return "N"
    
    # ë²¡í„° ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸
    scores = [candidate.get("res_score", 0) for candidate in candidates_total]
    max_score = max(scores) if scores else 0
    avg_score = sum(scores) / len(scores) if scores else 0
    
    print(f"[JUDGE] ê²€ìƒ‰ ê²°ê³¼ {len(candidates_total)}ê±´")
    print(f"[JUDGE] ìµœê³  ë²¡í„° ì ìˆ˜: {max_score:.4f}, í‰ê· : {avg_score:.4f}")
    
    # ë²¡í„° ìœ ì‚¬ë„ ê¸°ì¤€ ì„ê³„ê°’ (ì¼ë°˜ì ìœ¼ë¡œ 0.7 ì´ìƒì´ ë†’ì€ ìœ ì‚¬ë„)
    if max_score >= 0.7:
        print(f"[JUDGE] ë†’ì€ ìœ ì‚¬ë„ -> Y")
        return "Y"
    elif max_score >= 0.5:
        print(f"[JUDGE] ì¤‘ê°„ ìœ ì‚¬ë„ -> Y")
        return "Y"
    elif max_score >= 0.3:
        print(f"[JUDGE] ë‚®ì€ ìœ ì‚¬ë„ -> Y")
        return "Y"
    else:
        print(f"[JUDGE] ë§¤ìš° ë‚®ì€ ìœ ì‚¬ë„ -> N")
        return "N"

async def save_langgraph_result_to_db_stream(question: str, result: dict, db: Session, user_id: int = 1):
    """LangGraph ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
    try:
        print(f"[DB_SAVE_STREAM] ğŸ’¾ LangGraph ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ DB ì €ì¥ ì‹œì‘")
        print(f"[DB_SAVE_STREAM] - ì§ˆë¬¸: {question}")
        print(f"[DB_SAVE_STREAM] - ì‚¬ìš©ì ID: {user_id}")
        
        # ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        response = result.get('response', {})
        keywords = result.get('keyword', [])
        candidates_total = result.get('candidates_total', [])
        
        if isinstance(response, dict):
            answer = response.get('answer', '')
            image_url = response.get('analysis_image_url')
        else:
            answer = str(response) if response else ''
            image_url = None
            
        print(f"[DB_SAVE_STREAM] - ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
        print(f"[DB_SAVE_STREAM] - í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
        print(f"[DB_SAVE_STREAM] - ë¬¸ì„œ: {len(candidates_total)}ê±´")
        
        # ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
        db_contents_list = []
        if candidates_total:
            for idx, candidate in enumerate(candidates_total[:5]):  # ìƒìœ„ 5ê°œë§Œ ì €ì¥
                payload = candidate.get('res_payload', {})
                vector_data = payload.get('vector', {})
                
                # image_url ì²˜ë¦¬ - Qdrant êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                image_url_value = ''
                if 'image_url' in payload:
                    # payload ìµœìƒìœ„ì— image_urlì´ ìˆëŠ” ê²½ìš°
                    img_url = payload.get('image_url', '')
                    if isinstance(img_url, list) and len(img_url) > 0:
                        image_url_value = img_url[0]  # ë°°ì—´ì˜ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                    elif isinstance(img_url, str):
                        image_url_value = img_url
                elif isinstance(vector_data, dict) and 'image_url' in vector_data:
                    # vector ë‚´ë¶€ì— image_urlì´ ìˆëŠ” ê²½ìš°
                    img_url = vector_data.get('image_url', '')
                    if isinstance(img_url, list) and len(img_url) > 0:
                        image_url_value = img_url[0]
                    elif isinstance(img_url, str):
                        image_url_value = img_url
                
                db_content = {
                    'rank': idx + 1,
                    'document_name': payload.get('document_name', ''),
                    'score': candidate.get('res_score', 0),
                    'combined_score': candidate.get('combined_score', candidate.get('res_score', 0)),
                    'relevance_score': candidate.get('res_relevance', 0),
                    'text': vector_data.get('text', '') if isinstance(vector_data, dict) else '',
                    'summary_purpose': vector_data.get('summary_purpose', '') if isinstance(vector_data, dict) else '',
                    'summary_result': vector_data.get('summary_result', '') if isinstance(vector_data, dict) else '',
                    'summary_fb': vector_data.get('summary_fb', '') if isinstance(vector_data, dict) else '',
                    'image': image_url_value,  # Qdrant êµ¬ì¡°ì— ë§ê²Œ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ URL (image ì»¬ëŸ¼ìœ¼ë¡œ ì €ì¥)
                    'res_id': candidate.get('res_id', '')
                }
                db_contents_list.append(db_content)
        
        db_contents_json = json.dumps(db_contents_list, ensure_ascii=False)
        print(f"[DB_SAVE_STREAM] - ê²€ìƒ‰ ê²°ê³¼ JSON ê¸¸ì´: {len(db_contents_json)}ì")
        
        # ê¸°ì¡´ ëŒ€í™” ì‚¬ìš© (ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•ŠìŒ)
        # conversation_idëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì•„ì•¼ í•¨
        print(f"[DB_SAVE_STREAM] âŒ ì´ í•¨ìˆ˜ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë©”ì‹œì§€ ì €ì¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        return
        
        # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        from app.models import User
        user = db.query(User).filter(User.id == user_id).first()
        user_name = user.loginid if user and user.loginid else (user.username if user else "system")
        
        print(f"[DB_SAVE_STREAM] - ì‚¬ìš©ìëª…: {user_name}")
        
        # ë©”ì‹œì§€ ì €ì¥ (q_mode: 'search' - ë­ê·¸ë˜í”„ ì „ìš©)
        message = Message(
            conversation_id=conversation.id,
            role="user",
            question=question,
            ans=answer,
            q_mode='search',  # ë­ê·¸ë˜í”„ ì „ìš© ëª¨ë“œ
            keyword=str(keywords) if keywords else None,
            db_contents=db_contents_json,  # ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ ì •ë³´ ì €ì¥
            image=image_url,
            user_name=user_name  # ì‹¤ì œ ì‚¬ìš©ìëª…
        )
        
        db.add(message)
        db.commit()
        db.refresh(message)
        
        print(f"[DB_SAVE_STREAM] âœ… LangGraph ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ")
        print(f"[DB_SAVE_STREAM] - ëŒ€í™” ID: {conversation.id}")
        print(f"[DB_SAVE_STREAM] - ë©”ì‹œì§€ ID: {message.id}")
        
    except Exception as e:
        print(f"[DB_SAVE_STREAM] âŒ LangGraph ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"[DB_SAVE_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        if db:
            db.rollback()

async def save_langgraph_result_to_db(question: str, response: dict, keywords: list, candidates_total: list, image_url: str = None):
    """LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ DBì— ì§ì ‘ ì €ì¥ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    try:
        print(f"[DB_SAVE] LangGraph ê²°ê³¼ DB ì €ì¥ ì‹œì‘ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ì§ˆë¬¸: {question}")
        print(f"[DB_SAVE] ì‘ë‹µ: {response.get('answer', '')[:100]}...")
        print(f"[DB_SAVE] í‚¤ì›Œë“œ: {keywords}")
        print(f"[DB_SAVE] ë¬¸ì„œ: {len(candidates_total)}ê±´")
        print(f"[DB_SAVE] ì´ë¯¸ì§€ URL: {image_url}")
        
        # ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
        db_contents_list = []
        if candidates_total:
            for idx, candidate in enumerate(candidates_total[:5]):  # ìƒìœ„ 5ê°œë§Œ ì €ì¥
                payload = candidate.get('res_payload', {})
                vector_data = payload.get('vector', {})
                
                # image_url ì²˜ë¦¬ - Qdrant êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                image_url_value = ''
                if 'image_url' in payload:
                    # payload ìµœìƒìœ„ì— image_urlì´ ìˆëŠ” ê²½ìš°
                    img_url = payload.get('image_url', '')
                    if isinstance(img_url, list) and len(img_url) > 0:
                        image_url_value = img_url[0]  # ë°°ì—´ì˜ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                    elif isinstance(img_url, str):
                        image_url_value = img_url
                elif isinstance(vector_data, dict) and 'image_url' in vector_data:
                    # vector ë‚´ë¶€ì— image_urlì´ ìˆëŠ” ê²½ìš°
                    img_url = vector_data.get('image_url', '')
                    if isinstance(img_url, list) and len(img_url) > 0:
                        image_url_value = img_url[0]
                    elif isinstance(img_url, str):
                        image_url_value = img_url
                
                db_content = {
                    'rank': idx + 1,
                    'document_name': payload.get('document_name', ''),
                    'score': candidate.get('res_score', 0),
                    'combined_score': candidate.get('combined_score', candidate.get('res_score', 0)),
                    'relevance_score': candidate.get('res_relevance', 0),
                    'text': vector_data.get('text', '') if isinstance(vector_data, dict) else '',
                    'summary_purpose': vector_data.get('summary_purpose', '') if isinstance(vector_data, dict) else '',
                    'summary_result': vector_data.get('summary_result', '') if isinstance(vector_data, dict) else '',
                    'summary_fb': vector_data.get('summary_fb', '') if isinstance(vector_data, dict) else '',
                    'image': image_url_value,  # Qdrant êµ¬ì¡°ì— ë§ê²Œ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ URL (image ì»¬ëŸ¼ìœ¼ë¡œ ì €ì¥)
                    'res_id': candidate.get('res_id', '')
                }
                db_contents_list.append(db_content)
        
        db_contents_json = json.dumps(db_contents_list, ensure_ascii=False)
        print(f"[DB_SAVE] - ê²€ìƒ‰ ê²°ê³¼ JSON ê¸¸ì´: {len(db_contents_json)}ì")
        
        # ìƒˆ ëŒ€í™” ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì°¾ê¸°
        db = next(get_db())
        
        # ê¸°ì¡´ ëŒ€í™” ì‚¬ìš© (ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•ŠìŒ)
        # conversation_idëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì•„ì•¼ í•¨
        print(f"[DB_SAVE] âŒ ì´ í•¨ìˆ˜ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë©”ì‹œì§€ ì €ì¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        return
        
        # ë©”ì‹œì§€ ì €ì¥ (q_mode: 'search' - ë­ê·¸ë˜í”„ ì „ìš©)
        message = Message(
            conversation_id=conversation.id,
            role="user",
            question=question,
            ans=response.get('answer', ''),
            q_mode='search',  # ë­ê·¸ë˜í”„ ì „ìš© ëª¨ë“œ
            keyword=str(keywords) if keywords else None,
            db_contents=db_contents_json,  # ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ ì •ë³´ ì €ì¥
            image=image_url,  # ì´ë¯¸ì§€ URL ì¶”ê°€
            user_name="system"  # ê¸°ë³¸ ì‚¬ìš©ìëª…
        )
        
        db.add(message)
        db.commit()
        
        print(f"[DB_SAVE] âœ… LangGraph ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ëŒ€í™” ID: {conversation.id}")
        print(f"[DB_SAVE] ë©”ì‹œì§€ ID: {message.id}")
        print(f"[DB_SAVE] q_mode: {message.q_mode} (ë­ê·¸ë˜í”„ ì „ìš©)")
        
    except Exception as e:
        print(f"[DB_SAVE] âŒ LangGraph ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"[DB_SAVE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")


# LangGraph êµ¬ì„±
def create_langgraph():
    """LangGraph ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("node_rc_init", node_rc_init)
    workflow.add_node("node_rc_keyword", node_rc_keyword)
    workflow.add_node("node_rc_rag", node_rc_rag)
    workflow.add_node("node_rc_rerank", node_rc_rerank)
    workflow.add_node("node_rc_answer", node_rc_answer)
    workflow.add_node("node_rc_plain_answer", node_rc_plain_answer)
    
    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("node_rc_init")
    workflow.add_edge("node_rc_init", "node_rc_keyword")
    workflow.add_edge("node_rc_keyword", "node_rc_rag")
    workflow.add_conditional_edges(
        "node_rc_rag",
        judge_rc_ragscore,
        {
            "Y": "node_rc_rerank",
            "N": "node_rc_plain_answer"
        }
    )
    workflow.add_edge("node_rc_rerank", "node_rc_answer")
    workflow.add_edge("node_rc_answer", END)
    workflow.add_edge("node_rc_plain_answer", END)
    
    return workflow.compile()

# LangGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
try:
    langgraph_instance = create_langgraph()
    print("[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨: {e}")
    langgraph_instance = None


# LangGraph ì§ì ‘ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©) - ê¸°ì¡´ ìœ ì§€
@router.post("")
async def execute_langgraph(request: StreamRequest, db: Session = Depends(get_db)):
    """LangGraphë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë°˜í™˜ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"[LangGraph] ğŸš€ ë­ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸ (Judge í•¨ìˆ˜ ì‚¬ìš©)
        if request.conversation_id:
            judgment = judge_question_type(request.conversation_id, db)
            log_question_processing(request.question, judgment)
            
            if not judgment["is_first_question"]:
                print(f"[LangGraph] âš ï¸ ì¶”ê°€ ì§ˆë¬¸ ê°ì§€ë¨ - LangGraph ì‹¤í–‰ ì°¨ë‹¨")
                raise HTTPException(
                    status_code=400, 
                    detail="ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
                )
        
        # ì›Œí¬í”Œë¡œìš° í™•ì¸
        if langgraph_instance is None:
            raise HTTPException(status_code=500, detail="LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        initial_state = {"question": request.question}
        print(f"[LangGraph] ì‹¤í–‰ ì‹œì‘: {request.question}")
        print(f"[LangGraph] ì´ˆê¸° ìƒíƒœ: {initial_state}")
        
        result = await langgraph_instance.ainvoke(initial_state)
        
        print(f"[LangGraph] âœ… ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ì—ì„œ íƒœê·¸ì™€ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
        tags = None
        
        if isinstance(result, dict):
            # í‚¤ì›Œë“œ ì •ë³´ì—ì„œ íƒœê·¸ ì¶”ì¶œ
            if 'keyword' in result and result['keyword']:
                if isinstance(result['keyword'], list):
                    tags = ', '.join(result['keyword'])
                else:
                    tags = str(result['keyword'])
                print(f"[LangGraph] í‚¤ì›Œë“œ: {len(result['keyword'])}ê°œ")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
            if 'candidates_total' in result and result['candidates_total']:
                print(f"[LangGraph] ë¬¸ì„œ: {len(result['candidates_total'])}ê±´")
            
            # ì‘ë‹µ ì •ë³´ í™•ì¸
            if 'response' in result and result['response']:
                response_text = result['response'].get('answer', '')[:50] if isinstance(result['response'], dict) else str(result['response'])[:50]
                print(f"[LangGraph] ì‘ë‹µ: {response_text}...")
        
        print(f"[LangGraph] ìš”ì•½: í‚¤ì›Œë“œ {len(result.get('keyword', []))}ê°œ, ë¬¸ì„œ {len(result.get('candidates_total', []))}ê±´")
        
        return {
            "status": "success",
            "result": result,
            "tags": tags,
            "message": "LangGraph ì‹¤í–‰ ì™„ë£Œ (ì²« ë²ˆì§¸ ì§ˆë¬¸)"
        }
        
    except Exception as e:
        print(f"[LangGraph] ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[LangGraph] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")


# SSE ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©)
@router.post("/stream")
async def execute_langgraph_stream(request: StreamRequest, db: Session = Depends(get_db), current_user: User = Depends(get_current_user)):
    """LangGraph SSE ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    
    async def generate_sse():
        generator_id = str(uuid.uuid4())
        print(f"[SSE] ğŸ†” ìƒˆ ì œë„ˆë ˆì´í„° ìƒì„±: {generator_id}")
        generator = SSEGenerator(generator_id)
        sse_generators[generator_id] = generator
        print(f"[SSE] ğŸ“‹ í˜„ì¬ í™œì„± ì œë„ˆë ˆì´í„° ìˆ˜: {len(sse_generators)}")
        
        try:
            # OpenAI API í‚¤ í™•ì¸
            if not OPENAI_API_KEY:
                yield f"data: {json.dumps({'error': 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'})}\n\n"
                return
            
            print(f"[SSE] ğŸš€ LangGraph SSE ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {request.question}")
            
            # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸ (Judge í•¨ìˆ˜ ì‚¬ìš©)
            if request.conversation_id:
                judgment = judge_question_type(request.conversation_id, db)
                log_question_processing(request.question, judgment, current_user.id if current_user else None)
                
                if not judgment["is_first_question"]:
                    yield f"data: {json.dumps({'error': 'ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”'})}\n\n"
                    return
            
            # ì›Œí¬í”Œë¡œìš° í™•ì¸
            if langgraph_instance is None:
                yield f"data: {json.dumps({'error': 'LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'})}\n\n"
                return
            
            # ì´ˆê¸° ìƒíƒœì— generator_id ì¶”ê°€
            initial_state = {
                "question": request.question,
                "keyword": "",
                "candidates_each": [],
                "candidates_total": [],
                "response": [],
                "generator_id": generator_id
            }
            print(f"[SSE] ğŸ“‹ ì´ˆê¸° ìƒíƒœ ì„¤ì •: generator_id={generator_id}")
            
            print(f"[SSE] LangGraph ì‹¤í–‰ ì‹œì‘: {request.question}")
            
            # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë¨¼ì € ì „ì†¡
            test_message = {
                "stage": "TEST",
                "status": "started", 
                "result": {"message": "SSE ì—°ê²° í…ŒìŠ¤íŠ¸"}
            }
            print(f"[SSE] ğŸ§ª í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡")
            await generator.send_message(test_message)
            
            # LangGraph ì‹¤í–‰ì„ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            async def run_langgraph():
                try:
                    print(f"[LANGGRAPH] ğŸš€ LangGraph ì‹¤í–‰ ì‹œì‘ (generator_id: {generator_id})")
                    print(f"[LANGGRAPH] ğŸ“‹ ì´ˆê¸° ìƒíƒœ: {initial_state}")
                    
                    # LangGraph ì‹¤í–‰ ì „ ì œë„ˆë ˆì´í„° ìƒíƒœ í™•ì¸
                    print(f"[LANGGRAPH] ğŸ” ì‹¤í–‰ ì „ ì œë„ˆë ˆì´í„° ìƒíƒœ: active={generator.is_active}, queue_size={generator.message_queue.qsize()}")
                    
                    result = await langgraph_instance.ainvoke(initial_state)
                    
                    print(f"[LANGGRAPH] âœ… LangGraph ì‹¤í–‰ ì™„ë£Œ")
                    print(f"[LANGGRAPH] ğŸ“Š ê²°ê³¼ ìš”ì•½: keyword={len(result.get('keyword', []))}ê°œ, candidates={len(result.get('candidates_total', []))}ê°œ")
                    print(f"[LANGGRAPH] ğŸ” ì‹¤í–‰ í›„ ì œë„ˆë ˆì´í„° ìƒíƒœ: active={generator.is_active}, queue_size={generator.message_queue.qsize()}")
                    
                    # LangGraph ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
                    print(f"[LANGGRAPH] ğŸ’¾ LangGraph ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥ë©ë‹ˆë‹¤.")
                    
                    # DONE ë©”ì‹œì§€ì— ì „ì²´ LangGraph ê²°ê³¼ í¬í•¨
                    done_message = {
                        "stage": "DONE", 
                        "result": result,  # ì „ì²´ LangGraph ê²°ê³¼ í¬í•¨
                        "keyword": result.get('keyword', []),
                        "candidates_total": result.get('candidates_total', [])
                    }
                    print(f"[LANGGRAPH] ğŸ“¤ DONE ë©”ì‹œì§€ ì „ì†¡ ì‹œë„ (í í¬ê¸°: {generator.message_queue.qsize()})")
                    await generator.send_message(done_message)
                    print(f"[LANGGRAPH] âœ… DONE ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ (í í¬ê¸°: {generator.message_queue.qsize()})")
                    
                except Exception as e:
                    print(f"[LANGGRAPH] âŒ LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                    import traceback
                    print(f"[LANGGRAPH] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    error_message = {"stage": "ERROR", "error": str(e)}
                    print(f"[LANGGRAPH] ğŸ“¤ ERROR ë©”ì‹œì§€ ì „ì†¡ ì‹œë„")
                    await generator.send_message(error_message)
                finally:
                    print(f"[LANGGRAPH] ğŸ”š ì œë„ˆë ˆì´í„° ì¢…ë£Œ ì‹œì‘")
                    await generator.close()
                    print(f"[LANGGRAPH] âœ… ì œë„ˆë ˆì´í„° ì¢…ë£Œ ì™„ë£Œ")
            
            # LangGraph ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
            langgraph_task = asyncio.create_task(run_langgraph())
            print(f"[SSE] ğŸš€ LangGraph íƒœìŠ¤í¬ ì‹œì‘ë¨")
            
            # ì§§ì€ ì§€ì—° í›„ SSE ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (LangGraphê°€ ì‹œì‘í•  ì‹œê°„ì„ ì¤Œ)
            await asyncio.sleep(0.01)  # 0.1ì´ˆ â†’ 0.01ì´ˆë¡œ ë‹¨ì¶•
            
            # SSE ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
            print(f"[SSE] ğŸ“¡ SSE ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì‹œì‘")
            message_count = 0
            heartbeat_count = 0
            
            # LangGraph íƒœìŠ¤í¬ê°€ ì‹¤í–‰ ì¤‘ì´ê±°ë‚˜ ì œë„ˆë ˆì´í„°ê°€ í™œì„±í™”ëœ ë™ì•ˆ ê³„ì† ì‹¤í–‰
            while generator.is_active or not langgraph_task.done():
                try:
                    # í˜„ì¬ ìƒíƒœ ë¡œê¹…
                    current_queue_size = generator.message_queue.qsize()
                    task_done = langgraph_task.done()
                    # print(f"[SSE] ğŸ”„ ë£¨í”„ ìƒíƒœ: generator_active={generator.is_active}, task_done={task_done}, queue_size={current_queue_size}")
                    
                    # íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë ¤ì„œ ë©”ì‹œì§€ë¥¼ ë†“ì¹˜ì§€ ì•Šë„ë¡ í•¨
                    message = await asyncio.wait_for(generator.message_queue.get(), timeout=1.0)
                    
                    if message is None:  # ì¢…ë£Œ ì‹ í˜¸
                        print(f"[SSE] ğŸ”š ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                        break
                    
                    message_count += 1
                    # print(f"[SSE] ğŸ“¨ ë©”ì‹œì§€ #{message_count} ìˆ˜ì‹ : {message.get('stage', 'unknown')}:{message.get('status', 'unknown')}")
                    # print(f"[SSE] ğŸ“‹ ë©”ì‹œì§€ ë‚´ìš©: {json.dumps(message, ensure_ascii=False)[:200]}...")
                    
                    # SSE í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡
                    sse_data = f"data: {json.dumps(message, ensure_ascii=False)}\n\n"
                    # print(f"[SSE] ğŸ“¤ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡: {len(sse_data)} bytes")
                    yield sse_data
                    # print(f"[SSE] âœ… í´ë¼ì´ì–¸íŠ¸ ì „ì†¡ ì™„ë£Œ")
                    
                    # ë©”ì‹œì§€ ì „ì†¡ í›„ ì§§ì€ ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
                    
                except asyncio.TimeoutError:
                    # íƒ€ì„ì•„ì›ƒ ì‹œ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡ (10ë²ˆë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ë¡œê·¸)
                    heartbeat_count += 1
                    current_queue_size = generator.message_queue.qsize()
                    task_done = langgraph_task.done()
                    
                    if heartbeat_count % 10 == 0:
                        print(f"[SSE] ğŸ’“ í•˜íŠ¸ë¹„íŠ¸ #{heartbeat_count} - ë©”ì‹œì§€:{message_count}, í:{current_queue_size}, íƒœìŠ¤í¬ì™„ë£Œ:{task_done}")
                    
                    yield f"data: {json.dumps({'heartbeat': True, 'count': heartbeat_count, 'queue_size': current_queue_size, 'task_done': task_done})}\n\n"
                    continue
                except Exception as e:
                    print(f"[SSE] âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    import traceback
                    print(f"[SSE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    break
            
            print(f"[SSE] ğŸ“Š ì´ {message_count}ê°œ ë©”ì‹œì§€, {heartbeat_count}ê°œ í•˜íŠ¸ë¹„íŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            yield f"data: [DONE]\n\n"
            
        except Exception as e:
            print(f"[SSE] ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        finally:
            # ì •ë¦¬
            if generator_id in sse_generators:
                del sse_generators[generator_id]
            print(f"[SSE] ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ: {generator_id}")
    
    return StreamingResponse(
        generate_sse(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

