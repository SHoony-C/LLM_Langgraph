async def save_langgraph_result_to_db(question: str, response: dict, keywords: list, candidates_total: list, image_url: str = None):
    """LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ DBì— ì§ì ‘ ì €ì¥ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    try:
        print(f"[DB_SAVE] LangGraph ê²°ê³¼ DB ì €ì¥ ì‹œì‘ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ì§ˆë¬¸: {question}")
        print(f"[DB_SAVE] ì‘ë‹µ: {response.get('answer', '')[:100]}...")
        print(f"[DB_SAVE] í‚¤ì›Œë“œ: {keywords}")
        print(f"[DB_SAVE] ë¬¸ì„œ: {len(candidates_total)}ê±´")
        print(f"[DB_SAVE] ì´ë¯¸ì§€ URL: {image_url}")
        
        # ìƒˆ ëŒ€í™” ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì°¾ê¸°
        db = next(get_db())
        
        # ìƒˆ ëŒ€í™” ìƒì„±
        from datetime import datetime
        title = question[:50] + "..." if len(question) > 50 else question
        conversation = Conversation(
            title=title,
            user_id=1,  # ê¸°ë³¸ ì‚¬ìš©ì ID (ì‹¤ì œë¡œëŠ” ì¸ì¦ëœ ì‚¬ìš©ì ID ì‚¬ìš©)
            last_updated=datetime.utcnow()
        )
        db.add(conversation)
        db.commit()
        db.refresh(conversation)
        
        # ë©”ì‹œì§€ ì €ì¥ (q_mode: 'search' - ë­ê·¸ë˜í”„ ì „ìš©)
        message = Message(
            conversation_id=conversation.id,
            role="user",
            question=question,
            ans=response.get('answer', ''),
            q_mode='search',  # ë­ê·¸ë˜í”„ ì „ìš© ëª¨ë“œ
            keyword=str(keywords) if keywords else None,
            db_search_title=str([item.get('res_payload', {}).get('document_name', '') for item in candidates_total[:5]]) if candidates_total else None,
            image=image_url  # ì´ë¯¸ì§€ URL ì¶”ê°€
            # user_name í•„ë“œ ì œê±° - í•˜ë“œì½”ë”© ë°©ì§€
        )
        
        db.add(message)
        db.commit()
        
        print(f"[DB_SAVE] âœ… LangGraph ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ëŒ€í™” ID: {conversation.id}")
        print(f"[DB_SAVE] ë©”ì‹œì§€ ID: {message.id}")
        print(f"[DB_SAVE] q_mode: {message.q_mode} (ë­ê·¸ë˜í”„ ì „ìš©)")
        
    except Exception as e:
        print(f"[DB_SAVE] âŒ LangGraph ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"[DB_SAVE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")


# LangGraph êµ¬ì„±
def create_langgraph():
    """LangGraph ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("node_rc_init", node_rc_init)
    workflow.add_node("node_rc_keyword", node_rc_keyword)
    workflow.add_node("node_rc_rag", node_rc_rag)
    workflow.add_node("node_rc_rerank", node_rc_rerank)
    workflow.add_node("node_rc_answer", node_rc_answer)
    workflow.add_node("node_rc_plain_answer", node_rc_plain_answer)
    
    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("node_rc_init")
    workflow.add_edge("node_rc_init", "node_rc_keyword")
    workflow.add_edge("node_rc_keyword", "node_rc_rag")
    workflow.add_conditional_edges(
        "node_rc_rag",
        judge_rc_ragscore,
        {
            "Y": "node_rc_rerank",
            "N": "node_rc_plain_answer"
        }
    )
    workflow.add_edge("node_rc_rerank", "node_rc_answer")
    workflow.add_edge("node_rc_answer", END)
    workflow.add_edge("node_rc_plain_answer", END)
    
    return workflow.compile()

# LangGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
try:
    langgraph_instance = create_langgraph()
    print("[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨: {e}")
    langgraph_instance = None

# ê°„ë‹¨í•œ LLM ì‘ë‹µ í•¨ìˆ˜ (conversations.pyì—ì„œ ì‚¬ìš©)
async def get_llm_response(question: str) -> str:
    """ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    try:
        
        print(f"[LLM_RESPONSE] ì§ˆë¬¸: {question}")
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": question}
        ]
        
        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        httpx_client = httpx.AsyncClient(verify=False, timeout=None)

        # print(f"[messages í™•ì¸] {messages}")              
        
        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "c.seunghoon",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await client.chat.completions.create(
                model="openai/gpt-oss-120b",
                messages=messages,
                stream=True,
            )
        
        async for chunk in response:              # ì´ì œ chunkëŠ” OpenAIObject
                    delta = chunk.choices[0].delta
                    content = delta.content
                    # print(content)
                # for chunk in response:
                #     if chunk.choices[0].delta.get("content"):
                #         content = chunk.choices[0].delta.content
                    try:
                        # ë¹„-ASCII ë¬¸ì í—ˆìš©, UTF-8 bytes ë¡œ ì¦‰ì‹œ ì „ì†¡
                        payload = json.dumps({'content': content}, ensure_ascii=False)
                        yield (f"data: {payload}\n\n").encode("utf-8")

                        await asyncio.sleep(0.01)
                        # ì²­í¬ ì‚¬ì´ì— ì§€ì—° ì¶”ê°€í•˜ì—¬ ë‹¤ë¥¸ API ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•¨
                        await asyncio.sleep(0.01)
                    except (ConnectionResetError, BrokenPipeError, OSError, ConnectionAbortedError, ConnectionError) as e:
                        # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
                        print(f"Client disconnected during streaming lv2: {type(e).__name__}")
                        return
                    except Exception as e:
                        print(f"Unexpected error during streaming lv1: {str(e)}")
                        return
        
    except Exception as e:
        print(f"[LLM_RESPONSE] ì˜¤ë¥˜: {str(e)}")
        yield  f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"




# ì¼ë°˜ LLM ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (streaming ì§€ì›)
@router.post("/chat/stream")
async def stream_chat_with_llm(request: StreamRequest, http_request: Request, db: Session = Depends(get_db)):
    """Stream a response from general LLM chat using async method"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[Chat Stream] ========== LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‹œì‘ ==========")
        print(f"[Chat Stream] ìš”ì²­ ì •ë³´:")
        print(f"[Chat Stream] - ì§ˆë¬¸: {request.question}")
        print(f"[Chat Stream] - ëŒ€í™” ID: {request.conversation_id}")
        print(f"[Chat Stream] - conversation_id íƒ€ì…: {type(request.conversation_id)}")
        print(f"[Chat Stream] - conversation_idê°€ Noneì¸ê°€?: {request.conversation_id is None}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [{"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."}]
        
        if request.conversation_id:
            try:
                # í•´ë‹¹ ëŒ€í™”ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 10ê°œë§Œ)
                conversation_messages = db.query(Message).filter(
                    Message.conversation_id == request.conversation_id
                ).order_by(Message.created_at.asc()).limit(10).all()
                
                print(f"[Chat Stream] ì´ì „ ëŒ€í™” ë©”ì‹œì§€ {len(conversation_messages)}ê°œ ë¡œë“œ")
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ==========")
                
                # ì´ì „ ëŒ€í™”ë¥¼ messagesì— ì¶”ê°€
                for i, msg in enumerate(conversation_messages):
                    print(f"[Chat Stream] DB ë©”ì‹œì§€ {i+1}: ID={msg.id}, role={msg.role}, created_at={msg.created_at}")
                    if msg.question:
                        print(f"[Chat Stream] ì§ˆë¬¸: {msg.question}")
                        messages.append({"role": "user", "content": msg.question})
                    if msg.ans:
                        print(f"[Chat Stream] ë‹µë³€: {msg.ans}")
                        messages.append({"role": "assistant", "content": msg.ans})
                    print(f"[Chat Stream] ----------------------------------------")
                
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ ==========")
                        
            except Exception as e:
                print(f"[Chat Stream] ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[Chat Stream] ì „ì†¡í•  ë©”ì‹œì§€ ê°œìˆ˜: {len(messages)}")
        print(f"[Chat Stream] ========== OpenAI APIì— ì „ì†¡í•  ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ==========")
        print(f"[Chat Stream] ==========ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© : ")
        print(f"{messages}")
        
        print(f"[Chat Stream] ========== ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ë ==========")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©
        return StreamingResponse(
            get_streaming_response_async(messages, http_request, request.generate_image or False),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[Chat Stream] LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[Chat Stream] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")



# ì§ˆë¬¸ ìœ í˜• íŒë³„ í•¨ìˆ˜
def is_first_question_in_conversation(conversation_id: int, db: Session) -> bool:
    """ëŒ€í™”ì—ì„œ ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
    try:
        message_count = db.query(Message).filter(Message.conversation_id == conversation_id).count()
        print(f"[QUESTION_TYPE] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ ìˆ˜: {message_count}")
        return message_count == 0
    except Exception as e:
        print(f"[QUESTION_TYPE] ë©”ì‹œì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ ì‹œ ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼

def get_conversation_context(conversation_id: int, db: Session) -> dict:
    """ëŒ€í™”ì˜ ì»¨í…ìŠ¤íŠ¸ì™€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # í•´ë‹¹ ëŒ€í™”ì˜ ëª¨ë“  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ìˆœ ì •ë ¬)
        messages = db.query(Message).filter(
            Message.conversation_id == conversation_id
        ).order_by(Message.created_at.asc()).all()
        
        print(f"[CONTEXT] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ {len(messages)}ê°œ ë¡œë“œ")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´
        if len(messages) > 0:
            print(f"[CONTEXT] ë©”ì‹œì§€ ìƒì„¸:")
            for i, msg in enumerate(messages):
                print(f"[CONTEXT]   {i+1}. ID: {msg.id}, q_mode: {msg.q_mode}, role: {msg.role}, question: {msg.question[:50] if msg.question else 'None'}...")
        else:
            print(f"[CONTEXT] âš ï¸ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€í™” ID {conversation_id} í™•ì¸ í•„ìš”")
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ì°¾ê¸° (q_modeê°€ "search"ì¸ ë©”ì‹œì§€)
        first_message = None
        for msg in messages:
            if msg.q_mode == "search":
                first_message = msg
                print(f"[CONTEXT] ì²« ë²ˆì§¸ ì§ˆë¬¸ ë°œê²¬: ë©”ì‹œì§€ ID {msg.id}")
                break
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation_history = []
        for msg in messages:
            if msg.question:
                conversation_history.append({"role": "user", "content": msg.question})
            if msg.ans:
                conversation_history.append({"role": "assistant", "content": msg.ans})
        
        return {
            "first_message": first_message,
            "conversation_history": conversation_history,
            "message_count": len(messages)
        }
        
    except Exception as e:
        print(f"[CONTEXT] ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return {
            "first_message": None,
            "conversation_history": [],
            "message_count": 0
        }

# SSE ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©)
@router.post("/langgraph/stream")
async def execute_langgraph_stream(request: StreamRequest, db: Session = Depends(get_db)):
    """LangGraph SSE ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    
    async def generate_sse():
        generator_id = str(uuid.uuid4())
        generator = SSEGenerator(generator_id)
        sse_generators[generator_id] = generator
        
        try:
            # OpenAI API í‚¤ í™•ì¸
            if not OPENAI_API_KEY:
                yield f"data: {json.dumps({'error': 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'})}\n\n"
                return
            
            print(f"[SSE] ğŸš€ LangGraph SSE ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {request.question}")
            
            # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸
            if request.conversation_id:
                is_first = is_first_question_in_conversation(request.conversation_id, db)
                if not is_first:
                    yield f"data: {json.dumps({'error': 'ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”'})}\n\n"
                    return
            
            # ì›Œí¬í”Œë¡œìš° í™•ì¸
            if langgraph_instance is None:
                yield f"data: {json.dumps({'error': 'LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'})}\n\n"
                return
            
            # ì´ˆê¸° ìƒíƒœì— generator_id ì¶”ê°€
            initial_state = {
                "question": request.question,
                "generator_id": generator_id
            }
            
            print(f"[SSE] LangGraph ì‹¤í–‰ ì‹œì‘: {request.question}")
            
            # LangGraph ì‹¤í–‰ì„ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            async def run_langgraph():
                try:
                    result = await langgraph_instance.ainvoke(initial_state)
                    # DONE ë©”ì‹œì§€ì— ì „ì²´ LangGraph ê²°ê³¼ í¬í•¨
                    done_message = {
                        "stage": "DONE", 
                        "result": result,  # ì „ì²´ LangGraph ê²°ê³¼ í¬í•¨
                        "keyword": result.get('keyword', []),
                        "candidates_total": result.get('candidates_total', [])
                    }
                    await generator.send_message(done_message)
                except Exception as e:
                    await generator.send_message({"stage": "ERROR", "error": str(e)})
                finally:
                    await generator.close()
            
            # LangGraph ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘
            langgraph_task = asyncio.create_task(run_langgraph())
            
            # SSE ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
            while generator.is_active:
                try:
                    # íƒ€ì„ì•„ì›ƒì„ ì§§ê²Œ ì„¤ì •í•˜ì—¬ ì‘ë‹µì„± í–¥ìƒ
                    message = await asyncio.wait_for(generator.message_queue.get(), timeout=0.1)
                    
                    if message is None:  # ì¢…ë£Œ ì‹ í˜¸
                        break
                    
                    # SSE í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡
                    yield f"data: {json.dumps(message)}\n\n"
                    
                except asyncio.TimeoutError:
                    # íƒ€ì„ì•„ì›ƒ ì‹œ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡
                    yield f"data: {json.dumps({'heartbeat': True})}\n\n"
                    continue
                except Exception as e:
                    print(f"[SSE] ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    break
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            yield f"data: [DONE]\n\n"
            
        except Exception as e:
            print(f"[SSE] ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        finally:
            # ì •ë¦¬
            if generator_id in sse_generators:
                del sse_generators[generator_id]
            print(f"[SSE] ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ: {generator_id}")
    
    return StreamingResponse(
        generate_sse(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

# LangGraph ì§ì ‘ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©) - ê¸°ì¡´ ìœ ì§€
@router.post("/langgraph")
async def execute_langgraph(request: StreamRequest, db: Session = Depends(get_db)):
    """LangGraphë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë°˜í™˜ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"[LangGraph] ğŸš€ ë­ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸
        if request.conversation_id:
            is_first = is_first_question_in_conversation(request.conversation_id, db)
            if not is_first:
                print(f"[LangGraph] âš ï¸ ì¶”ê°€ ì§ˆë¬¸ ê°ì§€ë¨ - LangGraph ì‹¤í–‰ ì°¨ë‹¨")
                raise HTTPException(
                    status_code=400, 
                    detail="ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
                )
        
        # ì›Œí¬í”Œë¡œìš° í™•ì¸
        if langgraph_instance is None:
            raise HTTPException(status_code=500, detail="LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        initial_state = {"question": request.question}
        print(f"[LangGraph] ì‹¤í–‰ ì‹œì‘: {request.question}")
        print(f"[LangGraph] ì´ˆê¸° ìƒíƒœ: {initial_state}")
        
        result = await langgraph_instance.ainvoke(initial_state)
        
        print(f"[LangGraph] âœ… ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ì—ì„œ íƒœê·¸ì™€ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
        tags = None
        db_search_title = None
        
        if isinstance(result, dict):
            # í‚¤ì›Œë“œ ì •ë³´ì—ì„œ íƒœê·¸ ì¶”ì¶œ
            if 'keyword' in result and result['keyword']:
                if isinstance(result['keyword'], list):
                    tags = ', '.join(result['keyword'])
                else:
                    tags = str(result['keyword'])
                print(f"[LangGraph] í‚¤ì›Œë“œ: {len(result['keyword'])}ê°œ")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
            if 'candidates_total' in result and result['candidates_total']:
                db_search_title = f"{len(result['candidates_total'])}ê±´"
                print(f"[LangGraph] ë¬¸ì„œ: {db_search_title}")
            
            # ì‘ë‹µ ì •ë³´ í™•ì¸
            if 'response' in result and result['response']:
                response_text = result['response'].get('answer', '')[:50] if isinstance(result['response'], dict) else str(result['response'])[:50]
                print(f"[LangGraph] ì‘ë‹µ: {response_text}...")
        
        print(f"[LangGraph] ìš”ì•½: í‚¤ì›Œë“œ {len(result.get('keyword', []))}ê°œ, ë¬¸ì„œ {len(result.get('candidates_total', []))}ê±´")
        
        return {
            "status": "success",
            "result": result,
            "tags": tags,
            "db_search_title": db_search_title,
            "message": "LangGraph ì‹¤í–‰ ì™„ë£Œ (ì²« ë²ˆì§¸ ì§ˆë¬¸)"
        }
        
    except Exception as e:
        print(f"[LangGraph] ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[LangGraph] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")


# ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.post("/langgraph/followup/stream")
async def execute_followup_question_stream(request: StreamRequest, http_request: Request, db: Session = Depends(get_db)):
    """ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ - ê¸°ì¡´ RAG ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œìš©"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[FOLLOWUP_STREAM] ğŸ”„ LLM ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” ID í™•ì¸
        if not request.conversation_id:
            return Response(content="Error: ì¶”ê°€ ì§ˆë¬¸ì€ conversation_idê°€ í•„ìš”í•©ë‹ˆë‹¤", media_type="text/plain")
        
        # LangGraph ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ ê²½ìš°)
        langgraph_context = getattr(request, 'langgraph_context', None)
        include_langgraph_context = getattr(request, 'include_langgraph_context', False)
        
        if include_langgraph_context and langgraph_context:
            print(f"[FOLLOWUP_STREAM] ğŸ”¬ LangGraph ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©")
            print(f"[FOLLOWUP_STREAM] ì›ë³¸ ì§ˆë¬¸: {langgraph_context.get('original_question', 'N/A')}")
            print(f"[FOLLOWUP_STREAM] í‚¤ì›Œë“œ: {langgraph_context.get('keywords', 'N/A')}")
            print(f"[FOLLOWUP_STREAM] ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(langgraph_context.get('search_results', []))}")
            
            # LangGraph ì»¨í…ìŠ¤íŠ¸ë¡œ ë¬¸ì„œ ì •ë³´ êµ¬ì„±
            document_title = "LangGraph ê²€ìƒ‰ ê²°ê³¼"
            search_results = langgraph_context.get('search_results', [])
            keywords = langgraph_context.get('keywords', [])
            previous_answer = langgraph_context.get('previous_answer', '')
            original_question = langgraph_context.get('original_question', '')
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±
            search_content = ""
            for i, result in enumerate(search_results[:3], 1):
                if isinstance(result, dict) and 'res_payload' in result:
                    title = result['res_payload'].get('document_name', f'ë¬¸ì„œ {i}')
                    vector_data = top_payload.get("vector", {})
                    # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
                    content = vector_data.get("text") if isinstance(vector_data, dict) else None
                    search_content += f"\n[ë¬¸ì„œ {i}] {title}: {content}"
            
            document_content = f"""
[ì²« ë²ˆì§¸ ì§ˆë¬¸] {original_question}

[ì¶”ì¶œëœ í‚¤ì›Œë“œ] {', '.join(keywords) if isinstance(keywords, list) else keywords}

[ê²€ìƒ‰ëœ ë¬¸ì„œë“¤]{search_content}

[ì´ì „ ë‹µë³€] {previous_answer[:500]}...
"""
        else:
            # ê¸°ì¡´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
            context = get_conversation_context(request.conversation_id, db)
            
            if not context["first_message"]:
                print(f"[FOLLOWUP_STREAM] âš ï¸ ì²« ë²ˆì§¸ ì§ˆë¬¸ ì—†ìŒ - ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬")
                document_title = "ì¼ë°˜ ëŒ€í™”"
                document_content = "ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            else:
                # ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œì™€ ë¬¸ì„œ ì •ë³´ í™œìš©
                first_message = context["first_message"]
                document_title = first_message.db_search_title or "ê´€ë ¨ ë¬¸ì„œ"
                document_content = f"í‚¤ì›Œë“œ: {first_message.keyword}\nê²€ìƒ‰ ê²°ê³¼: {first_message.db_search_title}\nì²« ë²ˆì§¸ ì§ˆë¬¸: {first_message.question}\nì²« ë²ˆì§¸ ë‹µë³€: {first_message.ans[:500] if first_message.ans else 'ë‹µë³€ ì—†ìŒ'}..."
        
        print(f"[FOLLOWUP_STREAM] ğŸ“„ ì¬ì‚¬ìš©í•  RAG ë¬¸ì„œ:")
        print(f"[FOLLOWUP_STREAM] ì œëª©: {document_title}")
        print(f"[FOLLOWUP_STREAM] ë‚´ìš© ê¸¸ì´: {len(document_content)} ë¬¸ì")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        if include_langgraph_context and langgraph_context:
            # LangGraph ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì‹œ ê¸°ë³¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ë§Œ ê°€ì ¸ì˜¤ê¸°
            context = get_conversation_context(request.conversation_id, db)
            conversation_history = context["conversation_history"]
        else:
            # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            conversation_history = context["conversation_history"]
        
        print(f"[FOLLOWUP_STREAM] ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {str(document_content)[:1500]}...

ìœ„ ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""
        
        # LLM API í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ)
        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
        messages.extend(recent_history)
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[FOLLOWUP_STREAM] ğŸ“¤ LLMì— ì „ì†¡í•  ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        print(f"[FOLLOWUP_STREAM] ğŸ“ í˜„ì¬ ì§ˆë¬¸: {request.question}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©
        return StreamingResponse(
            get_streaming_response_async(messages, http_request, request.generate_image or False),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[FOLLOWUP_STREAM] LLM ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[FOLLOWUP_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")

async def generate_image(prompt: str) -> str:
    """Generate an image using OpenAI DALL-E API"""
    try:
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ë¶€ë¶„ì—ì„œ OpenAI DALL-E API í˜¸ì¶œ
        # ì˜ˆì‹œ: 
        # client = OpenAI(api_key=OPENAI_API_KEY)
        # response = client.images.generate(prompt=prompt, n=1, size="1024x1024")
        # image_url = response.data[0].url
        
        # ì´ë¯¸ì§€ URL ë°˜í™˜ (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
        return None
    except Exception as e:
        print(f"Error generating image: {str(e)}")
        return None

async def get_streaming_response_async(messages: List[Dict], request: Request, generate_image: bool = False):
    """Stream a response from LLM using AsyncOpenAI with custom headers"""
    try:
        print(f"[LLM_STREAM] ğŸš€ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
        
        # ì´ë¯¸ì§€ URL (ì´ë¯¸ì§€ ìƒì„±ì´ ìš”ì²­ëœ ê²½ìš°)
        image_url = None
        if generate_image:
            # ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ë¡œì§ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            image_url = await generate_image(messages[-1]["content"] if messages else "")
        
        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        httpx_client = httpx.AsyncClient(verify=False, timeout=None)

        # print(f"[messages í™•ì¸] {messages}")              
        

        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "c.seunghoon",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await client.chat.completions.create(
                model="openai/gpt-oss-120b",
                messages=messages,
                stream=True,
            )
        
        print(f"[LLM_STREAM] ğŸ“¥ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
        
        text_response = ""
        async for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                text_response += content
                
                try:
                    # ë¹„-ASCII ë¬¸ì í—ˆìš©, UTF-8 bytesë¡œ ì¦‰ì‹œ ì „ì†¡
                    payload = json.dumps({'content': content}, ensure_ascii=False)
                    yield (f"data: {payload}\n\n").encode("utf-8")
                    await asyncio.sleep(0.01)  # ì²­í¬ ì‚¬ì´ì— ì§€ì—° ì¶”ê°€í•˜ì—¬ ë‹¤ë¥¸ API ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•¨
                except (ConnectionResetError, BrokenPipeError, OSError, ConnectionAbortedError, ConnectionError) as e:
                    # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
                    print(f"[LLM_STREAM] Client disconnected during streaming lv2: {type(e).__name__}")
                    return
                except Exception as e:
                    print(f"[LLM_STREAM] Unexpected error during streaming lv1: {str(e)}")
                    return
        
        # í…ìŠ¤íŠ¸ ì‘ë‹µì´ ì™„ë£Œëœ í›„ ì´ë¯¸ì§€ URLì´ ìˆìœ¼ë©´ ì „ì†¡
        if image_url:
            try:
                response_data = {
                    "text": text_response,
                    "image_url": image_url
                }
                payload = json.dumps(response_data, ensure_ascii=False)
                yield (f"data: {payload}\n\n").encode("utf-8")
            except Exception as e:
                print(f"[LLM_STREAM] Error sending image data: {str(e)}")
        
        print(f"[LLM_STREAM] âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        yield "data: [DONE]\n\n".encode("utf-8")
        
    except Exception as e:
        print(f"[LLM_STREAM] Error in streaming response: {str(e)}")
        import traceback
        print(f"[LLM_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        try:
            error_payload = json.dumps({'error': str(e)}, ensure_ascii=False)
            yield (f"data: {error_payload}\n\n").encode("utf-8")
            yield "data: [DONE]\n\n".encode("utf-8")
        except Exception:
            # ì—ëŸ¬ ì „ì†¡ë„ ì‹¤íŒ¨í•œ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
            return
