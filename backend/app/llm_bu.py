import os
import openai
from openai import AsyncOpenAI
import asyncio
import json
import httpx
import uuid
from fastapi import APIRouter, HTTPException, Response, Depends, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from langgraph.graph import END, StateGraph
from collections import defaultdict
from qdrant_client import QdrantClient
from app.utils.config import (
    OPENAI_API_KEY, OPENAI_BASE_URL,
    QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION,
    IMAGE_BASE_URL, IMAGE_PATH_PREFIX, IS_OPENAI_CONFIGURED
)
from app.database import get_db
from app.models import Conversation, Message, User
from app.utils.auth import get_current_user
from sqlalchemy.orm import Session

from datetime import datetime

# Create router 
router = APIRouter()

# Redis ì œê±°ë¨ - SSE ë°©ì‹ ì‚¬ìš©

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”© í™•ì¸
print(f"[Config] OpenAI API Key: {'ì„¤ì •ë¨' if OPENAI_API_KEY else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
print(f"[Config] Qdrant: {QDRANT_HOST}:{QDRANT_PORT} (ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'})")

# ì§ì ‘ êµ¬í˜„í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° í´ë˜ìŠ¤
class DirectSimilarityCalculator:
    def __init__(self):
        self.stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 'í•œ', 'í•˜ëŠ”', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê·¸', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ'}
        
    def preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”"""
        import re
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        cleaned_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text.lower())
        # í† í°í™”
        tokens = cleaned_text.split()
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ 2 ì´ìƒ í† í°ë§Œ ìœ ì§€
        filtered_tokens = [token for token in tokens if token not in self.stopwords and len(token) >= 2]
        return filtered_tokens
    
    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        tokens1 = set(self.preprocess_text(text1))
        tokens2 = set(self.preprocess_text(text2))
        
        if not tokens1 and not tokens2:
            return 1.0
        if not tokens1 or not tokens2:
            return 0.0
            
        intersection = len(tokens1.intersection(tokens2))
        union = len(tokens1.union(tokens2))
        
        return intersection / union if union > 0 else 0.0
    
    def calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (TF ê¸°ë°˜)"""
        tokens1 = self.preprocess_text(text1)
        tokens2 = self.preprocess_text(text2)
        
        # TF ê³„ì‚°
        tf1 = {}
        tf2 = {}
        
        for token in tokens1:
            tf1[token] = tf1.get(token, 0) + 1
        for token in tokens2:
            tf2[token] = tf2.get(token, 0) + 1
        
        # ì „ì²´ ë‹¨ì–´ ì§‘í•©
        all_tokens = set(tokens1 + tokens2)
        
        if not all_tokens:
            return 1.0
        
        # ë²¡í„° ìƒì„±
        vector1 = [tf1.get(token, 0) for token in all_tokens]
        vector2 = [tf2.get(token, 0) for token in all_tokens]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        dot_product = sum(a * b for a, b in zip(vector1, vector2))
        magnitude1 = sum(a * a for a in vector1) ** 0.5
        magnitude2 = sum(b * b for b in vector2) ** 0.5
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
            
        return dot_product / (magnitude1 * magnitude2)
    
    def calculate_combined_similarity(self, text1: str, text2: str) -> float:
        """ìì¹´ë“œì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ ìµœì¢… ìœ ì‚¬ë„"""
        jaccard_sim = self.calculate_jaccard_similarity(text1, text2)
        cosine_sim = self.calculate_cosine_similarity(text1, text2)
        
        # ê°€ì¤‘ í‰ê·  (ìì¹´ë“œ 0.4, ì½”ì‚¬ì¸ 0.6)
        combined_sim = (jaccard_sim * 0.4) + (cosine_sim * 0.6)
        return combined_sim
    
    def find_similar_documents(self, query: str, documents: List[dict], top_k: int = 5) -> List[dict]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ì°¾ì•„ ë°˜í™˜"""
        results = []
        
        for doc in documents:
            # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ëª¨ë“  ê´€ë ¨ í•„ë“œ ê²°í•©
            doc_title = doc.get('document_name', '')
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ê²°í•©í•˜ì—¬ í’ë¶€í•œ ë‚´ìš© ìƒì„±
            text_parts = [doc_title]  # ì œëª©ì€ í•­ìƒ í¬í•¨
            
            if 'vector' in doc and isinstance(doc['vector'], dict):
                vector_data = doc['vector']
                # ëª¨ë“  í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ê²°í•© (ì‹¤ì œ DB êµ¬ì¡°ì— ë§ì¶¤)
                for field in ['text', 'summary_purpose', 'summary_result', 'summary_fb']:
                    if field in vector_data and vector_data[field]:
                        text_parts.append(str(vector_data[field]))
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ ì—°ê²°
            doc_text = " ".join(text_parts).strip()
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = self.calculate_combined_similarity(query, doc_text)
            
            results.append({
                'document': doc,
                'similarity': similarity,
                'matched_text': doc_text[:200] + '...' if len(doc_text) > 200 else doc_text
            })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]

# 4ì°¨ì› ë²¡í„° ìƒì„± í•¨ìˆ˜
def generate_4d_vector(text: str) -> List[float]:
    """í…ìŠ¤íŠ¸ë¥¼ 4ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜"""
    import hashlib
    import struct
    
    # í…ìŠ¤íŠ¸ë¥¼ í•´ì‹œë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ëœ ë²¡í„° ìƒì„±
    hash_obj = hashlib.md5(text.encode('utf-8'))
    hash_bytes = hash_obj.digest()
    
    # 4ê°œì˜ float ê°’ ìƒì„± (0.0 ~ 1.0 ë²”ìœ„)
    vector = []
    for i in range(4):
        # 4ë°”ì´íŠ¸ì”© ì½ì–´ì„œ floatë¡œ ë³€í™˜
        byte_chunk = hash_bytes[i*4:(i+1)*4]
        if len(byte_chunk) < 4:
            byte_chunk += b'\x00' * (4 - len(byte_chunk))
        
        # unsigned intë¡œ ë³€í™˜ í›„ ì •ê·œí™”
        uint_val = struct.unpack('>I', byte_chunk)[0]
        normalized_val = uint_val / (2**32 - 1)  # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì •ê·œí™”
        vector.append(normalized_val)
    
    return vector

# ë²¡í„° ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
async def direct_document_search(question_type: str, limit: int, queries: List[str], 
                               ip: str, port: int, collection: str) -> List[dict]:
    """ë²¡í„° ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
    try:
        print(f"[VECTOR_SEARCH] ë²¡í„° ê²€ìƒ‰ ì‹œì‘: {len(queries)}ê°œ ì¿¼ë¦¬")
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        if ip.startswith(('http://', 'https://')):
            qdrant_url = f"{ip.rstrip('/')}" + (f":{port}" if not ip.endswith(f":{port}") else "")
            client = QdrantClient(url=qdrant_url)
        else:
            client = QdrantClient(host=ip, port=port)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        try:
            collections = client.get_collections()
            if not collection or collection not in [col.name for col in collections.collections]:
                print(f"[VECTOR_SEARCH] ì»¬ë ‰ì…˜ '{collection}' ì‚¬ìš© ë¶ˆê°€")
                return []
        except Exception as e:
            print(f"[VECTOR_SEARCH] Qdrant ì—°ê²° ì˜¤ë¥˜: {e}")
            return []
        
        all_results = []
        
        for query in queries:
            try:
                # ìì²´ 4ì°¨ì› ë²¡í„° ìƒì„±
                query_vector = generate_4d_vector(query)
                
                # Qdrantì—ì„œ ë²¡í„° ê²€ìƒ‰
                search_result = client.search(
                    collection_name=collection,
                    query_vector=query_vector,
                    limit=limit,
                    with_payload=True,
                    with_vectors=False
                )
                
                # ê²°ê³¼ ë³€í™˜
                for hit in search_result:
                    all_results.append({
                        'res_id': hit.id,
                        'res_score': hit.score,
                        'type_question': question_type,
                        'type_vector': 'custom_4d_vector',
                        'res_payload': hit.payload
                    })
                
                print(f"[VECTOR_SEARCH] '{query}' ê²€ìƒ‰ ì™„ë£Œ: {len(search_result)}ê±´")
                
            except Exception as e:
                print(f"[VECTOR_SEARCH] ì¿¼ë¦¬ '{query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                # ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                fallback_results = await fallback_text_search([query], client, collection, limit)
                all_results.extend(fallback_results)
                continue
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        all_results.sort(key=lambda x: x['res_score'], reverse=True)
        final_results = all_results[:limit]
        
        print(f"[VECTOR_SEARCH] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(final_results)}ê±´")
        for i, result in enumerate(final_results[:3]):
            title = result['res_payload'].get('document_name', 'ì œëª©ì—†ìŒ')
            score = result['res_score']
            print(f"[VECTOR_SEARCH]   {i+1}. {title} (ë²¡í„° ìœ ì‚¬ë„: {score:.4f})")
        
        return final_results
        
    except Exception as e:
        print(f"[VECTOR_SEARCH] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# í´ë°±ìš© í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰
async def fallback_text_search(queries: List[str], client, collection: str, limit: int) -> List[dict]:
    """OpenAI API ë¯¸ì„¤ì • ì‹œ í´ë°±ìš© í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    try:
        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        scroll_result = client.scroll(
            collection_name=collection,
            limit=1000,
            with_payload=True
        )
        
        all_documents = []
        for point in scroll_result[0]:
            if point.payload:
                all_documents.append({
                    'id': point.id,
                    'payload': point.payload
                })
        
        if not all_documents:
            return []
        
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_calc = DirectSimilarityCalculator()
        all_results = []
        
        for query in queries:
            query_results = similarity_calc.find_similar_documents(
                query=query,
                documents=[doc['payload'] for doc in all_documents],
                top_k=limit
            )
            
            for i, result in enumerate(query_results):
                if result['similarity'] > 0:  # 0ì  ì œì™¸
                    all_results.append({
                        'res_id': all_documents[i]['id'] if i < len(all_documents) else f"doc_{i}",
                        'res_score': result['similarity'],
                        'type_question': 'fallback',
                        'type_vector': 'text_similarity',
                        'res_payload': result['document']
                    })
        
        all_results.sort(key=lambda x: x['res_score'], reverse=True)
        return all_results[:limit]
        
    except Exception as e:
        print(f"[FALLBACK_SEARCH] í´ë°± ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜ë“¤ ì œê±° - ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´




# ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì„ ìœ„í•œ í´ë˜ìŠ¤
class StreamRequest(BaseModel):
    question: str
    conversation_id: Optional[int] = None
    generate_image: Optional[bool] = False  # ì´ë¯¸ì§€ ìƒì„± í”Œë˜ê·¸ ì¶”ê°€
    # LangGraph ì»¨í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ê°€
    langgraph_context: Optional[dict] = None
    include_langgraph_context: Optional[bool] = False

# ì´ë¯¸ì§€ URL (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
# SAMPLE_IMAGE_URLS = []

# LangGraph ìƒíƒœ ì •ì˜
class SearchState(dict):
    question: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    keyword: str       # ì‚¬ìš©ì ì…ë ¥ ì§ˆì˜
    candidates_each: List[dict] 
    candidates_total: List[dict] 
    response: List[dict]     # LLMì´ ìƒì„±í•œ ì‘ë‹µ
    generator_id: str   # SSE ì œë„ˆë ˆì´í„° ID

# SSE ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì œë„ˆë ˆì´í„° í´ë˜ìŠ¤
class SSEGenerator:
    def __init__(self, generator_id: str):
        self.generator_id = generator_id
        self.message_queue = asyncio.Queue()
        self.is_active = True
        self.message_count = 0
        print(f"[SSE_GEN] ğŸ†• SSEGenerator ìƒì„±: {generator_id}")
        
    async def send_message(self, message: dict):
        """ë©”ì‹œì§€ë¥¼ íì— ì¶”ê°€"""
        if self.is_active:
            self.message_count += 1
            # print(f"[SSE_GEN] ğŸ“¥ ë©”ì‹œì§€ #{self.message_count} íì— ì¶”ê°€: {message.get('stage', 'unknown')}:{message.get('status', 'unknown')}")
            await self.message_queue.put(message)
            # print(f"[SSE_GEN] âœ… ë©”ì‹œì§€ í ì¶”ê°€ ì™„ë£Œ, í˜„ì¬ í í¬ê¸°: {self.message_queue.qsize()}")
        else:
            print(f"[SSE_GEN] âŒ ë¹„í™œì„± ì œë„ˆë ˆì´í„°ì— ë©”ì‹œì§€ ì „ì†¡ ì‹œë„: {self.generator_id}")
    
    async def close(self):
        """ì œë„ˆë ˆì´í„° ì¢…ë£Œ"""
        print(f"[SSE_GEN] ğŸ”š ì œë„ˆë ˆì´í„° ì¢…ë£Œ ì‹œì‘: {self.generator_id}")
        self.is_active = False
        await self.message_queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸
        print(f"[SSE_GEN] âœ… ì œë„ˆë ˆì´í„° ì¢…ë£Œ ì™„ë£Œ: {self.generator_id}")

# SSE ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
sse_generators = {}

# SSE ìƒíƒœ ë°œí–‰ í•¨ìˆ˜
async def yield_node_status(generator_id: str, node_name: str, status: str, data: dict):
    """SSEë¥¼ í†µí•´ ë…¸ë“œ ìƒíƒœë¥¼ ë°œí–‰"""
    # print(f"[SSE] ğŸ”„ ë©”ì‹œì§€ ì „ì†¡ ì‹œë„: {node_name}:{status} (generator_id: {generator_id})")
    
    if generator_id not in sse_generators:
        print(f"[SSE] âŒ ì œë„ˆë ˆì´í„° IDê°€ ì—†ìŒ - {node_name}: {status}")
        print(f"[SSE] í˜„ì¬ í™œì„± ì œë„ˆë ˆì´í„°: {list(sse_generators.keys())}")
        return
        
    try:
        message = {
            "stage": node_name,
            "status": status,
            "result": data,
            "timestamp": asyncio.get_event_loop().time()
        }
        
        # print(f"[SSE] ğŸ“¤ ì „ì†¡í•  ë©”ì‹œì§€: {message}")
        
        # SSE ì œë„ˆë ˆì´í„°ì— ë©”ì‹œì§€ ì „ì†¡
        generator = sse_generators.get(generator_id)
        if generator and generator.is_active:
            await generator.send_message(message)
            # print(f"[SSE] âœ… ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ: {node_name}:{status}")
            
            # ë©”ì‹œì§€ê°€ íì— ì œëŒ€ë¡œ ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸
            queue_size = generator.message_queue.qsize()
            # print(f"[SSE] ğŸ“Š ë©”ì‹œì§€ í í¬ê¸°: {queue_size}")
            
            # ë©”ì‹œì§€ ì „ì†¡ í›„ ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
        else:
            print(f"[SSE] âŒ ì œë„ˆë ˆì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {generator_id}")
            print(f"[SSE] ì œë„ˆë ˆì´í„° ìƒíƒœ: active={generator.is_active if generator else 'None'}")
    except Exception as e:
        print(f"[SSE] âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {node_name}:{status} - ì˜¤ë¥˜: {e}")
        import traceback
        print(f"[SSE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        pass

# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
async def node_rc_init(state: SearchState) -> SearchState:
    """ì´ˆê¸°í™” ë…¸ë“œ"""
    print("[inform]: node_rc_init")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    print(f"[NODE_INIT] ğŸ” generator_id: {generator_id}")
    print(f"[NODE_INIT] ğŸ” sse_generators í‚¤: {list(sse_generators.keys())}")
    
    if generator_id:
        print(f"[NODE_INIT] ğŸ“¤ A:started ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
        await yield_node_status(
            generator_id,
            "A",
            "started",
            {
                "message": "ì´ˆê¸°í™” ë‹¨ê³„ ì‹œì‘",
                "step": "A. ì´ˆê¸°í™”"
            }
        )
        print(f"[NODE_INIT] âœ… A:started ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
    else:
        print(f"[NODE_INIT] âŒ generator_idê°€ ì—†ìŒ")
    
    try: 
        question = state['question']
        generator_id = state.get('generator_id')
        
        if generator_id:
            print(f"[NODE_INIT] ğŸ“¤ A:completed ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
            await yield_node_status(
                generator_id,
                "A",
                "completed",
                {
                    "message": "ì´ˆê¸°í™” ë‹¨ê³„ ì™„ë£Œ",
                    "step": "A. ì´ˆê¸°í™” ì™„ë£Œ",
                    "question": question
                }
            )
            print(f"[NODE_INIT] âœ… A:completed ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
        
        return {
            "question": question,
            "keyword": "",
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": state.get('generator_id')  # ìƒíƒœì—ì„œ generator_id ìœ ì§€
        }
    except Exception as e:
        print("[error]: node_rc_init")
        raise RuntimeError(f"[error]: node_rc_init: {str(e)}")

async def node_rc_keyword(state: SearchState) -> SearchState:
    """í‚¤ì›Œë“œ ì¦ê°• ë…¸ë“œ - LLMì„ ì‚¬ìš©í•œ ë™ì  í‚¤ì›Œë“œ ìƒì„±"""
    print("[inform]: node_rc_keyword")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    print(f"[NODE_KEYWORD] ğŸ” generator_id: {generator_id}")
    
    if generator_id:
        print(f"[NODE_KEYWORD] ğŸ“¤ B:started ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
        await yield_node_status(
            generator_id,
            "B",
            "started",
            {
                "message": "í‚¤ì›Œë“œ ì¦ê°• ë‹¨ê³„ ì‹œì‘",
                "step": "B. í‚¤ì›Œë“œ ì¦ê°•",
                "question": state['question']
            }
        )
        print(f"[NODE_KEYWORD] âœ… B:started ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
    else:
        print(f"[NODE_KEYWORD] âŒ generator_idê°€ ì—†ìŒ")
    
    full_text_parts = []
    try:
        question = state['question']
        
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your-openai-api-key-here":
            print("[error]: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            base_keywords = [question]
            generator_id = state.get('generator_id')
            if generator_id:
                await yield_node_status(
                generator_id,
                "B",
                "completed",
                {
                    "message": "í‚¤ì›Œë“œ ì¦ê°• ë‹¨ê³„ ì™„ë£Œ (ê¸°ë³¸ ëª¨ë“œ)",
                    "step": "B. í‚¤ì›Œë“œ ì¦ê°• ì™„ë£Œ",
                    "keywords": base_keywords,
                    "keywords_count": len(base_keywords),
                    "reason": "OpenAI API í‚¤ ë¯¸ì„¤ì •"
                }
            )
            
            return {
                "question": state['question'],
                "keyword": base_keywords,
                "candidates_each": [],
                "candidates_total": [],
                "response": [],
                "generator_id": state.get('generator_id')  # ìƒíƒœì—ì„œ generator_id ìœ ì§€
            }
        
        try:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¦ê°• - ìƒˆë¡œìš´ LLM ë°©ì‹
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ì „ë¬¸ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³ , ìµœëŒ€ 15ê°œê¹Œì§€ ìƒì„±í•˜ì„¸ìš”."},
                {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”: {question}"}
            ]
            
            # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            httpx_client = httpx.AsyncClient(verify=False, timeout=None)

            # print(f"[messages í™•ì¸] {messages}")              
            
            # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "c.seunghoon",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
            # ë¹„ë™ê¸° í˜¸ì¶œ
            response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                stream=True,
            )
            # ìŠ¤íŠ¸ë¦¼ ì²­í¬ ìˆ˜ì‹ 
            async for chunk in response:
                if not chunk.choices:
                    continue
                delta = chunk.choices[0].delta
                content = getattr(delta, "content", None)
                if not content:
                    continue

                # ë¶€ë¶„ ì‘ë‹µ ëˆ„ì 
                full_text_parts.append(content)

                # ë„ˆë¬´ ë¹¡ë¹¡í•œ ë£¨í”„ ë°©ì§€
                await asyncio.sleep(0)

        except Exception as e:
            print(f"[error] streaming failed: {type(e).__name__}: {e}")
            full_text_parts = []

        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ì „ì²´ ì‘ë‹µ ë¬¸ìì—´ ì¡°ë¦½
        llm_response = "".join(full_text_parts).strip()

        # âš™ï¸ í‚¤ì›Œë“œ ë³€í™˜ ë¡œì§ (ê¸°ì¡´ ë™ì¼)
        if not llm_response:
            augmented_keywords = [question]
        else:
            keywords_text = llm_response.strip()
            # ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬ ë° ì •ë¦¬
            augmented_keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
            # ì› ì§ˆë¬¸ ì¶”ê°€
            augmented_keywords.insert(0, question)
            # ì¤‘ë³µ ì œê±° ë° 20ê°œ ì œí•œ
            augmented_keywords = list(dict.fromkeys(augmented_keywords))[:20]

        print(f"[inform]: LLMì„ í†µí•´ ìƒì„±ëœ í‚¤ì›Œë“œ: {len(augmented_keywords)}ê°œ")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "B",
                "completed",
                {
                    "message": "í‚¤ì›Œë“œ ì¦ê°• ë‹¨ê³„ ì™„ë£Œ",
                    "step": "B. í‚¤ì›Œë“œ ì¦ê°• ì™„ë£Œ",
                    "keywords": augmented_keywords,
                    "keywords_count": len(augmented_keywords)
                }
            )
        
        return {
            "question": state['question'],
            "keyword": augmented_keywords,
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": state.get('generator_id')  # ìƒíƒœì—ì„œ generator_id ìœ ì§€
        }
    except Exception as e:
        print("[error]: node_rc_keyword")
        raise RuntimeError(f"[error]: node_rc_keyword: {str(e)}")

async def node_rc_rag(state: SearchState) -> SearchState:
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("[inform]: node_rc_rag")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
            generator_id,
            "C",
            "started",
            {
                "message": "RAG ê²€ìƒ‰ ë‹¨ê³„ ì‹œì‘",
                "step": "C. RAG ê²€ìƒ‰",
                "keywords": state.get('keyword', [])
            }
        )
    
    try:
        # ë²¡í„° DB ì„¤ì •
        ip, port, collection = QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION
        
        # ì§ì ‘ ê²€ìƒ‰ ìˆ˜í–‰
        candidates_each = []
        
        # questionìœ¼ë¡œ ê²€ìƒ‰
        if state.get('question'):
            try:
                question_results = await direct_document_search('question', 5, [state['question']], ip, port, collection)
                candidates_each.extend(question_results)
            except Exception as e:
                print(f"Question ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # keywordë¡œ ê²€ìƒ‰ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬)
        if state.get('keyword'):
            try:
                # keywordê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                if isinstance(state['keyword'], list):
                    keywords = state['keyword']
                else:
                    keywords = [state['keyword']]
                
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ í•„í„°ë§
                keywords = [k for k in keywords if k and isinstance(k, str) and k.strip()]
                
                if keywords:
                    keyword_results = await direct_document_search('keyword', 3, keywords, ip, port, collection)
                    candidates_each.extend(keyword_results)
            except Exception as e:
                print(f"Keyword ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í•˜ë“œì½”ë”© ì œê±°)
        if not candidates_each:
            print("[RAG] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ë™ì  ì ìˆ˜ ì§‘ê³„ (í•˜ë“œì½”ë”© ì œê±°)
        aggregated_scores = defaultdict(float)
        payloads = {}
        
        # candidates_eachê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if candidates_each:
            for item in candidates_each:
                try:
                    res_id = item.get('res_id')
                    score = item.get('res_score', 0.0)
                    
                    if res_id is not None and score > 0:
                        # ë‹¨ìˆœ ì ìˆ˜ í•©ì‚° (ê°€ì¤‘ì¹˜ ì œê±°)
                        aggregated_scores[res_id] += score
                        if res_id not in payloads:
                            payloads[res_id] = item.get('res_payload', {})
                except Exception as e:
                    print(f"ê°œë³„ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ì„ íƒ (ê³ ì • ê°œìˆ˜ ì œê±°)
        candidates_total = sorted(
            [{'res_id': res_id, 'res_score': aggregated_scores[res_id], 'res_payload': payloads[res_id]}
             for res_id in aggregated_scores],
            key=lambda x: x['res_score'],
            reverse=True
        )
        
        # ë™ì ìœ¼ë¡œ ê²°ê³¼ ê°œìˆ˜ ê²°ì • (ìµœì†Œ 1ê°œ, ìµœëŒ€ 10ê°œ)
        if candidates_total:
            max_results = min(len(candidates_total), max(1, min(10, len(candidates_total))))
            candidates_total = candidates_total[:max_results]
        
        print(f"[RAG] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 5ê±´):")
        for i, candidate in enumerate(candidates_total):
            title = candidate.get('res_payload', {}).get('document_name', 'ì œëª© ì—†ìŒ')
            vector_data = candidate.get("vector", {})
            # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
            summary = vector_data.get("summary_result") if isinstance(vector_data, dict) else None
            score = candidate.get('res_score', 0)
            print(f"  {i+1}. {title} - {summary} (ìœ ì‚¬ë„: {score:.4f})")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "C",
                "completed",
                {
                    "message": "RAG ê²€ìƒ‰ ë‹¨ê³„ ì™„ë£Œ",
                    "step": "C. RAG ê²€ìƒ‰ ì™„ë£Œ",
                    "documents_count": len(candidates_total),
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ")
                        for candidate in candidates_total
                    ],
                    "search_results": [
                        {
                            "title": candidate.get("res_payload", {}).get("document_name", "ì œëª© ì—†ìŒ"),
                            "text": candidate.get("res_payload", {}).get("vector", {}).get("text", "ë‚´ìš© ì—†ìŒ"),
                            "summary": candidate.get("res_payload", {}).get("vector", {}).get("summary_result", "ìš”ì•½ ì—†ìŒ"),
                            "image_url": candidate.get("res_payload", {}).get("vector", {}).get("image_url", ""),
                            "score": candidate.get("res_score", 0)
                        }
                        for candidate in candidates_total[:5]  # ìƒìœ„ 5ê°œë§Œ
                    ]
                }
            )
        
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_total": candidates_total,
            "response": [],
            "generator_id": state.get('generator_id')
        }
    except Exception as e:
        print(f"[error]: node_rc_rag: {str(e)}")
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ìƒíƒœ ë°˜í™˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê³„ì† ì§„í–‰
        return {
            "question": state.get('question', ''),
            "keyword": state.get('keyword', ''),
            "candidates_each": [],
            "candidates_total": [],
            "response": [],
            "generator_id": state.get('generator_id')
        }

async def node_rc_rerank(state: SearchState) -> SearchState:
    """ë™ì  ì¬ìˆœìœ„ ë…¸ë“œ (í•˜ë“œì½”ë”© ì œê±°)"""
    print("[inform]: node_rc_rerank")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
            generator_id,
            "D",
            "started",
            {
                "message": "ë¬¸ì„œ ì¬ìˆœìœ„ ë‹¨ê³„ ì‹œì‘",
                "step": "D. ë¬¸ì„œ ì¬ìˆœìœ„",
                "documents_count": len(state['candidates_total'])
            }
        )
    
    try:
        candidates_top = state['candidates_total']
        
        if not candidates_top:
            print("[RERANK] ì¬ìˆœìœ„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "question": state['question'],
                "keyword": state["keyword"],
                "candidates_total": state["candidates_total"],
                "response": [],
                "generator_id": state.get('generator_id')
            }
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ë™ì  ì¬ìˆœìœ„ (í•˜ë“œì½”ë”©ëœ 0.1 ê°ì†Œ ì œê±°)
        similarity_calc = DirectSimilarityCalculator()
        question = state['question']
        
        for candidate in candidates_top:
            try:
                # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                payload = candidate.get('res_payload', {})
                doc_title = payload.get('document_name', '')
                vector_data = payload.get("vector", {})
                # vectorê°€ dictì¸ì§€ í™•ì¸ í›„ íŠ¹ì • í‚¤ê°’ë§Œ ì¶”ì¶œ
                doc_content = vector_data.get("text") if isinstance(vector_data, dict) else None

                doc_text = f"{doc_title} {doc_content}"
                
                # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê°„ ì§ì ‘ ìœ ì‚¬ë„ ê³„ì‚°
                relevance_score = similarity_calc.calculate_combined_similarity(question, doc_text)
                
                # ê¸°ì¡´ ê²€ìƒ‰ ì ìˆ˜ì™€ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê²°í•©
                original_score = candidate.get('res_score', 0.0)
                combined_score = (original_score * 0.6) + (relevance_score * 0.4)
                
                candidate.update({
                    'res_relevance': relevance_score,
                    'combined_score': combined_score
                })
                
            except Exception as e:
                print(f"[RERANK] ê°œë³„ ë¬¸ì„œ ì¬ìˆœìœ„ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                candidate.update({
                    'res_relevance': candidate.get('res_score', 0.0),
                    'combined_score': candidate.get('res_score', 0.0)
                })
        
        # ê²°í•© ì ìˆ˜ë¡œ ì •ë ¬ (í•˜ë“œì½”ë”©ëœ ì •ë ¬ ë°©ì‹ ì œê±°)
        sorted_candidates_top = sorted(candidates_top, key=lambda x: x.get('combined_score', 0), reverse=True)
        
        print(f"[RERANK] ì¬ìˆœìœ„ ì™„ë£Œ: {len(sorted_candidates_top)}ê°œ ë¬¸ì„œ")
        for i, candidate in enumerate(sorted_candidates_top[:3]):
            title = candidate.get('res_payload', {}).get('document_name', 'ì œëª©ì—†ìŒ')
            combined_score = candidate.get('combined_score', 0)
            relevance = candidate.get('res_relevance', 0)
            print(f"[RERANK]   {i+1}. {title} (ê²°í•©ì ìˆ˜: {combined_score:.4f}, ê´€ë ¨ì„±: {relevance:.4f})")
        
        generator_id = state.get('generator_id')
        if generator_id:
            await yield_node_status(
                generator_id,
                "D",
                "completed",
                {
                    "message": "ë¬¸ì„œ ì¬ìˆœìœ„ ë‹¨ê³„ ì™„ë£Œ",
                    "step": "D. ë¬¸ì„œ ì¬ìˆœìœ„ ì™„ë£Œ",
                    "documents_count": len(sorted_candidates_top),
                    "document_titles": [
                        candidate.get("res_payload", {}).get("document_name", "ì œëª©ì—†ìŒ")
                        for candidate in sorted_candidates_top[:3]
                    ]
                }
            )
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": sorted_candidates_top,
            "generator_id": state.get('generator_id')
        }
    except Exception as e:
        print("[error]: node_rc_rerank")
        raise RuntimeError(f"[error]: node_rc_rerank: {str(e)}")

async def node_rc_answer(state: SearchState) -> SearchState:
    """ë‹µë³€ ìƒì„± ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©) - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›"""
    print("[inform]: node_rc_answer ì‹¤í–‰")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
            generator_id,
            "E",
            "started",
            {
                "message": "ë‹µë³€ ìƒì„± ë‹¨ê³„ ì‹œì‘",
                "step": "E. ë‹µë³€ ìƒì„±",
                "search_results_count": len(state['response'])
            }
        )
    
    try:
        candidates_top = state['response']
        generator_id = state.get('generator_id')
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if candidates_top:
            print(f"[Answer] âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. LLM API í˜¸ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ìƒìœ„ 1ê±´ì˜ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            top_result = candidates_top[0]
            top_payload = top_result.get('res_payload', {})
            
            # ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
            document_title = top_payload.get('document_name', 'ì œëª© ì—†ìŒ')
            vector_data = top_payload.get("vector", {})
            document_content = vector_data.get("text") if isinstance(vector_data, dict) else None
            
            # LLMì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
ë¬¸ì„œ ì œëª©: {document_title}
ë¬¸ì„œ ë‚´ìš©: {str(document_content)[:1000]}...

[ì§ˆë¬¸]
{state['question']}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:
- í•œêµ­ì–´ë¡œ êµ¬ì–´ì²´ë¡œ ì‘ì„±
- í˜•ì‹ì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹µë³€ë§Œ ì‘ì„±í•˜ê³  ì¶”ê°€ì ì¸ í—¤ë”ë‚˜ í˜•ì‹ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
            """.strip()
            
            # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
            llm_answer = ""
            try:
                if IS_OPENAI_CONFIGURED:
                    print(f"[Answer] ğŸš€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° LLM API í˜¸ì¶œ ì‹œì‘...")
                    
                    messages = [{"role": "user", "content": prompt}]
                    
                    # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)
                    httpx_client = httpx.AsyncClient(
                        verify=False, 
                        timeout=httpx.Timeout(30.0, connect=10.0)  # ì—°ê²° íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    )
                    
                    # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    client = AsyncOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        http_client=httpx_client,
                        default_headers={
                            "x-dep-ticket": OPENAI_API_KEY,
                            "Send-System-Name": "ds2llm",
                            "User-Id": "c.seunghoon",
                            "User-Type": "AD_ID",
                            "Prompt-Msg-Id": str(uuid.uuid4()),
                            "Completion-Msg-Id": str(uuid.uuid4()),
                        }
                    )
                    
                    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                    response = await client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=messages,
                        stream=True,
                    )
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ SSEë¡œ ì „ì†¡
                    async for chunk in response:
                        if chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            llm_answer += content
                            
                            # ì‹¤ì‹œê°„ìœ¼ë¡œ SSEë¥¼ í†µí•´ ë‹µë³€ ì²­í¬ ì „ì†¡
                            if generator_id:
                                await yield_node_status(
                                    generator_id,
                                    "D",
                                    "streaming",
                                    {
                                        "message": "ë‹µë³€ ìƒì„± ì¤‘...",
                                        "content": content,
                                        "accumulated_answer": llm_answer,
                                        "is_streaming": True
                                    }
                                )
                            
                            # ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
                    
                    print(f"[Answer] âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
                    
                else:
                    print(f"[Answer] âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {str(document_content)[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
                    
            except Exception as e:
                error_message = str(e)
                print(f"[Answer] âŒ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {error_message}")
                
                # ì—°ê²° ì˜¤ë¥˜ íƒ€ì…ë³„ ì²˜ë¦¬
                if "APIConnectionError" in error_message or "Connection error" in error_message:
                    error_desc = "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                elif "timeout" in error_message.lower():
                    error_desc = "AI ì„œë¹„ìŠ¤ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                elif "authentication" in error_message.lower() or "unauthorized" in error_message.lower():
                    error_desc = "AI ì„œë¹„ìŠ¤ ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                else:
                    error_desc = "AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                
                llm_answer = f"""ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ: {document_title}

ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, {str(document_content)[:200]}...ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

âš ï¸ {error_desc}"""
            
            # ì´ë¯¸ì§€ URL ìƒì„± (ì²« ë²ˆì§¸ ë¬¸ì„œ ê¸°ë°˜)
            image_url = None
            if top_result and top_payload:
                doc_title = top_payload.get('document_name', '')
                doc_index = top_result.get('res_id', 0)
                
                if doc_title and doc_index:
                    import urllib.parse
                    import os
                    # .txt í™•ì¥ì ì œê±°í•˜ê³  _whole.jpgë¡œ ëŒ€ì²´
                    if doc_title.endswith('.txt'):
                        doc_title_without_ext = doc_title[:-4]  # .txt ì œê±°
                        image_filename = f"{doc_title_without_ext}_whole.jpg"
                    else:
                        # í™•ì¥ìê°€ ì—†ê±°ë‚˜ ë‹¤ë¥¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        image_filename = doc_title
                    
                    safe_title = urllib.parse.quote(image_filename, safe='')
                    image_url = f"{IMAGE_BASE_URL}{IMAGE_PATH_PREFIX}/{safe_title}"
                    print(f"[Answer] ğŸ–¼ï¸ ìƒì„±ëœ ì´ë¯¸ì§€ URL: {image_url}")

            # LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ ìœ„í•œ ì™„ì „í•œ ì‘ë‹µ êµ¬ì¡°
            response = {
                "res_id": [rest['res_id'] for rest in candidates_top],
                "answer": llm_answer,  # ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±ëœ ì „ì²´ ë‹µë³€
                "q_mode": "search",
                "keyword": state["keyword"],
                "db_contents": [] ,
                "top_document": top_result,
                "analysis_image_url": image_url
            }
        else:
            print(f"[Answer] âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            response = {
                "res_id": [],
                "answer": f"ì…ë ¥í•˜ì‹  '{state['question']}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "q_mode": "search",
                "keyword": state["keyword"],
            }
        
        # ìµœì¢… ì™„ë£Œ ìƒíƒœ ì „ì†¡
        if generator_id:
            await yield_node_status(
                generator_id,
                "E",
                "completed",
                {
                    "message": "ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ",
                    "step": "E. ë‹µë³€ ìƒì„± ì™„ë£Œ",
                    "answer": response.get("answer", ""),
                    "analysis_image_url": response.get("analysis_image_url"),
                    "keywords": response.get("keyword", state.get("keyword", [])),
                    "document_titles": [],  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (db_contents ì‚¬ìš©)
                    "top_document": response.get("top_document"),
                    "is_streaming": False
                },
            )
        
        print(f"[Answer] âœ… node_rc_answer í•¨ìˆ˜ ì™„ë£Œ")
        
        return {
            "question": state['question'],
            "keyword": state["keyword"],
            "candidates_total": state["candidates_total"],
            "response": response,
            "generator_id": state.get('generator_id')
        }
        
    except Exception as e:
        print("[error]: node_rc_answer")
        import traceback
        print(f"[error] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise RuntimeError(f"[error]: node_rc_answer: {str(e)}")

async def node_rc_plain_answer(state: SearchState) -> SearchState:
    """ê¸°ë³¸ ë‹µë³€ ë…¸ë“œ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    print("[inform]: node_rc_plain_answer ì‹¤í–‰")
    
    
    # ë‹¨ê³„ ì‹œì‘ ìƒíƒœ ì „ì†¡
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
            generator_id,
            "E",
            "started",
            {
                "message": "ê¸°ë³¸ ë‹µë³€ ìƒì„± ë‹¨ê³„ ì‹œì‘",
                "step": "E. ê¸°ë³¸ ë‹µë³€ ìƒì„±",
                "reason": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
            }
        )
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë” êµ¬ì²´ì ì¸ ì•ˆë‚´ ë©”ì‹œì§€ ìƒì„±
    question = state['question']
    keywords = state.get('keyword', [])
    
    if isinstance(keywords, list) and len(keywords) > 1:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}"
    else:
        keyword_info = f"ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}"
    
    detailed_answer = f"""ğŸ” **ë¶„ì„ ê²°ê³¼ ìš”ì•½**

**ì…ë ¥ ì§ˆë¬¸**: {question}

**í‚¤ì›Œë“œ ì¦ê°•**: {keyword_info}

**ê²€ìƒ‰ ê²°ê³¼**: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ê°œì„  ì œì•ˆ**:
1. **ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±**í•´ì£¼ì„¸ìš”
   - ì˜ˆ: "ì„±ê³¼ ê°œì„ " â†’ "2024ë…„ 1ë¶„ê¸° ì˜ì—…íŒ€ ì„±ê³¼ ê°œì„  ë°©ì•ˆ"
   - ì˜ˆ: "ì „ëµ ìˆ˜ë¦½" â†’ "ì‹ ì œí’ˆ ì¶œì‹œë¥¼ ìœ„í•œ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½"

2. **ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ê°€**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ í‚¤ì›Œë“œ: {keywords[:3] if isinstance(keywords, list) else keywords}
   - ì¶”ê°€ í‚¤ì›Œë“œ ì˜ˆì‹œ: êµ¬ì²´ì ì¸ ì—…ë¬´ ì˜ì—­, ê¸°ê°„, ë¶€ì„œëª… ë“±

3. **ë°ì´í„°ë² ì´ìŠ¤ì— ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸**í•´ì£¼ì„¸ìš”
   - í˜„ì¬ ì„¤ì •ëœ ë²¡í„° DB: {QDRANT_HOST}:{QDRANT_PORT}
   - ì»¬ë ‰ì…˜: {QDRANT_COLLECTION or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}

ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤."""
    
    # Redisë¥¼ í†µí•´ ì™„ë£Œ ìƒíƒœ ë°œí–‰ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ DB ì €ì¥ì„ ìœ„í•´)
    complete_result = {
        "res_id": [], 
        "answer": detailed_answer,
        "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
        "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
    }
    
    # LangGraph ì‹¤í–‰ ê²°ê³¼ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•˜ë„ë¡ ë³€ê²½ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
    # await save_langgraph_result_to_db(state['question'], complete_result, state["keyword"], state["candidates_total"])
    
    generator_id = state.get('generator_id')
    if generator_id:
        await yield_node_status(
                generator_id,
                "E",
                "completed",
                {
                    "message": "ê¸°ë³¸ ë‹µë³€ ìƒì„± ë‹¨ê³„ ì™„ë£Œ",
                    "step": "E. ê¸°ë³¸ ë‹µë³€ ìƒì„± ì™„ë£Œ",
                    "answer": detailed_answer,
                    "keywords": state.get("keyword", []),
                    "reason": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                }
            )
    
    return {
        "question": state['question'],
        "keyword": state["keyword"],
        "candidates_total": state["candidates_total"],
        "response": {
            "res_id": [], 
            "answer": detailed_answer,
            "q_mode": "search",  # ìµœì´ˆ ì§ˆë¬¸ì€ í•­ìƒ search ëª¨ë“œ
            "keyword": state["keyword"],  # í‚¤ì›Œë“œ ì¦ê°• ëª©ë¡
        },
        "generator_id": state.get('generator_id')
    }

def judge_rc_ragscore(state: SearchState) -> str:
    """ë²¡í„° ê¸°ë°˜ RAG ì ìˆ˜ íŒë‹¨"""
    candidates_total = state["candidates_total"]
    
    if not candidates_total:
        print(f"[JUDGE] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ -> N")
        return "N"
    
    # ë²¡í„° ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸
    scores = [candidate.get("res_score", 0) for candidate in candidates_total]
    max_score = max(scores) if scores else 0
    avg_score = sum(scores) / len(scores) if scores else 0
    
    print(f"[JUDGE] ê²€ìƒ‰ ê²°ê³¼ {len(candidates_total)}ê±´")
    print(f"[JUDGE] ìµœê³  ë²¡í„° ì ìˆ˜: {max_score:.4f}, í‰ê· : {avg_score:.4f}")
    
    # ë²¡í„° ìœ ì‚¬ë„ ê¸°ì¤€ ì„ê³„ê°’ (ì¼ë°˜ì ìœ¼ë¡œ 0.7 ì´ìƒì´ ë†’ì€ ìœ ì‚¬ë„)
    if max_score >= 0.7:
        print(f"[JUDGE] ë†’ì€ ìœ ì‚¬ë„ -> Y")
        return "Y"
    elif max_score >= 0.5:
        print(f"[JUDGE] ì¤‘ê°„ ìœ ì‚¬ë„ -> Y")
        return "Y"
    elif max_score >= 0.3:
        print(f"[JUDGE] ë‚®ì€ ìœ ì‚¬ë„ -> Y")
        return "Y"
    else:
        print(f"[JUDGE] ë§¤ìš° ë‚®ì€ ìœ ì‚¬ë„ -> N")
        return "N"

async def save_langgraph_result_to_db_stream(question: str, result: dict, db: Session, user_id: int = 1):
    """LangGraph ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
    try:
        print(f"[DB_SAVE_STREAM] ğŸ’¾ LangGraph ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ DB ì €ì¥ ì‹œì‘")
        print(f"[DB_SAVE_STREAM] - ì§ˆë¬¸: {question}")
        print(f"[DB_SAVE_STREAM] - ì‚¬ìš©ì ID: {user_id}")
        
        # ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        response = result.get('response', {})
        keywords = result.get('keyword', [])
        candidates_total = result.get('candidates_total', [])
        
        if isinstance(response, dict):
            answer = response.get('answer', '')
            image_url = response.get('analysis_image_url')
        else:
            answer = str(response) if response else ''
            image_url = None
            
        print(f"[DB_SAVE_STREAM] - ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
        print(f"[DB_SAVE_STREAM] - í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
        print(f"[DB_SAVE_STREAM] - ë¬¸ì„œ: {len(candidates_total)}ê±´")
        
        # ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
        db_contents_list = []
        if candidates_total:
            for idx, candidate in enumerate(candidates_total[:5]):  # ìƒìœ„ 5ê°œë§Œ ì €ì¥
                payload = candidate.get('res_payload', {})
                vector_data = payload.get('vector', {})
                
                # image_url ì²˜ë¦¬ - Qdrant êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                image_url_value = ''
                if 'image_url' in payload:
                    # payload ìµœìƒìœ„ì— image_urlì´ ìˆëŠ” ê²½ìš°
                    img_url = payload.get('image_url', '')
                    if isinstance(img_url, list) and len(img_url) > 0:
                        image_url_value = img_url[0]  # ë°°ì—´ì˜ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                    elif isinstance(img_url, str):
                        image_url_value = img_url
                elif isinstance(vector_data, dict) and 'image_url' in vector_data:
                    # vector ë‚´ë¶€ì— image_urlì´ ìˆëŠ” ê²½ìš°
                    img_url = vector_data.get('image_url', '')
                    if isinstance(img_url, list) and len(img_url) > 0:
                        image_url_value = img_url[0]
                    elif isinstance(img_url, str):
                        image_url_value = img_url
                
                db_content = {
                    'rank': idx + 1,
                    'document_name': payload.get('document_name', ''),
                    'score': candidate.get('res_score', 0),
                    'combined_score': candidate.get('combined_score', candidate.get('res_score', 0)),
                    'relevance_score': candidate.get('res_relevance', 0),
                    'text': vector_data.get('text', '') if isinstance(vector_data, dict) else '',
                    'summary_purpose': vector_data.get('summary_purpose', '') if isinstance(vector_data, dict) else '',
                    'summary_result': vector_data.get('summary_result', '') if isinstance(vector_data, dict) else '',
                    'summary_fb': vector_data.get('summary_fb', '') if isinstance(vector_data, dict) else '',
                    'image_url': image_url_value,  # Qdrant êµ¬ì¡°ì— ë§ê²Œ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ URL
                    'res_id': candidate.get('res_id', '')
                }
                db_contents_list.append(db_content)
        
        db_contents_json = json.dumps(db_contents_list, ensure_ascii=False)
        print(f"[DB_SAVE_STREAM] - ê²€ìƒ‰ ê²°ê³¼ JSON ê¸¸ì´: {len(db_contents_json)}ì")
        
        # ìƒˆ ëŒ€í™” ìƒì„±
        from datetime import datetime
        title = question[:50] + "..." if len(question) > 50 else question
        conversation = Conversation(
            title=title,
            user_id=user_id,  # ì‹¤ì œ ì‚¬ìš©ì ID ì‚¬ìš©
            last_updated=datetime.utcnow()
        )
        db.add(conversation)
        db.commit()
        db.refresh(conversation)
        
        # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        from app.models import User
        user = db.query(User).filter(User.id == user_id).first()
        user_name = user.loginid if user and user.loginid else (user.username if user else "system")
        
        print(f"[DB_SAVE_STREAM] - ì‚¬ìš©ìëª…: {user_name}")
        
        # ë©”ì‹œì§€ ì €ì¥ (q_mode: 'search' - ë­ê·¸ë˜í”„ ì „ìš©)
        message = Message(
            conversation_id=conversation.id,
            role="user",
            question=question,
            ans=answer,
            q_mode='search',  # ë­ê·¸ë˜í”„ ì „ìš© ëª¨ë“œ
            keyword=str(keywords) if keywords else None,
            db_contents=db_contents_json,  # ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ ì •ë³´ ì €ì¥
            image=image_url,
            user_name=user_name  # ì‹¤ì œ ì‚¬ìš©ìëª…
        )
        
        db.add(message)
        db.commit()
        db.refresh(message)
        
        print(f"[DB_SAVE_STREAM] âœ… LangGraph ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ")
        print(f"[DB_SAVE_STREAM] - ëŒ€í™” ID: {conversation.id}")
        print(f"[DB_SAVE_STREAM] - ë©”ì‹œì§€ ID: {message.id}")
        
    except Exception as e:
        print(f"[DB_SAVE_STREAM] âŒ LangGraph ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"[DB_SAVE_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        if db:
            db.rollback()

async def save_langgraph_result_to_db(question: str, response: dict, keywords: list, candidates_total: list, image_url: str = None):
    """LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ DBì— ì§ì ‘ ì €ì¥ (ë­ê·¸ë˜í”„ ì „ìš©)"""
    try:
        print(f"[DB_SAVE] LangGraph ê²°ê³¼ DB ì €ì¥ ì‹œì‘ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ì§ˆë¬¸: {question}")
        print(f"[DB_SAVE] ì‘ë‹µ: {response.get('answer', '')[:100]}...")
        print(f"[DB_SAVE] í‚¤ì›Œë“œ: {keywords}")
        print(f"[DB_SAVE] ë¬¸ì„œ: {len(candidates_total)}ê±´")
        print(f"[DB_SAVE] ì´ë¯¸ì§€ URL: {image_url}")
        
        # ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
        db_contents_list = []
        if candidates_total:
            for idx, candidate in enumerate(candidates_total[:5]):  # ìƒìœ„ 5ê°œë§Œ ì €ì¥
                payload = candidate.get('res_payload', {})
                vector_data = payload.get('vector', {})
                
                # image_url ì²˜ë¦¬ - Qdrant êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                image_url_value = ''
                if 'image_url' in payload:
                    # payload ìµœìƒìœ„ì— image_urlì´ ìˆëŠ” ê²½ìš°
                    img_url = payload.get('image_url', '')
                    if isinstance(img_url, list) and len(img_url) > 0:
                        image_url_value = img_url[0]  # ë°°ì—´ì˜ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                    elif isinstance(img_url, str):
                        image_url_value = img_url
                elif isinstance(vector_data, dict) and 'image_url' in vector_data:
                    # vector ë‚´ë¶€ì— image_urlì´ ìˆëŠ” ê²½ìš°
                    img_url = vector_data.get('image_url', '')
                    if isinstance(img_url, list) and len(img_url) > 0:
                        image_url_value = img_url[0]
                    elif isinstance(img_url, str):
                        image_url_value = img_url
                
                db_content = {
                    'rank': idx + 1,
                    'document_name': payload.get('document_name', ''),
                    'score': candidate.get('res_score', 0),
                    'combined_score': candidate.get('combined_score', candidate.get('res_score', 0)),
                    'relevance_score': candidate.get('res_relevance', 0),
                    'text': vector_data.get('text', '') if isinstance(vector_data, dict) else '',
                    'summary_purpose': vector_data.get('summary_purpose', '') if isinstance(vector_data, dict) else '',
                    'summary_result': vector_data.get('summary_result', '') if isinstance(vector_data, dict) else '',
                    'summary_fb': vector_data.get('summary_fb', '') if isinstance(vector_data, dict) else '',
                    'image_url': image_url_value,  # Qdrant êµ¬ì¡°ì— ë§ê²Œ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ URL
                    'res_id': candidate.get('res_id', '')
                }
                db_contents_list.append(db_content)
        
        db_contents_json = json.dumps(db_contents_list, ensure_ascii=False)
        print(f"[DB_SAVE] - ê²€ìƒ‰ ê²°ê³¼ JSON ê¸¸ì´: {len(db_contents_json)}ì")
        
        # ìƒˆ ëŒ€í™” ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì°¾ê¸°
        db = next(get_db())
        
        # ìƒˆ ëŒ€í™” ìƒì„±
        from datetime import datetime
        title = question[:50] + "..." if len(question) > 50 else question
        conversation = Conversation(
            title=title,
            user_id=1,  # ê¸°ë³¸ ì‚¬ìš©ì ID (ì‹¤ì œë¡œëŠ” ì¸ì¦ëœ ì‚¬ìš©ì ID ì‚¬ìš©)
            last_updated=datetime.utcnow()
        )
        db.add(conversation)
        db.commit()
        db.refresh(conversation)
        
        # ë©”ì‹œì§€ ì €ì¥ (q_mode: 'search' - ë­ê·¸ë˜í”„ ì „ìš©)
        message = Message(
            conversation_id=conversation.id,
            role="user",
            question=question,
            ans=response.get('answer', ''),
            q_mode='search',  # ë­ê·¸ë˜í”„ ì „ìš© ëª¨ë“œ
            keyword=str(keywords) if keywords else None,
            db_contents=db_contents_json,  # ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ ì •ë³´ ì €ì¥
            image=image_url,  # ì´ë¯¸ì§€ URL ì¶”ê°€
            user_name="system"  # ê¸°ë³¸ ì‚¬ìš©ìëª…
        )
        
        db.add(message)
        db.commit()
        
        print(f"[DB_SAVE] âœ… LangGraph ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (ë­ê·¸ë˜í”„ ì „ìš©)")
        print(f"[DB_SAVE] ëŒ€í™” ID: {conversation.id}")
        print(f"[DB_SAVE] ë©”ì‹œì§€ ID: {message.id}")
        print(f"[DB_SAVE] q_mode: {message.q_mode} (ë­ê·¸ë˜í”„ ì „ìš©)")
        
    except Exception as e:
        print(f"[DB_SAVE] âŒ LangGraph ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"[DB_SAVE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")


# LangGraph êµ¬ì„±
def create_langgraph():
    """LangGraph ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("node_rc_init", node_rc_init)
    workflow.add_node("node_rc_keyword", node_rc_keyword)
    workflow.add_node("node_rc_rag", node_rc_rag)
    workflow.add_node("node_rc_rerank", node_rc_rerank)
    workflow.add_node("node_rc_answer", node_rc_answer)
    workflow.add_node("node_rc_plain_answer", node_rc_plain_answer)
    
    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("node_rc_init")
    workflow.add_edge("node_rc_init", "node_rc_keyword")
    workflow.add_edge("node_rc_keyword", "node_rc_rag")
    workflow.add_conditional_edges(
        "node_rc_rag",
        judge_rc_ragscore,
        {
            "Y": "node_rc_rerank",
            "N": "node_rc_plain_answer"
        }
    )
    workflow.add_edge("node_rc_rerank", "node_rc_answer")
    workflow.add_edge("node_rc_answer", END)
    workflow.add_edge("node_rc_plain_answer", END)
    
    return workflow.compile()

# LangGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
try:
    langgraph_instance = create_langgraph()
    print("[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"[LangGraph] ì›Œí¬í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨: {e}")
    langgraph_instance = None

# ê°„ë‹¨í•œ LLM ì‘ë‹µ í•¨ìˆ˜ (conversations.pyì—ì„œ ì‚¬ìš©)
async def get_llm_response(question: str) -> str:
    """ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    try:
        
        print(f"[LLM_RESPONSE] ì§ˆë¬¸: {question}")
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": question}
        ]
        
        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        httpx_client = httpx.AsyncClient(verify=False, timeout=None)

        # print(f"[messages í™•ì¸] {messages}")              
        
        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "c.seunghoon",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                stream=True,
            )
        
        async for chunk in response:              # ì´ì œ chunkëŠ” OpenAIObject
                    delta = chunk.choices[0].delta
                    content = delta.content
                    # print(content)
                # for chunk in response:
                #     if chunk.choices[0].delta.get("content"):
                #         content = chunk.choices[0].delta.content
                    try:
                        # ë¹„-ASCII ë¬¸ì í—ˆìš©, UTF-8 bytes ë¡œ ì¦‰ì‹œ ì „ì†¡
                        payload = json.dumps({'content': content}, ensure_ascii=False)
                        yield (f"data: {payload}\n\n").encode("utf-8")

                        # ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
                    except (ConnectionResetError, BrokenPipeError, OSError, ConnectionAbortedError, ConnectionError) as e:
                        # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
                        print(f"Client disconnected during streaming lv2: {type(e).__name__}")
                        return
                    except Exception as e:
                        print(f"Unexpected error during streaming lv1: {str(e)}")
                        return
        
    except Exception as e:
        error_message = str(e)
        print(f"[LLM_RESPONSE] ì˜¤ë¥˜: {error_message}")
        
        # ì—°ê²° ì˜¤ë¥˜ íƒ€ì…ë³„ ì²˜ë¦¬
        if "APIConnectionError" in error_message or "Connection error" in error_message:
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "timeout" in error_message.lower():
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "authentication" in error_message.lower() or "unauthorized" in error_message.lower():
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
        else:
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."




# ì¼ë°˜ LLM ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (streaming ì§€ì›)
@router.post("/chat/stream")
async def stream_chat_with_llm(request: StreamRequest, http_request: Request, db: Session = Depends(get_db)):
    """Stream a response from general LLM chat using async method"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[Chat Stream] ========== LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‹œì‘ ==========")
        print(f"[Chat Stream] ìš”ì²­ ì •ë³´:")
        print(f"[Chat Stream] - ì§ˆë¬¸: {request.question}")
        print(f"[Chat Stream] - ëŒ€í™” ID: {request.conversation_id}")
        print(f"[Chat Stream] - conversation_id íƒ€ì…: {type(request.conversation_id)}")
        print(f"[Chat Stream] - conversation_idê°€ Noneì¸ê°€?: {request.conversation_id is None}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        system_content = "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        # ë­ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
        if request.include_langgraph_context and request.langgraph_context:
            langgraph_context = request.langgraph_context
            print(f"[Chat Stream] ë­ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨: {langgraph_context}")
            
            # ë¬¸ì„œ ì •ë³´ë¥¼ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
            if langgraph_context.get('documents'):
                documents_info = f"\n\nì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œ ì •ë³´:\n"
                for i, doc in enumerate(langgraph_context['documents'][:5], 1):  # ìµœëŒ€ 5ê°œ ë¬¸ì„œë§Œ
                    documents_info += f"{i}. {doc.get('title', 'ì œëª© ì—†ìŒ')}\n"
                    if doc.get('content'):
                        # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°
                        content = doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content']
                        documents_info += f"   ë‚´ìš©: {content}\n"
                    documents_info += "\n"
                
                system_content += documents_info
                system_content += "\nìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  'ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        messages = [{"role": "system", "content": system_content}]
        
        if request.conversation_id:
            try:
                # í•´ë‹¹ ëŒ€í™”ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 10ê°œë§Œ)
                conversation_messages = db.query(Message).filter(
                    Message.conversation_id == request.conversation_id
                ).order_by(Message.created_at.asc()).limit(10).all()
                
                print(f"[Chat Stream] ì´ì „ ëŒ€í™” ë©”ì‹œì§€ {len(conversation_messages)}ê°œ ë¡œë“œ")
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ==========")
                
                # ì´ì „ ëŒ€í™”ë¥¼ messagesì— ì¶”ê°€
                for i, msg in enumerate(conversation_messages):
                    print(f"[Chat Stream] DB ë©”ì‹œì§€ {i+1}: ID={msg.id}, role={msg.role}, created_at={msg.created_at}")
                    if msg.question:
                        print(f"[Chat Stream] ì§ˆë¬¸: {msg.question}")
                        messages.append({"role": "user", "content": msg.question})
                    if msg.ans:
                        print(f"[Chat Stream] ë‹µë³€: {msg.ans}")
                        messages.append({"role": "assistant", "content": msg.ans})
                    print(f"[Chat Stream] ----------------------------------------")
                
                print(f"[Chat Stream] ========== ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ ==========")
                        
            except Exception as e:
                print(f"[Chat Stream] ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[Chat Stream] ì „ì†¡í•  ë©”ì‹œì§€ ê°œìˆ˜: {len(messages)}")
        print(f"[Chat Stream] ========== OpenAI APIì— ì „ì†¡í•  ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ==========")
        print(f"[Chat Stream] ==========ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© : ")
        print(f"{messages}")
        
        print(f"[Chat Stream] ========== ì „ì²´ ë©”ì‹œì§€ ë‚´ìš© ë ==========")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš© - DB ì €ì¥ í¬í•¨
        return StreamingResponse(
            get_streaming_response_with_db_save(messages, http_request, request, db),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[Chat Stream] LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[Chat Stream] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")



# ì§ˆë¬¸ ìœ í˜• íŒë³„ í•¨ìˆ˜
def is_first_question_in_conversation(conversation_id: int, db: Session) -> bool:
    """ëŒ€í™”ì—ì„œ ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
    try:
        message_count = db.query(Message).filter(Message.conversation_id == conversation_id).count()
        print(f"[QUESTION_TYPE] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ ìˆ˜: {message_count}")
        return message_count == 0
    except Exception as e:
        print(f"[QUESTION_TYPE] ë©”ì‹œì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ ì‹œ ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼

def get_conversation_context(conversation_id: int, db: Session) -> dict:
    """ëŒ€í™”ì˜ ì»¨í…ìŠ¤íŠ¸ì™€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # í•´ë‹¹ ëŒ€í™”ì˜ ëª¨ë“  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ìˆœ ì •ë ¬)
        messages = db.query(Message).filter(
            Message.conversation_id == conversation_id
        ).order_by(Message.created_at.asc()).all()
        
        print(f"[CONTEXT] ëŒ€í™” ID {conversation_id}ì˜ ë©”ì‹œì§€ {len(messages)}ê°œ ë¡œë“œ")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´
        if len(messages) > 0:
            print(f"[CONTEXT] ë©”ì‹œì§€ ìƒì„¸:")
            for i, msg in enumerate(messages):
                print(f"[CONTEXT]   {i+1}. ID: {msg.id}, q_mode: {msg.q_mode}, role: {msg.role}, question: {msg.question[:50] if msg.question else 'None'}...")
        else:
            print(f"[CONTEXT] âš ï¸ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€í™” ID {conversation_id} í™•ì¸ í•„ìš”")
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ì°¾ê¸° (q_modeê°€ "search"ì¸ ë©”ì‹œì§€)
        first_message = None
        for msg in messages:
            if msg.q_mode == "search":
                first_message = msg
                print(f"[CONTEXT] ì²« ë²ˆì§¸ ì§ˆë¬¸ ë°œê²¬: ë©”ì‹œì§€ ID {msg.id}")
                break
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation_history = []
        for msg in messages:
            if msg.question:
                conversation_history.append({"role": "user", "content": msg.question})
            if msg.ans:
                conversation_history.append({"role": "assistant", "content": msg.ans})
        
        return {
            "first_message": first_message,
            "conversation_history": conversation_history,
            "message_count": len(messages)
        }
        
    except Exception as e:
        print(f"[CONTEXT] ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return {
            "first_message": None,
            "conversation_history": [],
            "message_count": 0
        }

# SSE ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©)
@router.post("/langgraph/stream")
async def execute_langgraph_stream(request: StreamRequest, db: Session = Depends(get_db), current_user: User = Depends(get_current_user)):
    """LangGraph SSE ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    
    async def generate_sse():
        generator_id = str(uuid.uuid4())
        print(f"[SSE] ğŸ†” ìƒˆ ì œë„ˆë ˆì´í„° ìƒì„±: {generator_id}")
        generator = SSEGenerator(generator_id)
        sse_generators[generator_id] = generator
        print(f"[SSE] ğŸ“‹ í˜„ì¬ í™œì„± ì œë„ˆë ˆì´í„° ìˆ˜: {len(sse_generators)}")
        
        try:
            # OpenAI API í‚¤ í™•ì¸
            if not OPENAI_API_KEY:
                yield f"data: {json.dumps({'error': 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'})}\n\n"
                return
            
            print(f"[SSE] ğŸš€ LangGraph SSE ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {request.question}")
            
            # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸
            if request.conversation_id:
                is_first = is_first_question_in_conversation(request.conversation_id, db)
                if not is_first:
                    yield f"data: {json.dumps({'error': 'ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”'})}\n\n"
                    return
            
            # ì›Œí¬í”Œë¡œìš° í™•ì¸
            if langgraph_instance is None:
                yield f"data: {json.dumps({'error': 'LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'})}\n\n"
                return
            
            # ì´ˆê¸° ìƒíƒœì— generator_id ì¶”ê°€
            initial_state = {
                "question": request.question,
                "keyword": "",
                "candidates_each": [],
                "candidates_total": [],
                "response": [],
                "generator_id": generator_id
            }
            print(f"[SSE] ğŸ“‹ ì´ˆê¸° ìƒíƒœ ì„¤ì •: generator_id={generator_id}")
            
            print(f"[SSE] LangGraph ì‹¤í–‰ ì‹œì‘: {request.question}")
            
            # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë¨¼ì € ì „ì†¡
            test_message = {
                "stage": "TEST",
                "status": "started", 
                "result": {"message": "SSE ì—°ê²° í…ŒìŠ¤íŠ¸"}
            }
            print(f"[SSE] ğŸ§ª í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡")
            await generator.send_message(test_message)
            
            # LangGraph ì‹¤í–‰ì„ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            async def run_langgraph():
                try:
                    print(f"[LANGGRAPH] ğŸš€ LangGraph ì‹¤í–‰ ì‹œì‘ (generator_id: {generator_id})")
                    print(f"[LANGGRAPH] ğŸ“‹ ì´ˆê¸° ìƒíƒœ: {initial_state}")
                    
                    # LangGraph ì‹¤í–‰ ì „ ì œë„ˆë ˆì´í„° ìƒíƒœ í™•ì¸
                    print(f"[LANGGRAPH] ğŸ” ì‹¤í–‰ ì „ ì œë„ˆë ˆì´í„° ìƒíƒœ: active={generator.is_active}, queue_size={generator.message_queue.qsize()}")
                    
                    result = await langgraph_instance.ainvoke(initial_state)
                    
                    print(f"[LANGGRAPH] âœ… LangGraph ì‹¤í–‰ ì™„ë£Œ")
                    print(f"[LANGGRAPH] ğŸ“Š ê²°ê³¼ ìš”ì•½: keyword={len(result.get('keyword', []))}ê°œ, candidates={len(result.get('candidates_total', []))}ê°œ")
                    print(f"[LANGGRAPH] ğŸ” ì‹¤í–‰ í›„ ì œë„ˆë ˆì´í„° ìƒíƒœ: active={generator.is_active}, queue_size={generator.message_queue.qsize()}")
                    
                    # LangGraph ê²°ê³¼ë¥¼ DBì— ì €ì¥
                    try:
                        print(f"[LANGGRAPH] ğŸ’¾ LangGraph ê²°ê³¼ DB ì €ì¥ ì‹œì‘")
                        await save_langgraph_result_to_db_stream(
                            initial_state["question"], 
                            result, 
                            db,
                            current_user.id  # ì‹¤ì œ ì‚¬ìš©ì ID ì „ë‹¬
                        )
                        print(f"[LANGGRAPH] âœ… LangGraph ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ")
                    except Exception as e:
                        print(f"[LANGGRAPH] âŒ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                        # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    
                    # DONE ë©”ì‹œì§€ì— ì „ì²´ LangGraph ê²°ê³¼ í¬í•¨
                    done_message = {
                        "stage": "DONE", 
                        "result": result,  # ì „ì²´ LangGraph ê²°ê³¼ í¬í•¨
                        "keyword": result.get('keyword', []),
                        "candidates_total": result.get('candidates_total', [])
                    }
                    print(f"[LANGGRAPH] ğŸ“¤ DONE ë©”ì‹œì§€ ì „ì†¡ ì‹œë„ (í í¬ê¸°: {generator.message_queue.qsize()})")
                    await generator.send_message(done_message)
                    print(f"[LANGGRAPH] âœ… DONE ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ (í í¬ê¸°: {generator.message_queue.qsize()})")
                    
                except Exception as e:
                    print(f"[LANGGRAPH] âŒ LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                    import traceback
                    print(f"[LANGGRAPH] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    error_message = {"stage": "ERROR", "error": str(e)}
                    print(f"[LANGGRAPH] ğŸ“¤ ERROR ë©”ì‹œì§€ ì „ì†¡ ì‹œë„")
                    await generator.send_message(error_message)
                finally:
                    print(f"[LANGGRAPH] ğŸ”š ì œë„ˆë ˆì´í„° ì¢…ë£Œ ì‹œì‘")
                    await generator.close()
                    print(f"[LANGGRAPH] âœ… ì œë„ˆë ˆì´í„° ì¢…ë£Œ ì™„ë£Œ")
            
            # LangGraph ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
            langgraph_task = asyncio.create_task(run_langgraph())
            print(f"[SSE] ğŸš€ LangGraph íƒœìŠ¤í¬ ì‹œì‘ë¨")
            
            # ì§§ì€ ì§€ì—° í›„ SSE ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (LangGraphê°€ ì‹œì‘í•  ì‹œê°„ì„ ì¤Œ)
            await asyncio.sleep(0.01)  # 0.1ì´ˆ â†’ 0.01ì´ˆë¡œ ë‹¨ì¶•
            
            # SSE ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
            print(f"[SSE] ğŸ“¡ SSE ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì‹œì‘")
            message_count = 0
            heartbeat_count = 0
            
            # LangGraph íƒœìŠ¤í¬ê°€ ì‹¤í–‰ ì¤‘ì´ê±°ë‚˜ ì œë„ˆë ˆì´í„°ê°€ í™œì„±í™”ëœ ë™ì•ˆ ê³„ì† ì‹¤í–‰
            while generator.is_active or not langgraph_task.done():
                try:
                    # í˜„ì¬ ìƒíƒœ ë¡œê¹…
                    current_queue_size = generator.message_queue.qsize()
                    task_done = langgraph_task.done()
                    # print(f"[SSE] ğŸ”„ ë£¨í”„ ìƒíƒœ: generator_active={generator.is_active}, task_done={task_done}, queue_size={current_queue_size}")
                    
                    # íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë ¤ì„œ ë©”ì‹œì§€ë¥¼ ë†“ì¹˜ì§€ ì•Šë„ë¡ í•¨
                    message = await asyncio.wait_for(generator.message_queue.get(), timeout=1.0)
                    
                    if message is None:  # ì¢…ë£Œ ì‹ í˜¸
                        print(f"[SSE] ğŸ”š ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                        break
                    
                    message_count += 1
                    # print(f"[SSE] ğŸ“¨ ë©”ì‹œì§€ #{message_count} ìˆ˜ì‹ : {message.get('stage', 'unknown')}:{message.get('status', 'unknown')}")
                    # print(f"[SSE] ğŸ“‹ ë©”ì‹œì§€ ë‚´ìš©: {json.dumps(message, ensure_ascii=False)[:200]}...")
                    
                    # SSE í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡
                    sse_data = f"data: {json.dumps(message, ensure_ascii=False)}\n\n"
                    # print(f"[SSE] ğŸ“¤ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡: {len(sse_data)} bytes")
                    yield sse_data
                    # print(f"[SSE] âœ… í´ë¼ì´ì–¸íŠ¸ ì „ì†¡ ì™„ë£Œ")
                    
                    # ë©”ì‹œì§€ ì „ì†¡ í›„ ì§§ì€ ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
                    
                except asyncio.TimeoutError:
                    # íƒ€ì„ì•„ì›ƒ ì‹œ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡ (10ë²ˆë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ë¡œê·¸)
                    heartbeat_count += 1
                    current_queue_size = generator.message_queue.qsize()
                    task_done = langgraph_task.done()
                    
                    if heartbeat_count % 10 == 0:
                        print(f"[SSE] ğŸ’“ í•˜íŠ¸ë¹„íŠ¸ #{heartbeat_count} - ë©”ì‹œì§€:{message_count}, í:{current_queue_size}, íƒœìŠ¤í¬ì™„ë£Œ:{task_done}")
                    
                    yield f"data: {json.dumps({'heartbeat': True, 'count': heartbeat_count, 'queue_size': current_queue_size, 'task_done': task_done})}\n\n"
                    continue
                except Exception as e:
                    print(f"[SSE] âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    import traceback
                    print(f"[SSE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    break
            
            print(f"[SSE] ğŸ“Š ì´ {message_count}ê°œ ë©”ì‹œì§€, {heartbeat_count}ê°œ í•˜íŠ¸ë¹„íŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            yield f"data: [DONE]\n\n"
            
        except Exception as e:
            print(f"[SSE] ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        finally:
            # ì •ë¦¬
            if generator_id in sse_generators:
                del sse_generators[generator_id]
            print(f"[SSE] ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ: {generator_id}")
    
    return StreamingResponse(
        generate_sse(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

# LangGraph ì§ì ‘ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ (ì²« ë²ˆì§¸ ì§ˆë¬¸ìš©) - ê¸°ì¡´ ìœ ì§€
@router.post("/langgraph")
async def execute_langgraph(request: StreamRequest, db: Session = Depends(get_db)):
    """LangGraphë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë°˜í™˜ (ì²« ë²ˆì§¸ ì§ˆë¬¸ ì „ìš©)"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"[LangGraph] ğŸš€ ë­ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” IDê°€ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ ìœ í˜• í™•ì¸
        if request.conversation_id:
            is_first = is_first_question_in_conversation(request.conversation_id, db)
            if not is_first:
                print(f"[LangGraph] âš ï¸ ì¶”ê°€ ì§ˆë¬¸ ê°ì§€ë¨ - LangGraph ì‹¤í–‰ ì°¨ë‹¨")
                raise HTTPException(
                    status_code=400, 
                    detail="ì¶”ê°€ ì§ˆë¬¸ì€ /langgraph/followup ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
                )
        
        # ì›Œí¬í”Œë¡œìš° í™•ì¸
        if langgraph_instance is None:
            raise HTTPException(status_code=500, detail="LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        initial_state = {"question": request.question}
        print(f"[LangGraph] ì‹¤í–‰ ì‹œì‘: {request.question}")
        print(f"[LangGraph] ì´ˆê¸° ìƒíƒœ: {initial_state}")
        
        result = await langgraph_instance.ainvoke(initial_state)
        
        print(f"[LangGraph] âœ… ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ì—ì„œ íƒœê·¸ì™€ ë¬¸ì„œ íƒ€ì´í‹€ ì¶”ì¶œ
        tags = None
        
        if isinstance(result, dict):
            # í‚¤ì›Œë“œ ì •ë³´ì—ì„œ íƒœê·¸ ì¶”ì¶œ
            if 'keyword' in result and result['keyword']:
                if isinstance(result['keyword'], list):
                    tags = ', '.join(result['keyword'])
                else:
                    tags = str(result['keyword'])
                print(f"[LangGraph] í‚¤ì›Œë“œ: {len(result['keyword'])}ê°œ")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
            if 'candidates_total' in result and result['candidates_total']:
                print(f"[LangGraph] ë¬¸ì„œ: {len(result['candidates_total'])}ê±´")
            
            # ì‘ë‹µ ì •ë³´ í™•ì¸
            if 'response' in result and result['response']:
                response_text = result['response'].get('answer', '')[:50] if isinstance(result['response'], dict) else str(result['response'])[:50]
                print(f"[LangGraph] ì‘ë‹µ: {response_text}...")
        
        print(f"[LangGraph] ìš”ì•½: í‚¤ì›Œë“œ {len(result.get('keyword', []))}ê°œ, ë¬¸ì„œ {len(result.get('candidates_total', []))}ê±´")
        
        return {
            "status": "success",
            "result": result,
            "tags": tags,
            "message": "LangGraph ì‹¤í–‰ ì™„ë£Œ (ì²« ë²ˆì§¸ ì§ˆë¬¸)"
        }
        
    except Exception as e:
        print(f"[LangGraph] ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[LangGraph] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")


# ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@router.post("/langgraph/followup/stream")
async def execute_followup_question_stream(request: StreamRequest, http_request: Request, db: Session = Depends(get_db), current_user: User = Depends(get_current_user)):
    """ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ - ê¸°ì¡´ RAG ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œìš©"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            return Response(content="Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", media_type="text/plain")
        
        print(f"[FOLLOWUP_STREAM] ğŸ”„ LLM ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘: {request.question}")
        
        # ëŒ€í™” ID í™•ì¸
        if not request.conversation_id:
            return Response(content="Error: ì¶”ê°€ ì§ˆë¬¸ì€ conversation_idê°€ í•„ìš”í•©ë‹ˆë‹¤", media_type="text/plain")
        
        # ê¸°ì¡´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì—ì„œ RAG ì •ë³´ ì¶”ì¶œ
        context = get_conversation_context(request.conversation_id, db)
        
        if not context["first_message"]:
            print(f"[FOLLOWUP_STREAM] âš ï¸ ì²« ë²ˆì§¸ ì§ˆë¬¸ ì—†ìŒ")
            return Response(content="Error: ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", media_type="text/plain")
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ RAG ê²€ìƒ‰ ê²°ê³¼ í™œìš©
        first_message = context["first_message"]
        
        # DBì—ì„œ ì²« ë²ˆì§¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ RAG ê²€ìƒ‰ ê²°ê³¼ ì¬êµ¬ì„±
        print(f"[FOLLOWUP_STREAM] ğŸ“„ ì²« ë²ˆì§¸ ì§ˆë¬¸ RAG ì •ë³´:")
        print(f"[FOLLOWUP_STREAM]   ì§ˆë¬¸: {first_message.question}")
        print(f"[FOLLOWUP_STREAM]   í‚¤ì›Œë“œ: {first_message.keyword}")
        print(f"[FOLLOWUP_STREAM]   ê²€ìƒ‰ ë¬¸ì„œ: {first_message.db_contents}")
        
        # ì‹¤ì œ RAG ë¬¸ì„œ ë‚´ìš© êµ¬ì„± (DBì—ì„œ ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ ì¬ì‚¬ìš©)
        document_title = "ê²€ìƒ‰ëœ ë¬¸ì„œ"
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•˜ì—¬ ì‹¤ì œ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        try:
            # ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œ íŒŒì‹±
            keywords_str = first_message.keyword or first_message.question
            if keywords_str and keywords_str != 'None':
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ë©´ íŒŒì‹±)
                if keywords_str.startswith('[') and keywords_str.endswith(']'):
                    import ast
                    keywords = ast.literal_eval(keywords_str)
                else:
                    keywords = [keywords_str]
                
                # ì‹¤ì œ RAG ê²€ìƒ‰ ìˆ˜í–‰
                rag_results = await direct_document_search('followup', 1, keywords[:3], QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION)
                
                if rag_results and len(rag_results) > 0:
                    top_doc = rag_results[0]['res_payload']
                    actual_document_content = ""
                    
                    # ì‹¤ì œ ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
                    if 'vector' in top_doc and isinstance(top_doc['vector'], dict):
                        vector_data = top_doc['vector']
                        for field in ['text', 'summary_purpose', 'summary_result', 'summary_fb']:
                            if field in vector_data and vector_data[field]:
                                actual_document_content += f"{vector_data[field]} "
                    
                    actual_document_content = actual_document_content.strip() or "ë¬¸ì„œ ë‚´ìš© ì—†ìŒ"
                else:
                    actual_document_content = "ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            else:
                actual_document_content = "í‚¤ì›Œë“œ ì •ë³´ ì—†ìŒ"
                
        except Exception as e:
            print(f"[FOLLOWUP_STREAM] RAG ì¬ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            actual_document_content = "ë¬¸ì„œ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        document_content = f"""
[ì›ë³¸ ì§ˆë¬¸] {first_message.question}
[ê²€ìƒ‰ëœ ë¬¸ì„œ] {document_title}
[ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©] {actual_document_content}
[ì´ì „ ë‹µë³€] {first_message.ans}
"""
        
        print(f"[FOLLOWUP_STREAM] ğŸ“„ ì¬ì‚¬ìš©í•  RAG ë¬¸ì„œ:")
        print(f"[FOLLOWUP_STREAM] ì œëª©: {document_title}")
        print(f"[FOLLOWUP_STREAM] ë‚´ìš©: {actual_document_content}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± (ì²« ë²ˆì§¸ ì§ˆë¬¸ê³¼ ë‹µë³€ë§Œ í¬í•¨)
        conversation_history = context["conversation_history"]
        print(f"[FOLLOWUP_STREAM] ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (RAG ë¬¸ì„œ ê¸°ë°˜)
        system_prompt = f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

{document_content}

ì¤‘ìš”í•œ ê·œì¹™:
1. ìœ„ì— ì œê³µëœ ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì¶”ê°€ì ì¸ ì •ë³´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”
5. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”"""
        
        # LLM API í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ)
        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
        messages.extend(recent_history)
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": request.question})
        
        print(f"[FOLLOWUP_STREAM] ğŸ“¤ LLMì— ì „ì†¡í•  ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        print(f"[FOLLOWUP_STREAM] ğŸ“ í˜„ì¬ ì§ˆë¬¸: {request.question}")
        print(f"[FOLLOWUP_STREAM] ğŸ‘¤ ì‚¬ìš©ì: {current_user.username if current_user else 'None'}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš© - DB ì €ì¥ í¬í•¨
        return StreamingResponse(
            get_streaming_response_with_db_save(messages, http_request, request, db, current_user),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization",
            }
        )
        
    except Exception as e:
        print(f"[FOLLOWUP_STREAM] LLM ì¶”ê°€ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[FOLLOWUP_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return Response(content=f"Error: {str(e)}", media_type="text/plain")

async def save_message_to_db(stream_request: StreamRequest, assistant_response: str, image_url: str = None, db: Session = None, current_user: User = None):
    """ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ë©”ì‹œì§€ë¥¼ DBì— ì €ì¥"""
    try:
        if not db:
            print(f"[DB_SAVE] âŒ DB ì„¸ì…˜ì´ ì—†ìŒ")
            return
            
        if not stream_request.conversation_id:
            print(f"[DB_SAVE] âŒ conversation_idê°€ ì—†ìŒ - ì¶”ê°€ ì§ˆë¬¸ì€ conversation_idê°€ í•„ìˆ˜ì…ë‹ˆë‹¤")
            return
            
        print(f"[DB_SAVE] ğŸ’¾ ë©”ì‹œì§€ DB ì €ì¥ ì‹œì‘")
        print(f"[DB_SAVE] - ëŒ€í™” ID: {stream_request.conversation_id}")
        print(f"[DB_SAVE] - ì§ˆë¬¸: {stream_request.question}")
        print(f"[DB_SAVE] - ë‹µë³€ ê¸¸ì´: {len(assistant_response)}ì")
        print(f"[DB_SAVE] - ì‚¬ìš©ì: {current_user.username if current_user else 'None'}")
        
        # ëŒ€í™” ì¡´ì¬ í™•ì¸
        conversation = db.query(Conversation).filter(
            Conversation.id == stream_request.conversation_id,
            Conversation.is_deleted == False
        ).first()
        
        if not conversation:
            print(f"[DB_SAVE] âŒ ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {stream_request.conversation_id}")
            return
        
        print(f"[DB_SAVE] âœ… ê¸°ì¡´ ëŒ€í™” ì°¾ìŒ - ìƒˆ conversation ìƒì„±í•˜ì§€ ì•ŠìŒ")
            
        # ì‚¬ìš©ì ì •ë³´ í™•ì¸ (current_user ìš°ì„ , ì—†ìœ¼ë©´ conversation.user)
        user_name = "system"  # ê¸°ë³¸ê°’
        if current_user:
            user_name = current_user.loginid or current_user.username or "system"
            print(f"[DB_SAVE] - current_userì—ì„œ ì‚¬ìš©ìëª… ê°€ì ¸ì˜´: {user_name}")
        elif conversation.user:
            user_name = conversation.user.loginid or conversation.user.username or "system"
            print(f"[DB_SAVE] - conversation.userì—ì„œ ì‚¬ìš©ìëª… ê°€ì ¸ì˜´: {user_name}")
        
        # ì¤‘ë³µ ì €ì¥ ë°©ì§€ - ë™ì¼í•œ ì§ˆë¬¸ì˜ ìµœê·¼ ë©”ì‹œì§€ í™•ì¸ (30ì´ˆ ì´ë‚´)
        from datetime import datetime, timedelta
        recent_time = datetime.utcnow() - timedelta(seconds=30)
        
        existing_message = db.query(Message).filter(
            Message.conversation_id == stream_request.conversation_id,
            Message.question == stream_request.question,
            Message.created_at >= recent_time
        ).first()
        
        if existing_message:
            print(f"[DB_SAVE] âš ï¸ ì¤‘ë³µ ë©”ì‹œì§€ ê°ì§€ë¨. ê¸°ì¡´ ë©”ì‹œì§€ ID: {existing_message.id}")
            return
        
        # ë©”ì‹œì§€ ìƒì„± ë° ì €ì¥
        # q_modeëŠ” conversation_idê°€ ìˆëŠ” ê²½ìš° 'add', ì—†ëŠ” ê²½ìš° None
        # conversation_idê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ëŒ€í™”ì— ì¶”ê°€ ì§ˆë¬¸ì´ë¯€ë¡œ 'add'
        # conversation_idê°€ ì—†ìœ¼ë©´ ìƒˆ ëŒ€í™”ì´ë¯€ë¡œ None (ì²« ë²ˆì§¸ ì§ˆë¬¸)
        q_mode_value = "add" if stream_request.conversation_id else None
        
        print(f"[DB_SAVE] - q_mode: {q_mode_value} (conversation_id: {stream_request.conversation_id})")
        
        message = Message(
            conversation_id=stream_request.conversation_id,
            role="user",
            question=stream_request.question,
            ans=assistant_response,
            user_name=user_name,
            q_mode=q_mode_value,  # conversation_idê°€ ìˆìœ¼ë©´ 'add', ì—†ìœ¼ë©´ None
            image=image_url
        )
        
        db.add(message)
        
        # ëŒ€í™”ì˜ last_updated ì‹œê°„ ì—…ë°ì´íŠ¸
        conversation.last_updated = datetime.utcnow()
        
        # ëŒ€í™”ì— ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¸ ê²½ìš° íƒ€ì´í‹€ ì„¤ì • (ì¶”ê°€ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
        if (not conversation.title or conversation.title == "New Conversation") and stream_request.q_mode != "add":
            title = stream_request.question[:50]
            if len(stream_request.question) > 50:
                title += "..."
            conversation.title = title
            print(f"[DB_SAVE] ğŸ“ ëŒ€í™” íƒ€ì´í‹€ ì„¤ì •: {title}")
        
        db.commit()
        db.refresh(message)
        db.refresh(conversation)
        
        print(f"[DB_SAVE] âœ… ë©”ì‹œì§€ ì €ì¥ ì™„ë£Œ. ID: {message.id}")
        
    except Exception as e:
        print(f"[DB_SAVE] âŒ DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[DB_SAVE] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        if db:
            db.rollback()

async def generate_image(prompt: str) -> str:
    """Generate an image using OpenAI DALL-E API"""
    try:
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ë¶€ë¶„ì—ì„œ OpenAI DALL-E API í˜¸ì¶œ
        # ì˜ˆì‹œ: 
        # client = OpenAI(api_key=OPENAI_API_KEY)
        # response = client.images.generate(prompt=prompt, n=1, size="1024x1024")
        # image_url = response.data[0].url
        
        # ì´ë¯¸ì§€ URL ë°˜í™˜ (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬)
        return None
    except Exception as e:
        print(f"Error generating image: {str(e)}")
        return None

async def get_streaming_response_with_db_save(messages: List[Dict], request: Request, stream_request: StreamRequest, db: Session, current_user: User = None):
    """Stream a response from LLM using AsyncOpenAI with custom headers and save to DB"""
    try:
        print(f"[LLM_STREAM_DB] ğŸš€ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (DB ì €ì¥ í¬í•¨)")
        
        # ì´ë¯¸ì§€ URL (ì´ë¯¸ì§€ ìƒì„±ì´ ìš”ì²­ëœ ê²½ìš°)
        image_url = None
        if stream_request.generate_image:
            # ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ë¡œì§ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            image_url = await generate_image(messages[-1]["content"] if messages else "")
        
        # API í‚¤ ê²€ì¦
        if not IS_OPENAI_CONFIGURED:
            error_payload = json.dumps({'error': 'AI ì„œë¹„ìŠ¤ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.'}, ensure_ascii=False)
            yield (f"data: {error_payload}\n\n").encode("utf-8")
            yield "data: [DONE]\n\n".encode("utf-8")
            return

        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)
        httpx_client = httpx.AsyncClient(
            verify=False, 
            timeout=httpx.Timeout(30.0, connect=10.0)
        )

        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "c.seunghoon",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                stream=True,
            )
        
        print(f"[LLM_STREAM_DB] ğŸ“¥ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
        
        text_response = ""
        async for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                text_response += content
                
                try:
                    # ë¹„-ASCII ë¬¸ì í—ˆìš©, UTF-8 bytesë¡œ ì¦‰ì‹œ ì „ì†¡
                    payload = json.dumps({'content': content}, ensure_ascii=False)
                    yield (f"data: {payload}\n\n").encode("utf-8")
                    # ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
                except (ConnectionResetError, BrokenPipeError, OSError, ConnectionAbortedError, ConnectionError) as e:
                    # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
                    print(f"[LLM_STREAM_DB] Client disconnected during streaming lv2: {type(e).__name__}")
                    return
                except Exception as e:
                    print(f"[LLM_STREAM_DB] Unexpected error during streaming lv1: {str(e)}")
                    return
        
        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ DBì— ì €ì¥
        try:
            print(f"[LLM_STREAM_DB] ğŸ’¾ DB ì €ì¥ ì‹œì‘")
            await save_message_to_db(stream_request, text_response, image_url, db, current_user)
            print(f"[LLM_STREAM_DB] âœ… DB ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"[LLM_STREAM_DB] âŒ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ìŠ¤íŠ¸ë¦¬ë°ì€ ê³„ì† ì§„í–‰
        
        # í…ìŠ¤íŠ¸ ì‘ë‹µì´ ì™„ë£Œëœ í›„ ì´ë¯¸ì§€ URLì´ ìˆìœ¼ë©´ ì „ì†¡
        if image_url:
            try:
                response_data = {
                    "text": text_response,
                    "image_url": image_url
                }
                payload = json.dumps(response_data, ensure_ascii=False)
                yield (f"data: {payload}\n\n").encode("utf-8")
            except Exception as e:
                print(f"[LLM_STREAM_DB] Error sending image data: {str(e)}")
        
        print(f"[LLM_STREAM_DB] âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        yield "data: [DONE]\n\n".encode("utf-8")
        
    except Exception as e:
        error_message = str(e)
        print(f"[LLM_STREAM_DB] Error in streaming response: {error_message}")
        import traceback
        print(f"[LLM_STREAM_DB] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        
        try:
            # ì—°ê²° ì˜¤ë¥˜ íƒ€ì…ë³„ ì²˜ë¦¬
            if "APIConnectionError" in error_message or "Connection error" in error_message:
                error_desc = "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            elif "timeout" in error_message.lower():
                error_desc = "AI ì„œë¹„ìŠ¤ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            elif "authentication" in error_message.lower() or "unauthorized" in error_message.lower():
                error_desc = "AI ì„œë¹„ìŠ¤ ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            else:
                error_desc = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
            error_payload = json.dumps({'error': error_desc}, ensure_ascii=False)
            yield (f"data: {error_payload}\n\n").encode("utf-8")
            yield "data: [DONE]\n\n".encode("utf-8")
        except Exception:
            # ì—ëŸ¬ ì „ì†¡ë„ ì‹¤íŒ¨í•œ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
            return

async def get_streaming_response_async(messages: List[Dict], request: Request, generate_image: bool = False):
    """Stream a response from LLM using AsyncOpenAI with custom headers"""
    try:
        print(f"[LLM_STREAM] ğŸš€ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
        
        # ì´ë¯¸ì§€ URL (ì´ë¯¸ì§€ ìƒì„±ì´ ìš”ì²­ëœ ê²½ìš°)
        image_url = None
        if generate_image:
            # ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ë¡œì§ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            image_url = await generate_image(messages[-1]["content"] if messages else "")
        
        # API í‚¤ ê²€ì¦
        if not IS_OPENAI_CONFIGURED:
            error_payload = json.dumps({'error': 'AI ì„œë¹„ìŠ¤ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.'}, ensure_ascii=False)
            yield (f"data: {error_payload}\n\n").encode("utf-8")
            yield "data: [DONE]\n\n".encode("utf-8")
            return

        # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)
        httpx_client = httpx.AsyncClient(
            verify=False, 
            timeout=httpx.Timeout(30.0, connect=10.0)
        )

        # print(f"[messages í™•ì¸] {messages}")              
        

        # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL,
                http_client=httpx_client,
                default_headers={
                    "x-dep-ticket": OPENAI_API_KEY,
                    "Send-System-Name": "ds2llm",
                    "User-Id": "c.seunghoon",
                    "User-Type": "AD_ID",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                }
            )
            
        # ë¹„ë™ê¸° í˜¸ì¶œ
        response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                stream=True,
            )
        
        print(f"[LLM_STREAM] ğŸ“¥ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
        
        text_response = ""
        async for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                text_response += content
                
                try:
                    # ë¹„-ASCII ë¬¸ì í—ˆìš©, UTF-8 bytesë¡œ ì¦‰ì‹œ ì „ì†¡
                    payload = json.dumps({'content': content}, ensure_ascii=False)
                    yield (f"data: {payload}\n\n").encode("utf-8")
                    # ì§€ì—° ì œê±° - ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´
                except (ConnectionResetError, BrokenPipeError, OSError, ConnectionAbortedError, ConnectionError) as e:
                    # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
                    print(f"[LLM_STREAM] Client disconnected during streaming lv2: {type(e).__name__}")
                    return
                except Exception as e:
                    print(f"[LLM_STREAM] Unexpected error during streaming lv1: {str(e)}")
                    return
        
        # í…ìŠ¤íŠ¸ ì‘ë‹µì´ ì™„ë£Œëœ í›„ ì´ë¯¸ì§€ URLì´ ìˆìœ¼ë©´ ì „ì†¡
        if image_url:
            try:
                response_data = {
                    "text": text_response,
                    "image_url": image_url
                }
                payload = json.dumps(response_data, ensure_ascii=False)
                yield (f"data: {payload}\n\n").encode("utf-8")
            except Exception as e:
                print(f"[LLM_STREAM] Error sending image data: {str(e)}")
        
        print(f"[LLM_STREAM] âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        yield "data: [DONE]\n\n".encode("utf-8")
        
    except Exception as e:
        error_message = str(e)
        print(f"[LLM_STREAM] Error in streaming response: {error_message}")
        import traceback
        print(f"[LLM_STREAM] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        
        try:
            # ì—°ê²° ì˜¤ë¥˜ íƒ€ì…ë³„ ì²˜ë¦¬
            if "APIConnectionError" in error_message or "Connection error" in error_message:
                error_desc = "AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            elif "timeout" in error_message.lower():
                error_desc = "AI ì„œë¹„ìŠ¤ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            elif "authentication" in error_message.lower() or "unauthorized" in error_message.lower():
                error_desc = "AI ì„œë¹„ìŠ¤ ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            else:
                error_desc = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
            error_payload = json.dumps({'error': error_desc}, ensure_ascii=False)
            yield (f"data: {error_payload}\n\n").encode("utf-8")
            yield "data: [DONE]\n\n".encode("utf-8")
        except Exception:
            # ì—ëŸ¬ ì „ì†¡ë„ ì‹¤íŒ¨í•œ ê²½ìš° ì¡°ìš©íˆ ì¢…ë£Œ
            return
