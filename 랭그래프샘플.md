###app_rc.py

from typing import Optional, TypedDict, List, Any
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import END, StateGraph
from collections import defaultdict
from utils.control_config import *
from utils.control_redis import *
from llm_model.model_engine import *
from db.control_db import *
from pathlib import Path
BASE_DIR = Path(__file__).resolve().parent

# 1. ✅ 상태(State) 정의
class SearchState(TypedDict, total = False):
    question: str       # 사용자 입력 질의
    keyword: str       # 사용자 입력 질의
    candidates_each : List[dict] 
    candidates_total : List[dict] 
    response: List[dict]     # LLM이 생성한 응답


    

class CustomChatbot:
    config_engine: None
    config_db: None
    config_prompt: None
    llm: CustomLLM = None
    embeddings: CustomEmbeddings = None
    graph: StateGraph = None

    def __init__(self) -> None:
        self.config_engine = load_config(BASE_DIR / 'llm_model', 'config_engine.yaml')
        self.config_db = load_config(BASE_DIR / 'db', 'config_db.yaml')
        self.config_prompt = load_config(BASE_DIR / 'llm_model', 'config_prompt.yaml')
        ## llm 모델 호출
        self.llm = CustomLLM(config = self.config_engine['ds_maverick_chat'])
        ## embedding 모델 호출, qdrant 예제
        self.embeddings = CustomEmbeddings(config = self.config_engine['cp_jinav3_emb'])

        ## 그래프 구성
        # 그래프를 초기화합니다.
        self.graph = StateGraph(SearchState)
        self.graph.add_node('node_rc_init', self.node_rc_init)
        self.graph.add_node('node_rc_keyword', self.node_rc_keyword)
        self.graph.add_node('node_rc_rag', self.node_rc_rag)
        self.graph.add_node('node_rc_rerank', self.node_rc_rerank)
        self.graph.add_node('node_rc_answer', self.node_rc_answer)
        self.graph.add_node('node_rc_plain_answer', self.node_rc_plain_answer)

        # 간선을 정의합니다. 시작지점, 종결지점을 정의합니다.
        self.graph.set_entry_point("node_rc_init")
        self.graph.add_edge("node_rc_init", "node_rc_keyword")
        self.graph.add_edge("node_rc_keyword", "node_rc_rag")
        self.graph.add_conditional_edges(
            "node_rc_rag",
            self.judge_rc_ragscore,
            {
                "Y": "node_rc_rerank",
                "N": "node_rc_plain_answer"
            }
        )
        self.graph.add_edge("node_rc_plain_answer", END)
        # self.graph.add_edge("node_rc_rag", "node_rc_rerank")
        self.graph.add_edge("node_rc_rerank", "node_rc_answer")
        self.graph.add_edge("node_rc_answer", END)
        self.graph = self.graph.compile()
        



    def invoke(self, question) -> str:
        answer = self.graph.invoke({"question": question})
        return answer


    def node_rc_init(self, state: SearchState) -> SearchState:
        print("[inform]: node_rc_init")
        try: 
            # 초기화
            question = state['question']
            publish_node_status("node_init", "completed", {"result": question})
            return {
                "question": question,
                "keyword": str,
                "candidates_each": List[dict],
                "candidates_total": List[dict],
                "response": List[dict]
            }
        except Exception as e:
            print("[error]: node_rc_init")
            raise RuntimeError(f"[error]: node_rc_init: {str(e)}")

    def node_rc_keyword(self, state: SearchState) -> SearchState:
        print("[inform]: node_rc_keyword")
        try:
            # 프롬프트 구성
            system_prompt = self.config_prompt["node_rc_keyword"].get('system_prompt')
            user_prompt =  self.config_prompt["node_rc_keyword"].get('user_prompt')
            user_prompt = user_prompt.format(question=state['question'])
            user_prompt = user_prompt.replace("{", "{{").replace("}", "}}")
            prompt = ChatPromptTemplate.from_messages([
                    ("system", system_prompt),
                    ("user", user_prompt)
            ])
            messages = prompt.format_messages()
            formatted_messages = [{"role": msg.type, "content": msg.content} for msg in messages]

            # llm 실행
            max_retries = 3
            keyword = None
            for attempt in range(1, max_retries + 1):
                llm_response = self.llm.generate(formatted_messages)
                keyword = extract_json5(llm_response)

                if keyword:
                    keyword = {
                        key: [s for s in value if s.strip()]
                        for key, value in keyword.items()
                    }
                    print(f"[inform]: node_rc_keyword: llm attempt {attempt}-th excute")
                    publish_node_status("node_rc_keyword", "completed", {"result": keyword["keyword"]})
                    return {
                        "question": state['question'],
                        "keyword": keyword["keyword"],
                        "candidates_each": List[dict],
                        "candidates_total": List[dict],
                        "response": List[dict]
                    }
            print("[warning]: node_rc_keyword: does not make keyword")
            publish_node_status("node_rc_keyword", "completed", {"result": None})
            return {
                        "question": state['question'],
                        "keyword": [],
                        "candidates_each": List[dict],
                        "candidates_total": List[dict],
                        "response": List[dict]
            }            
            # raise RuntimeError(f"[error]: node_rc_keyword: cannot make llm response")
        except Exception as e:
            print("[error]: node_rc_keyword")
            raise RuntimeError(f"[error]: node_rc_keyword: {str(e)}")


    def node_rc_rag(self, state: SearchState) -> SearchState:
        print("[inform]: node_rc_rag")
        try:
            ip, port, collection = self.config_db['qdrant_rc'].get('db_ip'), self.config_db['qdrant_rc'].get('db_port'), self.config_db['qdrant_rc'].get('db_collection')
            # ip, port, collection = self.config_db['chromadb_rc'].get('db_ip'), self.config_db['chromadb_rc'].get('db_port'), self.config_db['chromadb_rc'].get('db_collection')
            # question으로 검색
            candidates_each = []
            if state['question']:
                query_vectors = self.embeddings.embed_documents([state['question']])
                candidates_each.extend(rag_multivector('origin', 5, [state['question']], query_vectors, ip, port, collection))
                # query = [state['question']]
                # emb_vectors = self.embeddings.embed_documents(query)
                # candidates_each.extend(rag_vector_qdrant('question', 3, query, emb_vectors, ip, port, collection))
                # candidates_each.extend(rag_payload_qdrant('question', 3, query, emb_vectors, ip, port, collection))
                # candidates_each.extend(rag_vector_chromadb('question', 10, query, emb_vectors, ip, port, 'summary_pages_v3'))
                # candidates_each.extend(rag_payload_chromadb('question', 10, query, emb_vectors, ip, port, 'summary_pages_v3'))
            # keyword로 검색
            if state['keyword']:
                query_vectors = self.embeddings.embed_documents(state['keyword'])
                candidates_each.extend(rag_multivector('origin', 2, state['keyword'], query_vectors, ip, port, collection))
                # query = state['keyword']
                # emb_vectors = self.embeddings.embed_documents(query)
                # candidates_each.extend(rag_vector_qdrant('keyword', 3, query, emb_vectors, ip, port, collection))
                # candidates_each.extend(rag_payload_qdrant('keyword', 3, query, emb_vectors, ip, port, collection))
                # candidates_each.extend(rag_vector_chromadb('question', 10, query, emb_vectors, ip, port, 'summary_pages_v3'))
                # candidates_each.extend(rag_payload_chromadb('question', 10, query, emb_vectors, ip, port, 'summary_pages_v3'))

            # 가중 평균 summation
            w_question = {
                'question':1,
                'keyword':1
            }
            w_vector = {
                'text':1,
                'summary_purpose':0.5,
                'summary_result':0.5,
                'summary_fb':0.5,
            }
            aggregated_scores = defaultdict(float)
            payloads  = {}
            for item in candidates_each:
                res_id = item['res_id']
                score = item['res_score']
                type_question = item['type_question']
                type_vector = item['type_vector']
                aggregated_scores[res_id] += score * w_question.get(type_question, 1.0) * w_vector.get(type_vector, 1.0)
                if res_id not in payloads:
                    payloads[res_id] = item['res_payload']
            # 정렬된 결과 생성 (list of dicts 형태)
            candidates_total = sorted(
                [{'res_id': res_id, 'res_score': aggregated_scores[res_id], 'res_payload': payloads[res_id]}
                for res_id in aggregated_scores],
                key=lambda x: x['res_score'],
                reverse=True
            )

            # aggregated_scores = defaultdict(float)
            # payloads  = {}
            # for item in candidates_each:
            #     id = item['id']
            #     score = item['score']
            #     source = item['source']
            #     weight = weights.get(source, 1.0)  # 없는 타입은 기본 가중치 1.0
            #     aggregated_scores[id] += score * weight
            #     if id not in payloads:
            #         payloads[id] = item['payload']

            # # 정렬된 결과 생성 (list of dicts 형태)
            # candidates_total = sorted(
            #     [{'id': id, 'score': aggregated_scores[id], 'payload': payloads[id]}
            #     for id in aggregated_scores],
            #     key=lambda x: x['score'],
            #     reverse=True  # 높은 점수부터 정렬
            # )
            publish_node_status("node_rc_rag", "completed", {"result": candidates_total})
            return {
                    "question": state['question'],
                    "keyword": state["keyword"],
                    "candidates_each": candidates_each,
                    "candidates_total": candidates_total,
                    "response": List[dict]
                }
        except Exception as e:
            print("[error]: node_rc_rag")
            raise RuntimeError(f"[error]: node_rc_rag: {str(e)}")



    def node_rc_rerank(self, state: SearchState) -> SearchState:
        print("[inform]: node_rc_rerank")
        try:
            # 프롬프트 구성
            cnt_result = 5
            candiates_top = state['candidates_total'][:cnt_result]
            documents = [item['res_payload']['vector'] for item in candiates_top] # item['payload']['text']
            system_prompt = self.config_prompt["node_rc_rerank"].get('system_prompt')
            user_prompt = self.config_prompt["node_rc_rerank"].get('user_prompt')
            user_prompt = user_prompt.format(question=state['question'], documents=documents)
            user_prompt = user_prompt.replace("{", "{{").replace("}", "}}")
            prompt = ChatPromptTemplate.from_messages([
                    ("system", system_prompt),
                    ("user", user_prompt)
            ])
            messages = prompt.format_messages()
            formatted_messages = [{"role": msg.type, "content": msg.content} for msg in messages]

            # llm 실행
            max_retries = 3
            score_rerank = None
            for attempt in range(1, max_retries + 1):
                llm_response = self.llm.generate(formatted_messages)
                score_rerank = extract_json5(llm_response)
                # question_augment = question_augment['question'] + question_augment['question_augment']

                if score_rerank and is_all_numeric(score_rerank.get('score')): # question_augment
                    score_list = [float(x) for x in score_rerank.get('score')]
                    for idx in range(len(score_list)):
                        candiates_top[idx].update({'res_relevance': score_list[idx]})
                    sorted_candiates_top = sorted(candiates_top,key=lambda x: (-x['res_relevance'], -x['res_score']))
                    print(f"[inform]: node_rc_rerank: llm attempt {attempt}-th excute")            
                    publish_node_status("node_rc_rerank", "completed", {"result": sorted_candiates_top})
                    return {
                        "question": state['question'],
                        "keyword": state["keyword"],
                        "candidates_each": state["candidates_each"],
                        "candidates_total": state["candidates_total"],
                        "response": sorted_candiates_top
                    }
            for idx in range(len(candiates_top)):
                candiates_top[idx].update({'res_relevance': 0})
            publish_node_status("node_rc_rerank", "completed", {"result": candiates_top})
            return {
                    "question": state['question'],
                    "keyword": state["keyword"],
                    "candidates_each": state["candidates_each"],
                    "candidates_total": state["candidates_total"],
                    "response": candiates_top
                }
        except Exception as e:
            print("[error]: node_rc_rerank")
            raise RuntimeError(f"[error]: node_rc_rerank: {str(e)}")



    def node_rc_answer(self, state: SearchState) -> SearchState:
        print("[inform]: node_rc_answer")
        try:
            cnt_result = 5
            candiates_top = state['response'][:cnt_result]
            response = {
                # "answer":candiates_top[0]['res_id'],
                "res_id":[rest['res_id'] for rest in candiates_top]
                }
            # tmp = list(zip([rest['payload']['id'] for rest in candiates_top],[rest['score'] for rest in candiates_top],[rest['relevance'] for rest in candiates_top]))
            # print(tmp)
            return {
                    "question": state['question'],
                    "keyword": state["keyword"],
                    "candidates_each": state["candidates_each"],
                    "candidates_total": state["candidates_total"],
                    "response": response
                }
        except Exception as e:
            print("[error]: node_rc_answer")
            raise RuntimeError(f"[error]: node_rc_answer: {str(e)}")


    def node_rc_plain_answer(self, state: SearchState) -> SearchState:
        print("[inform]: node_rc_plain_answer")
        return {
                    "question": state['question'],
                    "keyword": state["keyword"],
                    "candidates_each": state["candidates_each"],
                    "candidates_total": state["candidates_total"],
                    "response": {"res_id":[]}
        }
        # try:
        #     # 프롬프트 구성
        #     system_prompt = self.config_prompt["node_rc_plain_answer"].get('system_prompt')
        #     user_prompt =  self.config_prompt["node_rc_plain_answer"].get('user_prompt')
        #     user_prompt = user_prompt.format(question=state['question'], keywords=state['keyword'])
        #     user_prompt = user_prompt.replace("{", "{{").replace("}", "}}")
        #     prompt = ChatPromptTemplate.from_messages([
        #             ("system", system_prompt),
        #             ("user", user_prompt)
        #     ])
        #     messages = prompt.format_messages()
        #     formatted_messages = [{"role": msg.type, "content": msg.content} for msg in messages]

        #     # llm 실행
        #     max_retries = 3
        #     plain_response = None
        #     for attempt in range(1, max_retries + 1):
        #         llm_response = self.llm.generate(formatted_messages)
        #         plain_response = extract_json5(llm_response)

        #         if plain_response:
        #             print(f"[inform]: node_rc_plain_answer: llm attempt {attempt}-th excute")
        #             publish_node_status("node_rc_plain_answer", "completed", {"result": llm_response})      
        #             return {
        #                 "question": state['question'],
        #                 "keyword": state["keyword"],
        #                 "candidates_each": state["candidates_each"],
        #                 "candidates_total": state["candidates_total"],
        #                 "response": llm_response
        #             }
        #     raise RuntimeError(f"[error]: node_rc_plain_answer: cannot make llm response")
        # except Exception as e:
        #     print("[error]: node_rc_plain_answer")
        #     raise RuntimeError(f"[error]: node_rc_plain_answer: {str(e)}")

    def judge_rc_ragscore(self, state: SearchState) -> str:
        candidates_total = state["candidates_total"]
        return "Y" if any(candidate.get("res_score", 0) >= 1 for candidate in candidates_total) else "N"




import os
for var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
    if var in os.environ:
        del os.environ[var]


if __name__ == "__main__":
    chatbot = CustomChatbot()


    # # QNA 평가용
    # df = pd.read_csv("./test/qna.csv", encoding='utf-8-sig')
    # df['answer'] = None
    # import time
    # for idx, row in df.iterrows():
    #     time.sleep(5)
    #     df.at[idx,'answer'] = str(chatbot.invoke(row['q'])['response'])

    # local debugging용
    while True:
        question = input("질문을 입력해주세요 (종료를 원하시면 '종료'를 입력해주세요.): ")
        if question == "종료":
            break
        else:
            # graph.invoke 함수를 사용하여 그래프를 실행하고, 최종 결과를 반환합니다.
            # 답변 생성에는 약 1분 정도 소요됩니다.
            print("Assistant: ", chatbot.invoke(question))

